{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 Bimghi Choi. All Rights Reserved.\n",
    "# 예측 + 투자전략 시스템\n",
    "\n",
    "# _*_ coding: utf-8 _*_\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import os\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]='0, 1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import preprocess as prepro\n",
    "import models\n",
    "import learn\n",
    "from learn import GenerateResult\n",
    "import math\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # 텐서플로가 첫 번째 GPU에 1GB 메모리만 할당하도록 제한\n",
    "  try:\n",
    "#    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')    \n",
    "    tf.config.experimental.set_visible_devices(gpus[1], 'GPU')    \n",
    "    #tf.config.experimental.set_virtual_device_configuration(\n",
    "    #    gpus[1],\n",
    "    #    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024)])\n",
    "  except RuntimeError as e:\n",
    "    # 프로그램 시작시에 가장 장치가 설정되어야만 합니다\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '~/Data/kospi200f_809_0515.csv'\n",
    "item_name = 'kospi200f_reinfo_809'\n",
    "train_start = '2000-01-31'\n",
    "train_end = '2018-11-14'\n",
    "test_start = '2018-11-15'\n",
    "test_end = '2020-05-15'\n",
    "\n",
    "remove_columns = ['date', '종가', '시가', '고가', '저가']\n",
    "target_column = '종가'\n",
    "input_columns = []\n",
    "target_type = 'rate'\n",
    "\n",
    "model_name = 'ddaeryuble5'\n",
    "channel = False\n",
    "\n",
    "trans_day = 5\n",
    "\n",
    "target_alpha = 100\n",
    "future_day = 5\n",
    "train_end_idx = -2\n",
    "n_timestep = 120\n",
    "time_interval = 1\n",
    "input_size = 809\n",
    "n_unit = 800\n",
    "batch_size = 32\n",
    "learning_rate = 0.0005\n",
    "epochs = 399\n",
    "\n",
    "alpha = 0.5\n",
    "\n",
    "comment = \"2개의 모델 (지수 등락 예측, 기하평균 수익률 예측), 2개의 loss function으로 번갈아 학습, 모델1의 결과를 모델2에서 기하평균 수익이 최대 수익이 되도록 보정\"\n",
    "\n",
    "checkpoint_path = model_name + \"/pred\"+str(future_day)+\"_trans\"+str(trans_day)+\".ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = util.read_datafile(file_name)\n",
    "df = dataframe.copy()\n",
    "#df = prepro.target_conversion(df, target_column, future_day, type=target_type)\n",
    "for i in range(len(df[target_column]) -future_day):\n",
    "    df.loc[i, target_column] = ((df.loc[i + future_day, target_column] - df.loc[i, target_column]) \n",
    "                                / df.loc[i, target_column]) * target_alpha "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>시가지수(포인트)</th>\n",
       "      <th>고가지수(포인트)</th>\n",
       "      <th>저가지수(포인트)</th>\n",
       "      <th>종가지수(포인트)</th>\n",
       "      <th>수익률(%)</th>\n",
       "      <th>수익률 (1주)(%)</th>\n",
       "      <th>수익률 (1개월)(%)</th>\n",
       "      <th>수익률 (3개월)(%)</th>\n",
       "      <th>수익률 (6개월)(%)</th>\n",
       "      <th>...</th>\n",
       "      <th>주요상품선물_천연가스(선물, NYMEX)($/mmBtu)</th>\n",
       "      <th>주요상품선물_금(선물)($/ounce)</th>\n",
       "      <th>주요상품선물_은(선물)($/ounce)</th>\n",
       "      <th>주요상품선물_알루미늄(선물)($/ton)</th>\n",
       "      <th>주요상품선물_옥수수(최근월물)(￠/bu)</th>\n",
       "      <th>대두박(￠/bu)</th>\n",
       "      <th>종가</th>\n",
       "      <th>시가</th>\n",
       "      <th>고가</th>\n",
       "      <th>저가</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-31</td>\n",
       "      <td>-0.218225</td>\n",
       "      <td>-0.493106</td>\n",
       "      <td>-0.400439</td>\n",
       "      <td>-0.223565</td>\n",
       "      <td>0.212872</td>\n",
       "      <td>1.352924</td>\n",
       "      <td>-0.851790</td>\n",
       "      <td>-0.382658</td>\n",
       "      <td>-0.162615</td>\n",
       "      <td>...</td>\n",
       "      <td>1.653438</td>\n",
       "      <td>-0.853010</td>\n",
       "      <td>0.974513</td>\n",
       "      <td>0.827681</td>\n",
       "      <td>0.373581</td>\n",
       "      <td>0.614542</td>\n",
       "      <td>1.379599</td>\n",
       "      <td>116.85</td>\n",
       "      <td>120.10</td>\n",
       "      <td>116.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-02-01</td>\n",
       "      <td>-0.213200</td>\n",
       "      <td>-0.230417</td>\n",
       "      <td>-0.373954</td>\n",
       "      <td>-0.595373</td>\n",
       "      <td>-0.407635</td>\n",
       "      <td>1.808646</td>\n",
       "      <td>-1.074203</td>\n",
       "      <td>-1.707405</td>\n",
       "      <td>-0.490731</td>\n",
       "      <td>...</td>\n",
       "      <td>1.687061</td>\n",
       "      <td>-0.039689</td>\n",
       "      <td>0.193571</td>\n",
       "      <td>1.002058</td>\n",
       "      <td>0.594935</td>\n",
       "      <td>0.706918</td>\n",
       "      <td>6.762029</td>\n",
       "      <td>120.85</td>\n",
       "      <td>121.70</td>\n",
       "      <td>115.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-02-02</td>\n",
       "      <td>-0.208520</td>\n",
       "      <td>-0.448886</td>\n",
       "      <td>-0.367787</td>\n",
       "      <td>-0.200398</td>\n",
       "      <td>0.696677</td>\n",
       "      <td>2.107257</td>\n",
       "      <td>-0.769928</td>\n",
       "      <td>-1.558400</td>\n",
       "      <td>0.074389</td>\n",
       "      <td>...</td>\n",
       "      <td>1.809524</td>\n",
       "      <td>0.800355</td>\n",
       "      <td>0.031266</td>\n",
       "      <td>0.069626</td>\n",
       "      <td>0.430159</td>\n",
       "      <td>0.725768</td>\n",
       "      <td>3.714648</td>\n",
       "      <td>116.55</td>\n",
       "      <td>119.70</td>\n",
       "      <td>115.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-02-03</td>\n",
       "      <td>-0.204125</td>\n",
       "      <td>-0.216603</td>\n",
       "      <td>-0.062403</td>\n",
       "      <td>-0.021445</td>\n",
       "      <td>0.366314</td>\n",
       "      <td>1.488978</td>\n",
       "      <td>-0.625155</td>\n",
       "      <td>-1.268930</td>\n",
       "      <td>-0.079967</td>\n",
       "      <td>...</td>\n",
       "      <td>1.213642</td>\n",
       "      <td>1.515992</td>\n",
       "      <td>0.623370</td>\n",
       "      <td>0.378121</td>\n",
       "      <td>0.084769</td>\n",
       "      <td>0.352875</td>\n",
       "      <td>1.341719</td>\n",
       "      <td>119.35</td>\n",
       "      <td>120.25</td>\n",
       "      <td>116.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-02-07</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.378008</td>\n",
       "      <td>0.426748</td>\n",
       "      <td>0.592789</td>\n",
       "      <td>0.949260</td>\n",
       "      <td>1.101574</td>\n",
       "      <td>1.281908</td>\n",
       "      <td>-1.288431</td>\n",
       "      <td>1.273730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.689954</td>\n",
       "      <td>3.856851</td>\n",
       "      <td>0.568635</td>\n",
       "      <td>-0.322023</td>\n",
       "      <td>0.518339</td>\n",
       "      <td>0.702038</td>\n",
       "      <td>-5.190029</td>\n",
       "      <td>120.40</td>\n",
       "      <td>123.45</td>\n",
       "      <td>119.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2000-02-08</td>\n",
       "      <td>-0.196110</td>\n",
       "      <td>0.353748</td>\n",
       "      <td>0.677387</td>\n",
       "      <td>0.254527</td>\n",
       "      <td>-0.366454</td>\n",
       "      <td>1.138729</td>\n",
       "      <td>0.998930</td>\n",
       "      <td>-1.329397</td>\n",
       "      <td>0.942469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.332052</td>\n",
       "      <td>2.657684</td>\n",
       "      <td>1.222974</td>\n",
       "      <td>-0.720402</td>\n",
       "      <td>0.474125</td>\n",
       "      <td>0.440887</td>\n",
       "      <td>-8.577320</td>\n",
       "      <td>122.90</td>\n",
       "      <td>123.60</td>\n",
       "      <td>120.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2000-02-09</td>\n",
       "      <td>-0.192447</td>\n",
       "      <td>0.306497</td>\n",
       "      <td>0.819824</td>\n",
       "      <td>0.658684</td>\n",
       "      <td>0.641075</td>\n",
       "      <td>1.076862</td>\n",
       "      <td>1.233568</td>\n",
       "      <td>-1.637689</td>\n",
       "      <td>0.994675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557674</td>\n",
       "      <td>3.026664</td>\n",
       "      <td>2.114708</td>\n",
       "      <td>-0.407511</td>\n",
       "      <td>0.622228</td>\n",
       "      <td>0.721716</td>\n",
       "      <td>-7.145757</td>\n",
       "      <td>122.40</td>\n",
       "      <td>124.10</td>\n",
       "      <td>121.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000-02-10</td>\n",
       "      <td>0.330923</td>\n",
       "      <td>0.617187</td>\n",
       "      <td>0.552762</td>\n",
       "      <td>0.366383</td>\n",
       "      <td>-0.311193</td>\n",
       "      <td>0.651653</td>\n",
       "      <td>0.275024</td>\n",
       "      <td>-1.607973</td>\n",
       "      <td>0.269679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.812787</td>\n",
       "      <td>3.316674</td>\n",
       "      <td>1.871659</td>\n",
       "      <td>-0.157431</td>\n",
       "      <td>0.985272</td>\n",
       "      <td>1.379493</td>\n",
       "      <td>-8.302808</td>\n",
       "      <td>120.85</td>\n",
       "      <td>125.05</td>\n",
       "      <td>120.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2000-02-11</td>\n",
       "      <td>0.977388</td>\n",
       "      <td>0.576306</td>\n",
       "      <td>0.048738</td>\n",
       "      <td>-0.013448</td>\n",
       "      <td>-0.428351</td>\n",
       "      <td>0.326747</td>\n",
       "      <td>0.131530</td>\n",
       "      <td>-1.828185</td>\n",
       "      <td>0.546650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.675096</td>\n",
       "      <td>2.384344</td>\n",
       "      <td>0.988933</td>\n",
       "      <td>-0.154629</td>\n",
       "      <td>0.473525</td>\n",
       "      <td>0.907189</td>\n",
       "      <td>-7.447249</td>\n",
       "      <td>123.60</td>\n",
       "      <td>124.75</td>\n",
       "      <td>119.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2000-02-14</td>\n",
       "      <td>-0.649462</td>\n",
       "      <td>-0.806483</td>\n",
       "      <td>-0.954529</td>\n",
       "      <td>-1.222378</td>\n",
       "      <td>-1.533955</td>\n",
       "      <td>-1.217328</td>\n",
       "      <td>-0.075927</td>\n",
       "      <td>-2.486079</td>\n",
       "      <td>0.140712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.504701</td>\n",
       "      <td>1.935983</td>\n",
       "      <td>0.085505</td>\n",
       "      <td>-0.246361</td>\n",
       "      <td>0.529501</td>\n",
       "      <td>0.923675</td>\n",
       "      <td>-8.017241</td>\n",
       "      <td>118.85</td>\n",
       "      <td>119.00</td>\n",
       "      <td>115.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2000-02-15</td>\n",
       "      <td>-1.074607</td>\n",
       "      <td>-1.359121</td>\n",
       "      <td>-1.825901</td>\n",
       "      <td>-1.959771</td>\n",
       "      <td>-1.104848</td>\n",
       "      <td>-1.601324</td>\n",
       "      <td>-0.717085</td>\n",
       "      <td>-2.587555</td>\n",
       "      <td>-0.669097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.904886</td>\n",
       "      <td>1.222183</td>\n",
       "      <td>-0.051403</td>\n",
       "      <td>-1.024949</td>\n",
       "      <td>0.518408</td>\n",
       "      <td>1.160291</td>\n",
       "      <td>-3.247632</td>\n",
       "      <td>116.45</td>\n",
       "      <td>116.80</td>\n",
       "      <td>110.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2000-02-16</td>\n",
       "      <td>-1.939741</td>\n",
       "      <td>-2.072462</td>\n",
       "      <td>-2.426875</td>\n",
       "      <td>-1.820694</td>\n",
       "      <td>0.171634</td>\n",
       "      <td>-1.786449</td>\n",
       "      <td>-0.703717</td>\n",
       "      <td>-2.563710</td>\n",
       "      <td>-0.401469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.590265</td>\n",
       "      <td>1.251580</td>\n",
       "      <td>0.487841</td>\n",
       "      <td>-1.605690</td>\n",
       "      <td>0.609070</td>\n",
       "      <td>0.911960</td>\n",
       "      <td>-3.323131</td>\n",
       "      <td>112.05</td>\n",
       "      <td>114.35</td>\n",
       "      <td>105.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2000-02-17</td>\n",
       "      <td>-2.062374</td>\n",
       "      <td>-1.576333</td>\n",
       "      <td>-1.627765</td>\n",
       "      <td>-1.281362</td>\n",
       "      <td>0.934836</td>\n",
       "      <td>-1.133411</td>\n",
       "      <td>-0.976304</td>\n",
       "      <td>-1.810490</td>\n",
       "      <td>1.006196</td>\n",
       "      <td>...</td>\n",
       "      <td>1.129172</td>\n",
       "      <td>1.099126</td>\n",
       "      <td>0.014284</td>\n",
       "      <td>-1.379601</td>\n",
       "      <td>0.356659</td>\n",
       "      <td>1.165770</td>\n",
       "      <td>-3.373280</td>\n",
       "      <td>112.35</td>\n",
       "      <td>113.55</td>\n",
       "      <td>109.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2000-02-18</td>\n",
       "      <td>-1.203728</td>\n",
       "      <td>-1.119424</td>\n",
       "      <td>-1.312861</td>\n",
       "      <td>-1.662198</td>\n",
       "      <td>-0.622664</td>\n",
       "      <td>-1.227625</td>\n",
       "      <td>-1.271035</td>\n",
       "      <td>-1.891012</td>\n",
       "      <td>0.605513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.911884</td>\n",
       "      <td>1.361637</td>\n",
       "      <td>0.227203</td>\n",
       "      <td>-1.264041</td>\n",
       "      <td>0.245730</td>\n",
       "      <td>1.381693</td>\n",
       "      <td>-2.637461</td>\n",
       "      <td>113.35</td>\n",
       "      <td>115.70</td>\n",
       "      <td>111.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2000-02-21</td>\n",
       "      <td>-2.011534</td>\n",
       "      <td>-2.360013</td>\n",
       "      <td>-2.117703</td>\n",
       "      <td>-2.493118</td>\n",
       "      <td>-1.243495</td>\n",
       "      <td>-1.039810</td>\n",
       "      <td>-0.905187</td>\n",
       "      <td>-2.286667</td>\n",
       "      <td>-0.665131</td>\n",
       "      <td>...</td>\n",
       "      <td>0.863101</td>\n",
       "      <td>1.274963</td>\n",
       "      <td>0.268968</td>\n",
       "      <td>-1.589226</td>\n",
       "      <td>0.185194</td>\n",
       "      <td>1.319954</td>\n",
       "      <td>-4.123711</td>\n",
       "      <td>109.80</td>\n",
       "      <td>109.80</td>\n",
       "      <td>105.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2000-02-22</td>\n",
       "      <td>-2.301061</td>\n",
       "      <td>-2.151107</td>\n",
       "      <td>-2.119718</td>\n",
       "      <td>-2.142064</td>\n",
       "      <td>0.422501</td>\n",
       "      <td>-0.214320</td>\n",
       "      <td>-0.740215</td>\n",
       "      <td>-1.957417</td>\n",
       "      <td>-0.444290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121219</td>\n",
       "      <td>1.230409</td>\n",
       "      <td>0.364772</td>\n",
       "      <td>-1.172751</td>\n",
       "      <td>-0.103243</td>\n",
       "      <td>1.100675</td>\n",
       "      <td>-4.009324</td>\n",
       "      <td>106.85</td>\n",
       "      <td>109.15</td>\n",
       "      <td>105.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2000-02-23</td>\n",
       "      <td>-1.670887</td>\n",
       "      <td>-1.656883</td>\n",
       "      <td>-1.569320</td>\n",
       "      <td>-1.267786</td>\n",
       "      <td>1.605126</td>\n",
       "      <td>0.506204</td>\n",
       "      <td>0.050075</td>\n",
       "      <td>-1.494619</td>\n",
       "      <td>-0.699133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160356</td>\n",
       "      <td>0.708148</td>\n",
       "      <td>-0.052186</td>\n",
       "      <td>-0.980503</td>\n",
       "      <td>-1.014657</td>\n",
       "      <td>0.126165</td>\n",
       "      <td>1.718679</td>\n",
       "      <td>108.55</td>\n",
       "      <td>110.85</td>\n",
       "      <td>107.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2000-02-24</td>\n",
       "      <td>-1.366238</td>\n",
       "      <td>-1.422324</td>\n",
       "      <td>-1.358331</td>\n",
       "      <td>-1.490032</td>\n",
       "      <td>-0.528189</td>\n",
       "      <td>-0.294080</td>\n",
       "      <td>-0.319013</td>\n",
       "      <td>-1.472892</td>\n",
       "      <td>-1.661528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228028</td>\n",
       "      <td>0.530452</td>\n",
       "      <td>-1.233068</td>\n",
       "      <td>-0.941193</td>\n",
       "      <td>-0.976769</td>\n",
       "      <td>0.385420</td>\n",
       "      <td>2.296739</td>\n",
       "      <td>110.20</td>\n",
       "      <td>111.60</td>\n",
       "      <td>107.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2000-02-25</td>\n",
       "      <td>-1.680249</td>\n",
       "      <td>-1.665735</td>\n",
       "      <td>-1.510077</td>\n",
       "      <td>-1.461250</td>\n",
       "      <td>0.033092</td>\n",
       "      <td>0.075897</td>\n",
       "      <td>0.493437</td>\n",
       "      <td>-1.430882</td>\n",
       "      <td>-2.424417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568840</td>\n",
       "      <td>-0.081697</td>\n",
       "      <td>-2.561532</td>\n",
       "      <td>-1.090127</td>\n",
       "      <td>0.174516</td>\n",
       "      <td>-0.148781</td>\n",
       "      <td>3.764922</td>\n",
       "      <td>107.75</td>\n",
       "      <td>109.85</td>\n",
       "      <td>106.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2000-02-28</td>\n",
       "      <td>-1.697353</td>\n",
       "      <td>-2.045428</td>\n",
       "      <td>-2.011407</td>\n",
       "      <td>-2.307633</td>\n",
       "      <td>-1.966604</td>\n",
       "      <td>-0.289909</td>\n",
       "      <td>-1.729787</td>\n",
       "      <td>-1.563181</td>\n",
       "      <td>-3.097790</td>\n",
       "      <td>...</td>\n",
       "      <td>1.162350</td>\n",
       "      <td>-0.149500</td>\n",
       "      <td>-2.235868</td>\n",
       "      <td>-2.265504</td>\n",
       "      <td>-0.179609</td>\n",
       "      <td>0.061992</td>\n",
       "      <td>11.045943</td>\n",
       "      <td>105.95</td>\n",
       "      <td>106.55</td>\n",
       "      <td>101.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 814 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  시가지수(포인트)  고가지수(포인트)  저가지수(포인트)  종가지수(포인트)    수익률(%)  \\\n",
       "0   2000-01-31  -0.218225  -0.493106  -0.400439  -0.223565  0.212872   \n",
       "1   2000-02-01  -0.213200  -0.230417  -0.373954  -0.595373 -0.407635   \n",
       "2   2000-02-02  -0.208520  -0.448886  -0.367787  -0.200398  0.696677   \n",
       "3   2000-02-03  -0.204125  -0.216603  -0.062403  -0.021445  0.366314   \n",
       "4   2000-02-07  -0.200000   0.378008   0.426748   0.592789  0.949260   \n",
       "5   2000-02-08  -0.196110   0.353748   0.677387   0.254527 -0.366454   \n",
       "6   2000-02-09  -0.192447   0.306497   0.819824   0.658684  0.641075   \n",
       "7   2000-02-10   0.330923   0.617187   0.552762   0.366383 -0.311193   \n",
       "8   2000-02-11   0.977388   0.576306   0.048738  -0.013448 -0.428351   \n",
       "9   2000-02-14  -0.649462  -0.806483  -0.954529  -1.222378 -1.533955   \n",
       "10  2000-02-15  -1.074607  -1.359121  -1.825901  -1.959771 -1.104848   \n",
       "11  2000-02-16  -1.939741  -2.072462  -2.426875  -1.820694  0.171634   \n",
       "12  2000-02-17  -2.062374  -1.576333  -1.627765  -1.281362  0.934836   \n",
       "13  2000-02-18  -1.203728  -1.119424  -1.312861  -1.662198 -0.622664   \n",
       "14  2000-02-21  -2.011534  -2.360013  -2.117703  -2.493118 -1.243495   \n",
       "15  2000-02-22  -2.301061  -2.151107  -2.119718  -2.142064  0.422501   \n",
       "16  2000-02-23  -1.670887  -1.656883  -1.569320  -1.267786  1.605126   \n",
       "17  2000-02-24  -1.366238  -1.422324  -1.358331  -1.490032 -0.528189   \n",
       "18  2000-02-25  -1.680249  -1.665735  -1.510077  -1.461250  0.033092   \n",
       "19  2000-02-28  -1.697353  -2.045428  -2.011407  -2.307633 -1.966604   \n",
       "\n",
       "    수익률 (1주)(%)  수익률 (1개월)(%)  수익률 (3개월)(%)  수익률 (6개월)(%)  ...  \\\n",
       "0      1.352924     -0.851790     -0.382658     -0.162615  ...   \n",
       "1      1.808646     -1.074203     -1.707405     -0.490731  ...   \n",
       "2      2.107257     -0.769928     -1.558400      0.074389  ...   \n",
       "3      1.488978     -0.625155     -1.268930     -0.079967  ...   \n",
       "4      1.101574      1.281908     -1.288431      1.273730  ...   \n",
       "5      1.138729      0.998930     -1.329397      0.942469  ...   \n",
       "6      1.076862      1.233568     -1.637689      0.994675  ...   \n",
       "7      0.651653      0.275024     -1.607973      0.269679  ...   \n",
       "8      0.326747      0.131530     -1.828185      0.546650  ...   \n",
       "9     -1.217328     -0.075927     -2.486079      0.140712  ...   \n",
       "10    -1.601324     -0.717085     -2.587555     -0.669097  ...   \n",
       "11    -1.786449     -0.703717     -2.563710     -0.401469  ...   \n",
       "12    -1.133411     -0.976304     -1.810490      1.006196  ...   \n",
       "13    -1.227625     -1.271035     -1.891012      0.605513  ...   \n",
       "14    -1.039810     -0.905187     -2.286667     -0.665131  ...   \n",
       "15    -0.214320     -0.740215     -1.957417     -0.444290  ...   \n",
       "16     0.506204      0.050075     -1.494619     -0.699133  ...   \n",
       "17    -0.294080     -0.319013     -1.472892     -1.661528  ...   \n",
       "18     0.075897      0.493437     -1.430882     -2.424417  ...   \n",
       "19    -0.289909     -1.729787     -1.563181     -3.097790  ...   \n",
       "\n",
       "    주요상품선물_천연가스(선물, NYMEX)($/mmBtu)  주요상품선물_금(선물)($/ounce)  \\\n",
       "0                          1.653438              -0.853010   \n",
       "1                          1.687061              -0.039689   \n",
       "2                          1.809524               0.800355   \n",
       "3                          1.213642               1.515992   \n",
       "4                          0.689954               3.856851   \n",
       "5                          0.332052               2.657684   \n",
       "6                          0.557674               3.026664   \n",
       "7                          0.812787               3.316674   \n",
       "8                          0.675096               2.384344   \n",
       "9                          0.504701               1.935983   \n",
       "10                         0.904886               1.222183   \n",
       "11                         0.590265               1.251580   \n",
       "12                         1.129172               1.099126   \n",
       "13                         0.911884               1.361637   \n",
       "14                         0.863101               1.274963   \n",
       "15                         0.121219               1.230409   \n",
       "16                         0.160356               0.708148   \n",
       "17                         0.228028               0.530452   \n",
       "18                         0.568840              -0.081697   \n",
       "19                         1.162350              -0.149500   \n",
       "\n",
       "    주요상품선물_은(선물)($/ounce)  주요상품선물_알루미늄(선물)($/ton)  주요상품선물_옥수수(최근월물)(￠/bu)  \\\n",
       "0                0.974513                0.827681                0.373581   \n",
       "1                0.193571                1.002058                0.594935   \n",
       "2                0.031266                0.069626                0.430159   \n",
       "3                0.623370                0.378121                0.084769   \n",
       "4                0.568635               -0.322023                0.518339   \n",
       "5                1.222974               -0.720402                0.474125   \n",
       "6                2.114708               -0.407511                0.622228   \n",
       "7                1.871659               -0.157431                0.985272   \n",
       "8                0.988933               -0.154629                0.473525   \n",
       "9                0.085505               -0.246361                0.529501   \n",
       "10              -0.051403               -1.024949                0.518408   \n",
       "11               0.487841               -1.605690                0.609070   \n",
       "12               0.014284               -1.379601                0.356659   \n",
       "13               0.227203               -1.264041                0.245730   \n",
       "14               0.268968               -1.589226                0.185194   \n",
       "15               0.364772               -1.172751               -0.103243   \n",
       "16              -0.052186               -0.980503               -1.014657   \n",
       "17              -1.233068               -0.941193               -0.976769   \n",
       "18              -2.561532               -1.090127                0.174516   \n",
       "19              -2.235868               -2.265504               -0.179609   \n",
       "\n",
       "    대두박(￠/bu)         종가      시가      고가      저가  \n",
       "0    0.614542   1.379599  116.85  120.10  116.65  \n",
       "1    0.706918   6.762029  120.85  121.70  115.35  \n",
       "2    0.725768   3.714648  116.55  119.70  115.95  \n",
       "3    0.352875   1.341719  119.35  120.25  116.80  \n",
       "4    0.702038  -5.190029  120.40  123.45  119.75  \n",
       "5    0.440887  -8.577320  122.90  123.60  120.95  \n",
       "6    0.721716  -7.145757  122.40  124.10  121.85  \n",
       "7    1.379493  -8.302808  120.85  125.05  120.40  \n",
       "8    0.907189  -7.447249  123.60  124.75  119.85  \n",
       "9    0.923675  -8.017241  118.85  119.00  115.25  \n",
       "10   1.160291  -3.247632  116.45  116.80  110.45  \n",
       "11   0.911960  -3.323131  112.05  114.35  105.25  \n",
       "12   1.165770  -3.373280  112.35  113.55  109.95  \n",
       "13   1.381693  -2.637461  113.35  115.70  111.15  \n",
       "14   1.319954  -4.123711  109.80  109.80  105.95  \n",
       "15   1.100675  -4.009324  106.85  109.15  105.50  \n",
       "16   0.126165   1.718679  108.55  110.85  107.10  \n",
       "17   0.385420   2.296739  110.20  111.60  107.35  \n",
       "18  -0.148781   3.764922  107.75  109.85  106.50  \n",
       "19   0.061992  11.045943  105.95  106.55  101.95  \n",
       "\n",
       "[20 rows x 814 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ndef loss_fn_(m, train_x, train_y):\\n    \\n    #preds = m(train_x)[:, n_timestep-1, :-1].reshape((-1))\\n    targets = train_y[:, n_timestep-1, 1:].reshape((-1))\\n    base_prices = train_x[:, n_timestep-1, :-1].reshape((-1))\\n    rates = (tagets - bse_prices) / base_prices\\n    updown = np.sign(targets - base_prices)\\n    preds = model(train_x)\\n    \\n    \\n    returns = [1.0]\\n    avg_return = 1.0\\n    avg_std = 0.0\\n    state = np.zeros((4))\\n    state[0] = 1.0 # average return\\n    state[1] = 0.0 # standard dev. of returns\\n    state[2] = 1.0\\n    state[3] = preds[train_x[0]]\\n    \\n    model2 = layers.Dense(units=10, activaton='tanh')\\n    \\n    #investment rate\\n    inv_r = np.zeros((len(train_y)))\\n    best_inv = np.zeros((len(train_y)))\\n    \\n    for i in range(len(targets)):\\n        state[0] = \\n        state[1] = \\n        state[2] = (len(train_y) - i) / len(train_y) \\n        inv_r[i] = model2(state)\\n        returns.append(rates[i]*inv_r[i] + 1-abs(inv_r[i]))\\n        \\n        r = tf.random.normal([100], mean=inv_r[i])\\n        sharp_ratio = np.zeros((len(r)))\\n        returns_random = [1.0]\\n\\n        for j in range(len(r)):  \\n            returns_random.append(rates[i]*r[j] + 1-abs(r[j]))\\n            for k in range(i+1, len(train_y)):\\n                state[0] =\\n                state[1] = \\n                state[2] = (len(train_y) - k) / len(train_y)\\n                inv_r[k] = model2(state, training=False)\\n                returns_random.append(rates[k]*inv_r[k] + 1-abs(inv_r[k]))\\n            avg_return = tf.math.reduce_prod(returns_random)**(1/(len(r)-i))\\n            std_return = tf.math.reduce_std(returns_random)\\n            sharp_ratio[j] = avg_return / std_return\\n        best_return_index = tf.math.argmax(sharp_ratio)\\n        best_inv[i] = r[best_return_index]\\n\\n    return tf.math.square(inv_r - best_inv)\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def loss_fn_(m, train_x, train_y):\n",
    "    \n",
    "    #preds = m(train_x)[:, n_timestep-1, :-1].reshape((-1))\n",
    "    targets = train_y[:, n_timestep-1, 1:].reshape((-1))\n",
    "    base_prices = train_x[:, n_timestep-1, :-1].reshape((-1))\n",
    "    rates = (tagets - bse_prices) / base_prices\n",
    "    updown = np.sign(targets - base_prices)\n",
    "    preds = model(train_x)\n",
    "    \n",
    "    \n",
    "    returns = [1.0]\n",
    "    avg_return = 1.0\n",
    "    avg_std = 0.0\n",
    "    state = np.zeros((4))\n",
    "    state[0] = 1.0 # average return\n",
    "    state[1] = 0.0 # standard dev. of returns\n",
    "    state[2] = 1.0\n",
    "    state[3] = preds[train_x[0]]\n",
    "    \n",
    "    model2 = layers.Dense(units=10, activaton='tanh')\n",
    "    \n",
    "    #investment rate\n",
    "    inv_r = np.zeros((len(train_y)))\n",
    "    best_inv = np.zeros((len(train_y)))\n",
    "    \n",
    "    for i in range(len(targets)):\n",
    "        state[0] = \n",
    "        state[1] = \n",
    "        state[2] = (len(train_y) - i) / len(train_y) \n",
    "        inv_r[i] = model2(state)\n",
    "        returns.append(rates[i]*inv_r[i] + 1-abs(inv_r[i]))\n",
    "        \n",
    "        r = tf.random.normal([100], mean=inv_r[i])\n",
    "        sharp_ratio = np.zeros((len(r)))\n",
    "        returns_random = [1.0]\n",
    "\n",
    "        for j in range(len(r)):  \n",
    "            returns_random.append(rates[i]*r[j] + 1-abs(r[j]))\n",
    "            for k in range(i+1, len(train_y)):\n",
    "                state[0] =\n",
    "                state[1] = \n",
    "                state[2] = (len(train_y) - k) / len(train_y)\n",
    "                inv_r[k] = model2(state, training=False)\n",
    "                returns_random.append(rates[k]*inv_r[k] + 1-abs(inv_r[k]))\n",
    "            avg_return = tf.math.reduce_prod(returns_random)**(1/(len(r)-i))\n",
    "            std_return = tf.math.reduce_std(returns_random)\n",
    "            sharp_ratio[j] = avg_return / std_return\n",
    "        best_return_index = tf.math.argmax(sharp_ratio)\n",
    "        best_inv[i] = r[best_return_index]\n",
    "\n",
    "    return tf.math.square(inv_r - best_inv)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_remaining_days(n_batches, n_steps):\n",
    "    remaining_days = []\n",
    "    for i in range(n_batches):\n",
    "        temp = []    \n",
    "        for j in range(n_steps):\n",
    "            temp.append((n_batches - i) / n_batches)\n",
    "        remaining_days.append(np.array(temp))\n",
    "    return np.array(remaining_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def loss_fn_model1(m1, train_x, train_y):\n",
    "    \n",
    "    rates = train_y / 100\n",
    "    updown = tf.math.sign(train_y)\n",
    "    preds = tf.cast(m1(train_x), dtype=tf.float64)\n",
    "    profits = tf.cast(1 + tf.convert_to_tensor(rates, dtype=tf.float64)*preds, dtype=tf.float64)\n",
    "    \n",
    "    loss1 = keras.losses.MSE(preds, updown)\n",
    "    \n",
    "    batches = train_y.shape[0]\n",
    "\n",
    "    return_plus = []\n",
    "    return_minus = []\n",
    "    for i in range(batches):\n",
    "        if profits[i, -1, 0] - 1 > 0: return_plus.append(profits[i, -1, 0]-1)      \n",
    "        else: return_minus.append(1-profits[i, -1, 0])    \n",
    "\n",
    "    loss2 = tf.cast(tf.math.reduce_mean(return_plus), dtype=tf.float64) - tf.cast(tf.math.reduce_mean(return_minus), dtype=tf.float64) \n",
    "    \n",
    "    return loss1 - loss2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def loss_fn_model2(m1, m2, train_x, train_y):\n",
    "    \n",
    "    rates = train_y / 100\n",
    "    updown = tf.math.sign(train_y)\n",
    "    preds = tf.cast(m1(train_x), dtype=tf.float64)\n",
    "    profits = tf.cast(1 + tf.convert_to_tensor(rates, dtype=tf.float64)*preds, dtype=tf.float64)\n",
    "    \n",
    "    batches = train_y.shape[0]\n",
    "    steps = train_y.shape[1]     \n",
    "\n",
    "    n = - int(future_day/time_interval) # calc the number of unkown steps    \n",
    "    \n",
    "    # input = [preds, profits, the rate of remaining days]\n",
    "    train_x_m2 = np.zeros((batches, steps, 3))\n",
    "    train_x_m2[:, :, 0] = preds[:, :, 0]\n",
    "    train_x_m2[:, :, 1] = profits[:, :, 0]\n",
    "    train_x_m2[:, :, 2] = make_remaining_days(batches, steps)\n",
    "    \n",
    "    # unknown future profits are relaced by average profits\n",
    "    #for i in range(len(train_x_m2)):\n",
    "    #    train_x_m2[i, n:, 1] = tf.math.reduce_mean(train_x_m2[i, :n, 1])\n",
    "\n",
    "    train_x_m2 = tf.concat([preds, profits], 2)\n",
    "    train_x_m2 = tf.concat([train_x_m2, make_remaining_days(batches, steps).reshape((batches, steps, 1))], 2)    \n",
    "\n",
    "    # model2의 output이 목표일까지의 남은 일수의 기하평균 수익률이 되도록 학습\n",
    "    avg_returns = []\n",
    "    \n",
    "    for i in range(batches):\n",
    "      \n",
    "        returns = []\n",
    "        for j in range(i+1, batches):\n",
    "            returns.append(profits[j, -1, 0])\n",
    "        avg_returns.append(tf.cast(tf.math.reduce_prod(returns)**(1/(batches-i)), dtype=tf.float64))\n",
    "    \n",
    "    loss = keras.losses.MSE(tf.reshape(m2(train_x_m2)[:, -1, 0], [-1]), avg_returns)\n",
    "\n",
    "  \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def test(m1, m2, test_x, test_y):    \n",
    "    rates = test_y / 100\n",
    "    n = - int(future_day/time_interval)\n",
    "    for i in range(len(rates)):\n",
    "        rates[i, n:, 0] = 0#tf.math.reduce_mean(rates[i, :n, 0])\n",
    "    preds = tf.cast(m1.predict(test_x), dtype=tf.float64)\n",
    "    #rates = tf.cast(tf.random.normal((), mean=test_y, stddev=1) / 100, dtype=tf.float64)\n",
    "    profits = 1 + tf.convert_to_tensor(rates, dtype=tf.float64)*preds\n",
    "    \n",
    "    batches = test_y.shape[0]\n",
    "    steps = test_y.shape[1]     \n",
    "\n",
    "    test_x_m2 = np.concatenate((preds, profits), axis=2)\n",
    "    test_x_m2 = np.concatenate((test_x_m2, make_remaining_days(batches, steps).reshape(batches, steps, 1)), axis=2)\n",
    "    \n",
    "    correct_preds = np.array(preds[:, -1, 0]).reshape(-1).copy()\n",
    "    \n",
    "    for i in range(batches):\n",
    "\n",
    "        for j in range(100):\n",
    "            \n",
    "            random_test_x_m2 = test_x_m2[i, :, :].reshape((1, n_timestep, 3)).copy()\n",
    "            \n",
    "            value = m2(random_test_x_m2)[-1, -1, 0]\n",
    "                 \n",
    "            random_test_x_m2[-1, -1, 0] = max(-1, min(1, tf.cast(tf.random.normal((), mean=correct_preds[i], stddev=0.1), dtype=tf.float64)))\n",
    "            \n",
    "            if m2(random_test_x_m2)[-1, -1, 0] > value: \n",
    "                correct_preds[i] = random_test_x_m2[-1, -1, 0].copy()\n",
    "               \n",
    "    return correct_preds    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def gradient1(model1, input_data, output_data):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn_model1(model1, input_data, output_data)\n",
    "    return tape.gradient(loss, model1.trainable_variables)\n",
    "#@tf.function\n",
    "def gradient2(model1, model2, input_data, output_data):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn_model2(model1, model2, input_data, output_data)\n",
    "    return tape.gradient(loss, model2.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#strategy = tf.distribute.MirroredStrategy()\n",
    "#with strategy.scope():\n",
    "model1 = models.LSTM(n_timestep,input_size,n_unit,regularizers_alpha=0.01,drop_rate=0.5)\n",
    "model2 = models.LSTM(n_timestep,3,10,regularizers_alpha=0.01,drop_rate=0.5)\n",
    "\n",
    "    #cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    #    checkpoint_path, verbose=1, save_weights_only=True,\n",
    "        # 다섯 번째 에포크마다 가중치를 저장합니다\n",
    "    #    save_freq=5)\n",
    "\n",
    "#    model.compile(optimizer='adam',\n",
    "#                  loss='mse')\n",
    "                  #callbacks=[cp-callback]\n",
    "              #metrics=['accuracy'])\n",
    "            \n",
    "#    model2 = models.LSTM(n_timestep,input_size,n_unit,regularizers_alpha=0.01,drop_rate=0.5)\n",
    "#    model2.compile(optimizer='adam',\n",
    "#                  loss='mse')\n",
    "                  #callbacks=[cp-callback]\n",
    "              #metrics=['accuracy'])            \n",
    "\n",
    "#modle_name = model_name + \"tanh\"            \n",
    "#model.save_weights(checkpoint_path)\n",
    "#model2.save_weights(\"modle2_\"+ checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:303: setdiff1d (from tensorflow.python.ops.array_ops) is deprecated and will be removed after 2018-11-30.\n",
      "Instructions for updating:\n",
      "This op will be removed after the deprecation date. Please switch to tf.sets.difference().\n",
      "prediction_MSE = 1.9405249811247445\n",
      "prediction_MSE = 1.0426662625969925\n",
      "prediction_MSE = 1.0237717118797913\n",
      "prediction_MSE = 1.2637983827015327\n",
      "step :  1\n",
      "step :  2\n",
      "step :  3\n",
      "Training process is stopped early....\n",
      "test dates 2018-11-15~2018-11-21\n",
      "prediction1 accuracy =  tf.Tensor(0.6, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.6, shape=(), dtype=float64)\n",
      "prediction_MSE = 2.5063210014858854\n",
      "prediction_MSE = 1.1316030833601816\n",
      "prediction_MSE = 1.1120236338580831\n",
      "step :  4\n",
      "Training process is stopped early....\n",
      "test dates 2018-11-22~2018-11-28\n",
      "prediction1 accuracy =  tf.Tensor(0.6, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.2, shape=(), dtype=float64)\n",
      "prediction_MSE = 0.9943870582227072\n",
      "prediction_MSE = 1.0022878883417008\n",
      "prediction_MSE = 2.0714592081585295\n",
      "step :  5\n",
      "Training process is stopped early....\n",
      "test dates 2018-11-29~2018-12-05\n",
      "prediction1 accuracy =  tf.Tensor(0.2, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.4, shape=(), dtype=float64)\n",
      "prediction_MSE = 0.7667416920503921\n",
      "prediction_MSE = 3.010887657057836\n",
      "prediction_MSE = 2.7463849481823885\n",
      "step :  6\n",
      "Training process is stopped early....\n",
      "test dates 2018-12-06~2018-12-12\n",
      "prediction1 accuracy =  tf.Tensor(0.2, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "prediction_MSE = 0.7362081307684639\n",
      "prediction_MSE = 1.515383403062485\n",
      "prediction_MSE = 1.5956048061316062\n",
      "step :  7\n",
      "Training process is stopped early....\n",
      "test dates 2018-12-13~2018-12-19\n",
      "prediction1 accuracy =  tf.Tensor(0.6, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.6, shape=(), dtype=float64)\n",
      "prediction_MSE = 0.027273980371825955\n",
      "prediction_MSE = 0.0017748498337333274\n",
      "prediction_MSE = 0.00562692510832079\n",
      "prediction_MSE = 0.029931446448235022\n",
      "step :  1\n",
      "step :  2\n",
      "step :  3\n",
      "Training process is stopped early....\n",
      "test dates 2018-12-20~2018-12-27\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "prediction_MSE = 1.5075550623957683\n",
      "prediction_MSE = 2.360000783483774\n",
      "prediction_MSE = 2.13209136761627\n",
      "step :  4\n",
      "Training process is stopped early....\n",
      "test dates 2018-12-28~2019-01-07\n",
      "prediction1 accuracy =  tf.Tensor(0.4, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.6, shape=(), dtype=float64)\n",
      "prediction_MSE = 1.6849279322076238\n",
      "prediction_MSE = 0.47391330594762604\n",
      "prediction_MSE = 0.011767453188764421\n",
      "step :  5\n",
      "Training process is stopped early....\n",
      "test dates 2019-01-08~2019-01-14\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.8, shape=(), dtype=float64)\n",
      "prediction_MSE = 0.07519898467411963\n",
      "prediction_MSE = 1.3150542898427233\n",
      "prediction_MSE = 0.33091880626919046\n",
      "step :  6\n",
      "Training process is stopped early....\n",
      "test dates 2019-01-15~2019-01-21\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.6, shape=(), dtype=float64)\n",
      "prediction_MSE = 1.8846767897470087\n",
      "prediction_MSE = 1.5452525472609324\n",
      "prediction_MSE = 1.0340265626518566\n",
      "step :  7\n",
      "Training process is stopped early....\n",
      "test dates 2019-01-22~2019-01-28\n",
      "prediction1 accuracy =  tf.Tensor(0.6, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.6, shape=(), dtype=float64)\n",
      "prediction_MSE = 1.8428600672844169\n",
      "prediction_MSE = 0.02945273900897689\n",
      "prediction_MSE = 0.027634119255716884\n",
      "step :  8\n",
      "Training process is stopped early....\n",
      "test dates 2019-01-29~2019-02-07\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.6, shape=(), dtype=float64)\n",
      "prediction_MSE = 3.3769642523006693\n",
      "prediction_MSE = 6.378673060037215\n",
      "prediction_MSE = 3.194880404484772\n",
      "step :  9\n",
      "Training process is stopped early....\n",
      "test dates 2019-02-08~2019-02-14\n",
      "prediction1 accuracy =  tf.Tensor(0.2, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.6, shape=(), dtype=float64)\n",
      "prediction_MSE = 0.0012358697927908225\n",
      "prediction_MSE = 0.0008095177294563882\n",
      "prediction_MSE = 0.07212337636913321\n",
      "step :  10\n",
      "Training process is stopped early....\n",
      "test dates 2019-02-15~2019-02-21\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.8, shape=(), dtype=float64)\n",
      "prediction_MSE = 0.8089647752404574\n",
      "prediction_MSE = 0.4323340827396157\n",
      "prediction_MSE = 0.7824150955197959\n",
      "step :  11\n",
      "Training process is stopped early....\n",
      "test dates 2019-02-22~2019-02-28\n",
      "prediction1 accuracy =  tf.Tensor(0.8, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.6, shape=(), dtype=float64)\n",
      "prediction_MSE = 3.4144749225689837\n",
      "prediction_MSE = 3.8760587567726645\n",
      "prediction_MSE = 4.511353059892644\n",
      "step :  12\n",
      "Training process is stopped early....\n",
      "test dates 2019-03-04~2019-03-08\n",
      "prediction1 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.2, shape=(), dtype=float64)\n",
      "prediction_MSE = 2.508631420417994\n",
      "prediction_MSE = 3.2044454686158206\n",
      "prediction_MSE = 2.8964417898217696\n",
      "step :  13\n",
      "Training process is stopped early....\n",
      "test dates 2019-03-11~2019-03-15\n",
      "prediction1 accuracy =  tf.Tensor(0.2, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.4, shape=(), dtype=float64)\n",
      "prediction_MSE = 0.007522561525914994\n",
      "prediction_MSE = 3.6205892427345483\n",
      "prediction_MSE = 3.772179806581785\n",
      "step :  14\n",
      "Training process is stopped early....\n",
      "test dates 2019-03-18~2019-03-22\n",
      "prediction1 accuracy =  tf.Tensor(0.4, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.4, shape=(), dtype=float64)\n",
      "prediction_MSE = 3.410612207870377\n",
      "prediction_MSE = 0.01975821269907385\n",
      "prediction_MSE = 0.6386667636263923\n",
      "step :  15\n",
      "Training process is stopped early....\n",
      "test dates 2019-03-25~2019-03-29\n",
      "prediction1 accuracy =  tf.Tensor(0.8, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.2, shape=(), dtype=float64)\n",
      "prediction_MSE = 0.7877063055755693\n",
      "prediction_MSE = 1.6572241390510953\n",
      "prediction_MSE = 1.6386180428978014\n",
      "step :  16\n",
      "Training process is stopped early....\n",
      "test dates 2019-04-01~2019-04-05\n",
      "prediction1 accuracy =  tf.Tensor(0.6, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.4, shape=(), dtype=float64)\n",
      "prediction_MSE = 4.219907029739429\n",
      "prediction_MSE = 0.0019462317685125185\n",
      "prediction_MSE = 0.03053788873508694\n",
      "step :  17\n",
      "Training process is stopped early....\n",
      "test dates 2019-04-08~2019-04-12\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.8, shape=(), dtype=float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction_MSE = 0.829655709616938\n",
      "prediction_MSE = 0.8346494130283247\n",
      "prediction_MSE = 0.814790516596051\n",
      "step :  18\n",
      "Training process is stopped early....\n",
      "test dates 2019-04-15~2019-04-19\n",
      "prediction1 accuracy =  tf.Tensor(0.8, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.8, shape=(), dtype=float64)\n",
      "prediction_MSE = 0.8871257943798163\n",
      "prediction_MSE = 1.4882296489279532\n",
      "prediction_MSE = 1.612482413865358\n",
      "step :  19\n",
      "Training process is stopped early....\n",
      "test dates 2019-04-22~2019-04-26\n",
      "prediction1 accuracy =  tf.Tensor(0.6, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.4, shape=(), dtype=float64)\n",
      "prediction_MSE = 0.8394815210721163\n",
      "prediction_MSE = 3.288384577506116\n",
      "prediction_MSE = 3.4098953049183054\n",
      "step :  20\n",
      "Training process is stopped early....\n",
      "test dates 2019-04-29~2019-05-07\n",
      "prediction1 accuracy =  tf.Tensor(0.2, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.4, shape=(), dtype=float64)\n",
      "prediction_MSE = 3.0929012985998954\n",
      "prediction_MSE = 3.975388989557943\n",
      "prediction_MSE = 3.527049441172602\n",
      "step :  21\n",
      "Training process is stopped early....\n",
      "test dates 2019-05-08~2019-05-14\n",
      "prediction1 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "prediction_MSE = 4.1300408644395645\n",
      "prediction_MSE = 4.1916360943268645\n",
      "prediction_MSE = 0.0058039168345537465\n",
      "step :  22\n",
      "Training process is stopped early....\n",
      "test dates 2019-05-15~2019-05-21\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "prediction_MSE = 0.8576075151636156\n",
      "prediction_MSE = 0.7987016703055169\n",
      "prediction_MSE = 0.7893584110777148\n",
      "step :  23\n",
      "Training process is stopped early....\n",
      "test dates 2019-05-22~2019-05-28\n",
      "prediction1 accuracy =  tf.Tensor(0.8, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.8, shape=(), dtype=float64)\n",
      "prediction_MSE = 0.6913531273122899\n",
      "prediction_MSE = 0.3149574328859181\n",
      "prediction_MSE = 0.022872938987237034\n",
      "step :  24\n",
      "Training process is stopped early....\n",
      "test dates 2019-05-29~2019-06-04\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.8, shape=(), dtype=float64)\n",
      "prediction_MSE = 0.009736051961894532\n",
      "prediction_MSE = 0.014780550512418245\n",
      "prediction_MSE = 4.708694217686696\n",
      "step :  25\n",
      "Training process is stopped early....\n",
      "test dates 2019-06-05~2019-06-12\n",
      "prediction1 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.2, shape=(), dtype=float64)\n",
      "prediction_MSE = 6.111860069707538\n",
      "prediction_MSE = 2.2778444417538237\n",
      "prediction_MSE = 2.385150398292518\n",
      "step :  26\n",
      "Training process is stopped early....\n",
      "test dates 2019-06-13~2019-06-19\n",
      "prediction1 accuracy =  tf.Tensor(0.4, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.8, shape=(), dtype=float64)\n",
      "prediction_MSE = 4.2793361049100564\n",
      "prediction_MSE = 1.4325180376102262\n",
      "prediction_MSE = 1.574202375542437\n",
      "step :  27\n",
      "Training process is stopped early....\n",
      "test dates 2019-06-20~2019-06-26\n",
      "prediction1 accuracy =  tf.Tensor(0.6, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.8, shape=(), dtype=float64)\n",
      "prediction_MSE = 4.00062965986875\n",
      "prediction_MSE = 3.5737863274962534\n",
      "prediction_MSE = 0.9176593027154183\n",
      "step :  28\n",
      "Training process is stopped early....\n",
      "test dates 2019-06-27~2019-07-03\n",
      "prediction1 accuracy =  tf.Tensor(0.8, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "prediction_MSE = 4.2312824549776025\n",
      "prediction_MSE = 4.183255009954453\n",
      "prediction_MSE = 4.212227924690507\n",
      "step :  29\n",
      "Training process is stopped early....\n",
      "test dates 2019-07-04~2019-07-10\n",
      "prediction1 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "prediction_MSE = 2.1175592175922673\n",
      "prediction_MSE = 0.5626467272684877\n",
      "prediction_MSE = 0.5558606476465702\n",
      "step :  30\n",
      "Training process is stopped early....\n",
      "test dates 2019-07-11~2019-07-17\n",
      "prediction1 accuracy =  tf.Tensor(0.8, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.2, shape=(), dtype=float64)\n",
      "prediction_MSE = 0.7886201064386456\n",
      "prediction_MSE = 0.8362138664665508\n",
      "prediction_MSE = 2.550813833619357\n",
      "step :  31\n",
      "Training process is stopped early....\n",
      "test dates 2019-07-18~2019-07-24\n",
      "prediction1 accuracy =  tf.Tensor(0.2, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.4, shape=(), dtype=float64)\n",
      "prediction_MSE = 1.5716027966712864\n",
      "prediction_MSE = 3.3960816968175296\n",
      "prediction_MSE = 3.2695953914259754\n",
      "step :  32\n",
      "Training process is stopped early....\n",
      "test dates 2019-07-25~2019-07-31\n",
      "prediction1 accuracy =  tf.Tensor(0.2, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.2, shape=(), dtype=float64)\n",
      "prediction_MSE = 8.443458387490114\n",
      "prediction_MSE = 3.4385361356120114\n",
      "prediction_MSE = 1.2034262403566387\n",
      "step :  33\n",
      "Training process is stopped early....\n",
      "test dates 2019-08-01~2019-08-07\n",
      "prediction1 accuracy =  tf.Tensor(0.6, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.4, shape=(), dtype=float64)\n",
      "prediction_MSE = 3.5337806185624117\n",
      "prediction_MSE = 4.816569125045893\n",
      "prediction_MSE = 3.0349651175201116\n",
      "step :  34\n",
      "Training process is stopped early....\n",
      "test dates 2019-08-08~2019-08-14\n",
      "prediction1 accuracy =  tf.Tensor(0.2, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.4, shape=(), dtype=float64)\n",
      "prediction_MSE = 0.47745062204068345\n",
      "prediction_MSE = 0.024737157177682434\n",
      "prediction_MSE = 1.3097762005260996\n",
      "step :  35\n",
      "Training process is stopped early....\n",
      "test dates 2019-08-16~2019-08-22\n",
      "prediction1 accuracy =  tf.Tensor(0.8, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.8, shape=(), dtype=float64)\n",
      "prediction_MSE = 9.33837756197137\n",
      "prediction_MSE = 3.341519581524108\n",
      "prediction_MSE = 3.288647951381356\n",
      "step :  36\n",
      "Training process is stopped early....\n",
      "test dates 2019-08-23~2019-08-29\n",
      "prediction1 accuracy =  tf.Tensor(0.2, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.2, shape=(), dtype=float64)\n",
      "prediction_MSE = 0.008153525204625822\n",
      "prediction_MSE = 0.6100200429666206\n",
      "prediction_MSE = 0.3260517646873032\n",
      "step :  37\n",
      "Training process is stopped early....\n",
      "test dates 2019-08-30~2019-09-05\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "prediction_MSE = 2.415671657192862\n",
      "prediction_MSE = 0.0035923132659121393\n",
      "prediction_MSE = 0.012423774167668\n",
      "step :  38\n",
      "Training process is stopped early....\n",
      "test dates 2019-09-06~2019-09-16\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "prediction_MSE = 0.00026504513921992156\n",
      "prediction_MSE = 0.0003820645178905124\n",
      "prediction_MSE = 0.00021233043329331734\n",
      "prediction_MSE = 0.010866886988256396\n",
      "step :  1\n",
      "step :  2\n",
      "step :  3\n",
      "Training process is stopped early....\n",
      "test dates 2019-09-17~2019-09-23\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "prediction_MSE = 2.119707095662764\n",
      "prediction_MSE = 1.7712306679738077\n",
      "prediction_MSE = 1.731906440613497\n",
      "step :  4\n",
      "Training process is stopped early....\n",
      "test dates 2019-09-24~2019-09-30\n",
      "prediction1 accuracy =  tf.Tensor(0.6, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.8, shape=(), dtype=float64)\n",
      "prediction_MSE = 3.3110124219133326\n",
      "prediction_MSE = 0.8096471662260172\n",
      "prediction_MSE = 3.1613078727663355\n",
      "step :  5\n",
      "Training process is stopped early....\n",
      "test dates 2019-10-01~2019-10-08\n",
      "prediction1 accuracy =  tf.Tensor(0.2, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "prediction_MSE = 0.8240281797518548\n",
      "prediction_MSE = 0.821483356505982\n",
      "prediction_MSE = 0.8054085922596641\n",
      "step :  6\n",
      "Training process is stopped early....\n",
      "test dates 2019-10-10~2019-10-16\n",
      "prediction1 accuracy =  tf.Tensor(0.8, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.8, shape=(), dtype=float64)\n",
      "prediction_MSE = 1.8848733540414941\n",
      "prediction_MSE = 1.9377893831887725\n",
      "prediction_MSE = 1.2600615502652797\n",
      "step :  7\n",
      "Training process is stopped early....\n",
      "test dates 2019-10-17~2019-10-23\n",
      "prediction1 accuracy =  tf.Tensor(0.4, shape=(), dtype=float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction2 accuracy =  tf.Tensor(0.8, shape=(), dtype=float64)\n",
      "prediction_MSE = 1.3545300518901264\n",
      "prediction_MSE = 3.2861242516453304\n",
      "prediction_MSE = 3.4897397939323667\n",
      "step :  8\n",
      "Training process is stopped early....\n",
      "test dates 2019-10-24~2019-10-30\n",
      "prediction1 accuracy =  tf.Tensor(0.6, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.6, shape=(), dtype=float64)\n",
      "prediction_MSE = 1.9055823954897875\n",
      "prediction_MSE = 0.7513814172324335\n",
      "prediction_MSE = 0.7365890588433732\n",
      "step :  9\n",
      "Training process is stopped early....\n",
      "test dates 2019-10-31~2019-11-06\n",
      "prediction1 accuracy =  tf.Tensor(0.8, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.8, shape=(), dtype=float64)\n",
      "prediction_MSE = 0.6182464033246682\n",
      "prediction_MSE = 1.1538099715544123\n",
      "prediction_MSE = 1.4635837357659185\n",
      "step :  10\n",
      "Training process is stopped early....\n",
      "test dates 2019-11-07~2019-11-13\n",
      "prediction1 accuracy =  tf.Tensor(0.4, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.6, shape=(), dtype=float64)\n",
      "prediction_MSE = 0.4947087076018823\n",
      "prediction_MSE = 0.7828151536798467\n",
      "prediction_MSE = 0.6383229939465254\n",
      "step :  11\n",
      "Training process is stopped early....\n",
      "test dates 2019-11-14~2019-11-20\n",
      "prediction1 accuracy =  tf.Tensor(0.6, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.8, shape=(), dtype=float64)\n",
      "prediction_MSE = 1.5287903934019014\n",
      "prediction_MSE = 2.8713637033107418\n",
      "prediction_MSE = 3.2278222544556767\n",
      "step :  12\n",
      "Training process is stopped early....\n",
      "test dates 2019-11-21~2019-11-27\n",
      "prediction1 accuracy =  tf.Tensor(0.4, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "prediction_MSE = 5.996207094226904\n",
      "prediction_MSE = 3.8997132997377326\n",
      "prediction_MSE = 3.715154927244927\n",
      "step :  13\n",
      "Training process is stopped early....\n",
      "test dates 2019-11-28~2019-12-04\n",
      "prediction1 accuracy =  tf.Tensor(0.2, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "prediction_MSE = 2.2707425295611245\n",
      "prediction_MSE = 0.7364201712126593\n",
      "prediction_MSE = 0.5529847952090208\n",
      "step :  14\n",
      "Training process is stopped early....\n",
      "test dates 2019-12-05~2019-12-11\n",
      "prediction1 accuracy =  tf.Tensor(0.8, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.8, shape=(), dtype=float64)\n",
      "prediction_MSE = 0.005708309317299154\n",
      "prediction_MSE = 0.03739937462021033\n",
      "prediction_MSE = 0.3863681382826769\n",
      "step :  15\n",
      "Training process is stopped early....\n",
      "test dates 2019-12-12~2019-12-18\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "prediction_MSE = 2.2390946555496343\n",
      "prediction_MSE = 1.9916257161179125\n",
      "prediction_MSE = 1.9356872861177408\n",
      "step :  16\n",
      "Training process is stopped early....\n",
      "test dates 2019-12-19~2019-12-26\n",
      "prediction1 accuracy =  tf.Tensor(0.4, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.4, shape=(), dtype=float64)\n",
      "prediction_MSE = 2.2950634811387873\n",
      "prediction_MSE = 2.3319451115158536\n",
      "prediction_MSE = 2.826798947657076\n",
      "step :  17\n",
      "Training process is stopped early....\n",
      "test dates 2019-12-27~2020-01-06\n",
      "prediction1 accuracy =  tf.Tensor(0.4, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.4, shape=(), dtype=float64)\n",
      "prediction_MSE = 2.7977332227788994\n",
      "prediction_MSE = 2.0473940663252477\n",
      "prediction_MSE = 2.4279584286822433\n",
      "step :  18\n",
      "Training process is stopped early....\n",
      "test dates 2020-01-07~2020-01-13\n",
      "prediction1 accuracy =  tf.Tensor(0.4, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.4, shape=(), dtype=float64)\n",
      "prediction_MSE = 3.4499783384074467\n",
      "prediction_MSE = 2.723579653408369\n",
      "prediction_MSE = 2.3265405235070746\n",
      "step :  19\n",
      "Training process is stopped early....\n",
      "test dates 2020-01-14~2020-01-20\n",
      "prediction1 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "prediction_MSE = 0.6398810732294933\n",
      "prediction_MSE = 2.5871759154350604\n",
      "prediction_MSE = 2.6487929328127597\n",
      "step :  20\n",
      "Training process is stopped early....\n",
      "test dates 2020-01-21~2020-01-29\n",
      "prediction1 accuracy =  tf.Tensor(0.2, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.2, shape=(), dtype=float64)\n",
      "prediction_MSE = 3.906422706225348\n",
      "prediction_MSE = 4.6103331812486665\n",
      "prediction_MSE = 8.704117868406655\n",
      "step :  21\n",
      "Training process is stopped early....\n",
      "test dates 2020-01-30~2020-02-05\n",
      "prediction1 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "prediction_MSE = 0.023442538751010746\n",
      "prediction_MSE = 0.4808095140625719\n",
      "prediction_MSE = 0.23440664606101097\n",
      "step :  22\n",
      "Training process is stopped early....\n",
      "test dates 2020-02-06~2020-02-12\n",
      "prediction1 accuracy =  tf.Tensor(0.8, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.8, shape=(), dtype=float64)\n",
      "prediction_MSE = 2.985131564316158\n",
      "prediction_MSE = 2.568845219143501\n",
      "prediction_MSE = 2.2256912943639664\n",
      "step :  23\n",
      "Training process is stopped early....\n",
      "test dates 2020-02-13~2020-02-19\n",
      "prediction1 accuracy =  tf.Tensor(0.4, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.4, shape=(), dtype=float64)\n",
      "prediction_MSE = 4.294870239102428\n",
      "prediction_MSE = 3.6087688458310283\n",
      "prediction_MSE = 0.8930199625225181\n",
      "step :  24\n",
      "Training process is stopped early....\n",
      "test dates 2020-02-20~2020-02-26\n",
      "prediction1 accuracy =  tf.Tensor(0.6, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.6, shape=(), dtype=float64)\n",
      "prediction_MSE = 8.412838527931358\n",
      "prediction_MSE = 5.499586861886004\n",
      "prediction_MSE = 4.9916770125561785\n",
      "step :  25\n",
      "Training process is stopped early....\n",
      "test dates 2020-02-27~2020-03-04\n",
      "prediction1 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "prediction_MSE = 2.443627531690481\n",
      "prediction_MSE = 1.5604507053321819\n",
      "prediction_MSE = 1.6975010266635386\n",
      "step :  26\n",
      "Training process is stopped early....\n",
      "test dates 2020-03-05~2020-03-11\n",
      "prediction1 accuracy =  tf.Tensor(0.6, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.2, shape=(), dtype=float64)\n",
      "prediction_MSE = 3.2877484388459863\n",
      "prediction_MSE = 2.5564692283837003\n",
      "prediction_MSE = 1.3378665694729222\n",
      "step :  27\n",
      "Training process is stopped early....\n",
      "test dates 2020-03-12~2020-03-18\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "prediction_MSE = 1.05350650074775\n",
      "prediction_MSE = 1.258064215676029\n",
      "prediction_MSE = 0.9257158837820818\n",
      "step :  28\n",
      "Training process is stopped early....\n",
      "test dates 2020-03-19~2020-03-25\n",
      "prediction1 accuracy =  tf.Tensor(0.8, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.8, shape=(), dtype=float64)\n",
      "prediction_MSE = 3.8969415680223647\n",
      "prediction_MSE = 4.136163407073522\n",
      "prediction_MSE = 3.753178526955472\n",
      "step :  29\n",
      "Training process is stopped early....\n",
      "test dates 2020-03-26~2020-04-01\n",
      "prediction1 accuracy =  tf.Tensor(0.2, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.2, shape=(), dtype=float64)\n",
      "prediction_MSE = 1.0308912130283412\n",
      "prediction_MSE = 2.8861948439999554\n",
      "prediction_MSE = 1.4338862768919796\n",
      "step :  30\n",
      "Training process is stopped early....\n",
      "test dates 2020-04-02~2020-04-08\n",
      "prediction1 accuracy =  tf.Tensor(0.8, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.8, shape=(), dtype=float64)\n",
      "prediction_MSE = 0.004046458717578361\n",
      "prediction_MSE = 0.03465689008877746\n",
      "prediction_MSE = 0.00016163518136522726\n",
      "prediction_MSE = 0.009464290587003177\n",
      "step :  1\n",
      "step :  2\n",
      "step :  3\n",
      "Training process is stopped early....\n",
      "test dates 2020-04-09~2020-04-16\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "prediction_MSE = 0.1590312835723438\n",
      "prediction_MSE = 0.010470186257089153\n",
      "prediction_MSE = 0.0035511317426113465\n",
      "step :  4\n",
      "Training process is stopped early....\n",
      "test dates 2020-04-17~2020-04-23\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "prediction_MSE = 1.6562343767785315\n",
      "prediction_MSE = 1.474115675947867\n",
      "prediction_MSE = 1.6109452258120505\n",
      "step :  5\n",
      "Training process is stopped early....\n",
      "test dates 2020-04-24~2020-05-04\n",
      "prediction1 accuracy =  tf.Tensor(0.6, shape=(), dtype=float64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction2 accuracy =  tf.Tensor(0.6, shape=(), dtype=float64)\n",
      "prediction_MSE = 1.7506409869402695\n",
      "prediction_MSE = 1.6150217871399917\n",
      "prediction_MSE = 1.6646622879185855\n",
      "step :  6\n",
      "Training process is stopped early....\n",
      "test dates 2020-05-06~2020-05-12\n",
      "prediction1 accuracy =  tf.Tensor(0.6, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.6, shape=(), dtype=float64)\n",
      "prediction_MSE = 2.9366789595263847\n",
      "prediction_MSE = 2.2929050668233892\n",
      "prediction_MSE = 2.2815576290162958\n",
      "step :  7\n",
      "Training process is stopped early....\n",
      "test dates 2020-05-13~2020-05-15\n",
      "prediction1 accuracy =  tf.Tensor(0.6666666666666666, shape=(), dtype=float64)\n",
      "prediction2 accuracy =  tf.Tensor(0.6666666666666666, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "current_train_start = df.loc[prepro.date_to_index(df, train_start), 'date']\n",
    "current_train_end = df.loc[prepro.date_to_index(df, train_end), 'date']\n",
    "current_test_start = df.loc[prepro.date_to_index(df, test_start), 'date']\n",
    "current_test_end = df.loc[prepro.date_to_index(df, test_start) + trans_day - 1, 'date']\n",
    "\n",
    "\n",
    "#  각 transfer 구간의 예측값들을 합치기 위하여\n",
    "test_prediction1 = []\n",
    "test_prediction2 = []\n",
    "test_target = []\n",
    "\n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(patience=3, verbose=1)\n",
    "early_stopping = learn.EarlyStopping(patience=2, verbose=1)\n",
    "\n",
    "while True:\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    train_data, test_data = prepro.get_train_test_data(df, target_column, remove_columns, \n",
    "                                                           current_train_start, current_train_end,\n",
    "                                                           current_test_start, current_test_end,\n",
    "                                                           future_day, n_timestep, time_interval)\n",
    "    \n",
    "    # input_size, columns reset\n",
    "    input_size = len(df.columns) - len(remove_columns)\n",
    "    input_columns = df.columns.copy()\n",
    "\n",
    "    train_x, train_y = prepro.get_LSTM_dataset(train_data, n_timestep, time_interval, input_size, future_day)\n",
    "    test_x, test_y = prepro.get_LSTM_dataset(test_data, n_timestep, time_interval, input_size, future_day)\n",
    "\n",
    "    #model.compile(optimizer='adam',\n",
    "    #          loss=loss_fn)\n",
    "    #          #callbacks=[cp-callback]\n",
    "    #          #metrics=['accuracy'])\n",
    "    \n",
    "    # the firs training dataset\n",
    "    train_x = train_x[:-1]#train_x[:-int(future_day/2)]\n",
    "    train_y = train_y[:-1]#train_y[:-int(future_day/2)]    \n",
    "    \n",
    "    #global_step = tf.train.get_or_create_global_step()\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    #lr_decay = tf.train.exponential_decay(learning_rate, global_step,\n",
    "    #                                      train_input.shape[0]/batch_size*5, 0.5, staircase=True)\n",
    "    lr_decay = tf.compat.v1.train.exponential_decay(learning_rate,global_step, int(len(train_y)/batch_size), 0.96, staircase=True)\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    #optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "    updown = np.sign(test_y[:, -1, 0]).reshape((-1))    \n",
    "    iter = epochs\n",
    "    basic_epochs = tf.cast(epochs / 5, dtype=tf.int32)\n",
    "    for iteration in range(iter):\n",
    "        batch_input, batch_output = learn.next_random_batch(train_x, train_y, batch_size)\n",
    "        \n",
    "        gradients1 = gradient1(model1, batch_input, batch_output)\n",
    "        optimizer.apply_gradients(zip(gradients1, model1.trainable_variables))\n",
    "        gradients2 = gradient2(model1, model2, batch_input, batch_output)\n",
    "        optimizer.apply_gradients(zip(gradients2, model2.trainable_variables))\n",
    "        if iteration % 100 == 0:\n",
    "            #test_MSE = model.evaluate(test_x, test_y)\n",
    "            prediction = model1.predict(test_x)\n",
    "            prediction_MSE = sum((updown - prediction[:, -1, 0]).reshape(-1)**2)/len(test_y)\n",
    "            print('prediction_MSE =', prediction_MSE)\n",
    "\n",
    "        if iteration > epochs / 2 and early_stopping.validate(prediction_MSE)==True:\n",
    "            break\n",
    "     \n",
    "    if iter > basic_epochs: iter -= basic_epochs\n",
    "    if iter < basic_epochs: iter= basic_epochs\n",
    "\n",
    "    print('test dates ' + current_test_start + \"~\" + current_test_end)\n",
    "    \n",
    "    # prediction1 accuracy\n",
    "    prediction1 = model1.predict(test_x)[:, -1, 0].reshape(-1)\n",
    "    temp = tf.math.multiply(updown, prediction1)\n",
    "    accu = tf.reduce_sum(list(map(lambda x: 1 if x > 0 else 0, temp)))/len(temp)\n",
    "    print('prediction1 accuracy = ', accu)\n",
    "\n",
    "    # reinforced prediction2\n",
    "    prediction2 = test(model1, model2, test_x, test_y)    \n",
    "    temp = tf.math.multiply(updown, prediction2)\n",
    "    accu = tf.reduce_sum(list(map(lambda x: 1 if x > 0 else 0, temp)))/len(temp)\n",
    "    print('prediction2 accuracy = ', accu)\n",
    "    \n",
    "    test_prediction1.append(prediction1)\n",
    "    test_prediction2.append(prediction2)\n",
    "    \n",
    "    # escape from while\n",
    "    if current_test_end == test_end:\n",
    "     break\n",
    "\n",
    "    #train, start dates shift\n",
    "    current_train_start = df.loc[prepro.date_to_index(df, current_train_start) + trans_day, 'date']\n",
    "    current_train_end = df.loc[prepro.date_to_index(df, current_train_end) + trans_day, 'date']\n",
    "    current_test_start = df.loc[prepro.date_to_index(df, current_test_start) + trans_day, 'date']\n",
    "    if prepro.date_to_index(df, test_end) - prepro.date_to_index(df, current_test_start) < trans_day:\n",
    "        current_test_end = test_end\n",
    "    else:\n",
    "        current_test_end = df.loc[prepro.date_to_index(df, current_test_end) + trans_day, 'date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = np.concatenate(test_prediction1)\n",
    "t2 = np.concatenate(test_prediction2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_prediction = np.concatenate(test_prediction, axis=0).reshape((-1, n_timestep, 1))\n",
    "#train_prediction = learn.predict_batch_test(model1, train_x[:batch_size], len(train_x[:batch_size]))\n",
    "\n",
    "# 원 지수, 날짜는 target conversion이 되기 전 dataframe에서 가져옴\n",
    "test_dates, test_base_prices, train_dates, train_base_prices = prepro.get_test_dates_prices(dataframe, test_start, test_end,\n",
    "                                                      train_start, train_end, n_timestep, time_interval, future_day, target_column)\n",
    "\n",
    "# 전체 test_oouput 생성\n",
    "_, test_data = prepro.get_train_test_data(df, target_column, remove_columns,\n",
    "                                                   train_start, train_end,\n",
    "                                                   test_start, test_end,\n",
    "                                                   future_day, n_timestep, time_interval)\n",
    "_, test_y = prepro.get_LSTM_dataset(test_data, n_timestep, time_interval, input_size, future_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy =  tf.Tensor(0.5625, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "updown = np.sign(test_y[:, -1, 0]).reshape((-1))\n",
    "#calculate accuracy\n",
    "temp = tf.math.multiply(updown, t1.reshape((-1)))\n",
    "accu = tf.reduce_sum(list(map(lambda x: 1 if x > 0 else 0, temp)))/len(temp)\n",
    "print('accuracy = ', accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(368,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updown.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_base_prices = train_base_prices[:batch_size]\n",
    "#train_prediction = train_prediction[:batch_size]\n",
    "#train_y = train_y[:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = GenerateResult(t1, t2, test_y[:, -1, 0].reshape(-1), test_dates, n_timestep, future_day, trans_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result.extract_last_output()\n",
    "result.convert_price(test_base_prices,conversion_type=target_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info : ddaeryuble5_kospi200f_reinfo_809_120_1_5_0.565\n",
      "MSE : 12.1361 , Accuracy : 0.565\n",
      "WARNING:tensorflow:From /home/ubuntu/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1786: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: models/ddaeryuble5_kospi200f_reinfo_809_120_1_5_0.565/assets\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "result.evaluation()\n",
    "result.table()\n",
    "result.save_result(model_name,item_name,n_unit,target_type,batch_size,n_timestep,time_interval,epochs,str(alpha),comment)\n",
    "result.save_visualization()\n",
    "result.save_model(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncurrent_train_start = df.loc[prepro.date_to_index(df, train_start), \\'date\\']\\ncurrent_train_end = df.loc[prepro.date_to_index(df, train_end), \\'date\\']\\ncurrent_test_start = df.loc[prepro.date_to_index(df, test_start), \\'date\\']\\ncurrent_test_end = df.loc[prepro.date_to_index(df, test_start) + trans_day - 1, \\'date\\']\\n\\n\\n#  각 transfer 구간의 예측값들을 합치기 위하여\\ntest_prediction1 = []\\ntest_prediction2 = []\\ntest_target = []\\n\\n# early_stopping = tf.keras.callbacks.EarlyStopping(patience=3, verbose=1)\\nearly_stopping = learn.EarlyStopping(patience=2, verbose=1)\\n\\n\\ngc.collect()\\n\\ntrain_data, test_data = prepro.get_train_test_data(df, target_column, remove_columns, \\n                                                       current_train_start, current_train_end,\\n                                                       current_test_start, current_test_end,\\n                                                       future_day, n_timestep, time_interval)\\n\\n# input_size, columns reset\\ninput_size = len(df.columns) - len(remove_columns)\\ninput_columns = df.columns.copy()\\n\\ntrain_x, train_y = prepro.get_LSTM_dataset(train_data, n_timestep, time_interval, input_size, future_day)\\ntest_x, test_y = prepro.get_LSTM_dataset(test_data, n_timestep, time_interval, input_size, future_day)\\n\\n#model.compile(optimizer=\\'adam\\',\\n#          loss=loss_fn)\\n#          #callbacks=[cp-callback]\\n#          #metrics=[\\'accuracy\\'])\\n\\n# the firs training dataset\\ntrain_x = train_x[:-future_day]\\ntrain_y = train_y[:-future_day]    \\n\\n#global_step = tf.train.get_or_create_global_step()\\nglobal_step = tf.Variable(0, trainable=False)\\n#lr_decay = tf.train.exponential_decay(learning_rate, global_step,\\n#                                      train_input.shape[0]/batch_size*5, 0.5, staircase=True)\\nlr_decay = tf.compat.v1.train.exponential_decay(learning_rate,global_step, int(len(train_y)/batch_size), 0.96, staircase=True)\\noptimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\\n#optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\\n\\nupdown = np.sign(test_y[:, -1, 0]).reshape((-1))    \\nepochs = len(train_y)\\nfor iteration in range(399):\\n    batch_input, batch_output = learn.next_random_interval_batch(train_x, train_y, batch_size, future_day)\\n\\n    #noise = 2*np.random.randn(batch_size,n_timestep,1)\\n    #batch_output = batch_output+noise\\n    #batch_input = encoder(train_input[idx])\\n    gradients1 = gradient1(model1, model2, batch_input, batch_output)\\n    optimizer.apply_gradients(zip(gradients1, model1.trainable_variables))\\n    \\n    targets = tf.reshape(train_y[:, -1, 0], [-1])\\n    rates = targets / 100\\n    preds = tf.reshape(model1(train_x, training=False)[:, -1, 0], [-1])\\n    \\n    n = len(targets)    \\n    returns = [1.0]\\n    losses = []\\n    for i in range(n - 1):\\n       \\n        # average_return, std of returns, remaining days, preds[0] \\n        state = []\\n        \\n        random_rates = []\\n        for k in range(i+1):\\n            random_rates.append(rates[k])\\n        # 실재 수익률을 기반으로 random 수익률 생성, 예측에 의한 porfits 생성 \\n        for k in range(i+1, n):\\n            random_rates.append(tf.random.normal((), mean=targets[k] / 100, stddev=0.1, dtype=tf.float64))\\n        profits = tf.convert_to_tensor(random_rates, dtype=tf.float64)*tf.cast(preds, dtype=tf.float64) + 1\\n        \\n        # 현재까지의 예측에 의한 수익률 기하평균 구하기\\n        returns_past = []\\n        for k in range(i+1):\\n            returns_past.append(profits[k])\\n        avg_return = tf.cast(tf.math.reduce_prod(returns_past)**(1/(i+1)), dtype=tf.float64)\\n        \\n        state.append(avg_return)\\n        state.append(tf.math.reduce_std(returns))\\n        state.append((n - i) / n)\\n        state.append(preds[i])\\n        state = np.array(state).reshape((1, 4))\\n     \\n        # 목표일까지의 기대 기하 평균 수익률 구하기 \\n        returns_future = []\\n        for j in range(i+1, n):\\n            returns_future.append(profits[j])\\n        avg_return_future = tf.math.reduce_prod(returns_future)**(1/(n-i-1))\\n\\n        # 예측 기하 평균 수익률과 기대 기하 평균 수익률의 MSE\\n        value = tf.cast(model2(state, training=True)[0, 0], dtype=tf.float64)\\n        losses.append((value - avg_return_future)**2)\\n        if n == 3: break\\n    print(\"losses\", losses)\\n    print(\"value\", value)    \\n    with tf.GradientTape() as tape:\\n        gradients2 = tape.gradient(tf.math.reduce_sum(losses), model2.trainable_variables)\\n    optimizer.apply_gradients(zip(gradients2, model2.trainable_variables))\\n    \\n\\n    \\n    if iteration % 100 == 0:\\n        #test_MSE = model.evaluate(test_x, test_y)\\n        prediction = model1.predict(test_x)\\n        prediction_MSE = sum((updown - np.sign(prediction[:, -1, 0]).reshape(-1))**2)/len(test_y)\\n        print(\\'prediction_MSE =\\', prediction_MSE)\\n\\n    if iteration > epochs / 2 and early_stopping.validate(prediction_MSE)==True:\\n        break\\nepochs -= epochs / 5\\nif epochs <= 0: epochs = 100\\n'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "current_train_start = df.loc[prepro.date_to_index(df, train_start), 'date']\n",
    "current_train_end = df.loc[prepro.date_to_index(df, train_end), 'date']\n",
    "current_test_start = df.loc[prepro.date_to_index(df, test_start), 'date']\n",
    "current_test_end = df.loc[prepro.date_to_index(df, test_start) + trans_day - 1, 'date']\n",
    "\n",
    "\n",
    "#  각 transfer 구간의 예측값들을 합치기 위하여\n",
    "test_prediction1 = []\n",
    "test_prediction2 = []\n",
    "test_target = []\n",
    "\n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(patience=3, verbose=1)\n",
    "early_stopping = learn.EarlyStopping(patience=2, verbose=1)\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "train_data, test_data = prepro.get_train_test_data(df, target_column, remove_columns, \n",
    "                                                       current_train_start, current_train_end,\n",
    "                                                       current_test_start, current_test_end,\n",
    "                                                       future_day, n_timestep, time_interval)\n",
    "\n",
    "# input_size, columns reset\n",
    "input_size = len(df.columns) - len(remove_columns)\n",
    "input_columns = df.columns.copy()\n",
    "\n",
    "train_x, train_y = prepro.get_LSTM_dataset(train_data, n_timestep, time_interval, input_size, future_day)\n",
    "test_x, test_y = prepro.get_LSTM_dataset(test_data, n_timestep, time_interval, input_size, future_day)\n",
    "\n",
    "#model.compile(optimizer='adam',\n",
    "#          loss=loss_fn)\n",
    "#          #callbacks=[cp-callback]\n",
    "#          #metrics=['accuracy'])\n",
    "\n",
    "# the firs training dataset\n",
    "train_x = train_x[:-future_day]\n",
    "train_y = train_y[:-future_day]    \n",
    "\n",
    "#global_step = tf.train.get_or_create_global_step()\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "#lr_decay = tf.train.exponential_decay(learning_rate, global_step,\n",
    "#                                      train_input.shape[0]/batch_size*5, 0.5, staircase=True)\n",
    "lr_decay = tf.compat.v1.train.exponential_decay(learning_rate,global_step, int(len(train_y)/batch_size), 0.96, staircase=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "#optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "updown = np.sign(test_y[:, -1, 0]).reshape((-1))    \n",
    "epochs = len(train_y)\n",
    "for iteration in range(399):\n",
    "    batch_input, batch_output = learn.next_random_interval_batch(train_x, train_y, batch_size, future_day)\n",
    "\n",
    "    #noise = 2*np.random.randn(batch_size,n_timestep,1)\n",
    "    #batch_output = batch_output+noise\n",
    "    #batch_input = encoder(train_input[idx])\n",
    "    gradients1 = gradient1(model1, model2, batch_input, batch_output)\n",
    "    optimizer.apply_gradients(zip(gradients1, model1.trainable_variables))\n",
    "    \n",
    "    targets = tf.reshape(train_y[:, -1, 0], [-1])\n",
    "    rates = targets / 100\n",
    "    preds = tf.reshape(model1(train_x, training=False)[:, -1, 0], [-1])\n",
    "    \n",
    "    n = len(targets)    \n",
    "    returns = [1.0]\n",
    "    losses = []\n",
    "    for i in range(n - 1):\n",
    "       \n",
    "        # average_return, std of returns, remaining days, preds[0] \n",
    "        state = []\n",
    "        \n",
    "        random_rates = []\n",
    "        for k in range(i+1):\n",
    "            random_rates.append(rates[k])\n",
    "        # 실재 수익률을 기반으로 random 수익률 생성, 예측에 의한 porfits 생성 \n",
    "        for k in range(i+1, n):\n",
    "            random_rates.append(tf.random.normal((), mean=targets[k] / 100, stddev=0.1, dtype=tf.float64))\n",
    "        profits = tf.convert_to_tensor(random_rates, dtype=tf.float64)*tf.cast(preds, dtype=tf.float64) + 1\n",
    "        \n",
    "        # 현재까지의 예측에 의한 수익률 기하평균 구하기\n",
    "        returns_past = []\n",
    "        for k in range(i+1):\n",
    "            returns_past.append(profits[k])\n",
    "        avg_return = tf.cast(tf.math.reduce_prod(returns_past)**(1/(i+1)), dtype=tf.float64)\n",
    "        \n",
    "        state.append(avg_return)\n",
    "        state.append(tf.math.reduce_std(returns))\n",
    "        state.append((n - i) / n)\n",
    "        state.append(preds[i])\n",
    "        state = np.array(state).reshape((1, 4))\n",
    "     \n",
    "        # 목표일까지의 기대 기하 평균 수익률 구하기 \n",
    "        returns_future = []\n",
    "        for j in range(i+1, n):\n",
    "            returns_future.append(profits[j])\n",
    "        avg_return_future = tf.math.reduce_prod(returns_future)**(1/(n-i-1))\n",
    "\n",
    "        # 예측 기하 평균 수익률과 기대 기하 평균 수익률의 MSE\n",
    "        value = tf.cast(model2(state, training=True)[0, 0], dtype=tf.float64)\n",
    "        losses.append((value - avg_return_future)**2)\n",
    "        if n == 3: break\n",
    "    print(\"losses\", losses)\n",
    "    print(\"value\", value)    \n",
    "    with tf.GradientTape() as tape:\n",
    "        gradients2 = tape.gradient(tf.math.reduce_sum(losses), model2.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients2, model2.trainable_variables))\n",
    "    \n",
    "\n",
    "    \n",
    "    if iteration % 100 == 0:\n",
    "        #test_MSE = model.evaluate(test_x, test_y)\n",
    "        prediction = model1.predict(test_x)\n",
    "        prediction_MSE = sum((updown - np.sign(prediction[:, -1, 0]).reshape(-1))**2)/len(test_y)\n",
    "        print('prediction_MSE =', prediction_MSE)\n",
    "\n",
    "    if iteration > epochs / 2 and early_stopping.validate(prediction_MSE)==True:\n",
    "        break\n",
    "epochs -= epochs / 5\n",
    "if epochs <= 0: epochs = 100\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow)",
   "language": "python",
   "name": "conda_tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
