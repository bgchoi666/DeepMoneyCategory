{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 Bimghi Choi. All Rights Reserved.\n",
    "# 예측 + 투자전략 시스템\n",
    "\n",
    "# _*_ coding: utf-8 _*_\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#import os\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]='0, 1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import preprocess as prepro\n",
    "import models\n",
    "import learn\n",
    "from learn import GenerateResult\n",
    "import math\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # 텐서플로가 첫 번째 GPU에 1GB 메모리만 할당하도록 제한\n",
    "  try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')    \n",
    "    #tf.config.experimental.set_visible_devices(gpus[1], 'GPU')    \n",
    "#    tf.config.experimental.set_virtual_device_configuration(\n",
    "#        gpus[0],\n",
    "#        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=10240)])\n",
    "    #tf.config.experimental.set_virtual_device_configuration(\n",
    "    #    gpus[1],\n",
    "    #    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])    \n",
    "  except RuntimeError as e:\n",
    "    # 프로그램 시작시에 가장 장치가 설정되어야만 합니다\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bayesian opt로 최적화된 parameter\n",
    "학습기간 2000-01-31 ~ 2017-01-02\n",
    "test기간 2017-01-02 ~ 2029-05-15\n",
    "\n",
    "2일 예측을 1일 예측으로 사용\n",
    "\n",
    "best parameter\n",
    "steps 30, interval 1, units 1024, batch size 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '~/Data/kospi200f_809_0515.csv'\n",
    "item_name = 'kospi200f_loss_cut'\n",
    "train_start = '2000-01-31'\n",
    "train_end = '2017-01-02'\n",
    "test_start = '2017-01-02'\n",
    "test_end = '2020-05-15'\n",
    "\n",
    "remove_columns = ['date', '종가']\n",
    "target_column = '종가'\n",
    "input_columns = []\n",
    "target_type = 'rate'\n",
    "\n",
    "model_name = 'bayesian-opt'\n",
    "channel = False\n",
    "\n",
    "trans_day = 1\n",
    "\n",
    "target_alpha = 100\n",
    "future_day = 2\n",
    "train_end_back = -1\n",
    "n_timestep = 30\n",
    "time_interval = 1\n",
    "input_size = 809\n",
    "n_unit = 1024\n",
    "batch_size = 10\n",
    "learning_rate = 0.0005\n",
    "epochs = 3\n",
    "\n",
    "alpha = 0.05\n",
    "beta = 0.01\n",
    "\n",
    "comment = \"2일 예측을 이용 1일 예측, bayesian optimization (timestep, interval, units, batch_size,  epochs) loss - model1\"\n",
    "\n",
    "checkpoint_path = model_name + \"/pred\"+str(future_day)+\"-\"+datetime.now().strftime(\"%Y-%m-%d:%H:%M:%S\")+\".ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = util.read_datafile(file_name)\n",
    "df = dataframe.copy()\n",
    "#df = prepro.target_conversion(df, target_column, future_day, type=target_type)\n",
    "#a = []\n",
    "#b = []\n",
    "for i in range(len(df[target_column]) -future_day):\n",
    "    df.loc[i, target_column] = ((df.loc[i + future_day, target_column] - df.loc[i, target_column]) \n",
    "                                / df.loc[i, target_column]) * target_alpha \n",
    "    df.loc[i, '시가'] = df.loc[i + future_day + train_end_back + 1, '시가']\n",
    "#    a.append(max(df.loc[i + future_day + train_end_back + 1:i + future_day + 1, '고가']))\n",
    "#    b.append(min(df.loc[i + future_day + train_end_back + 1:i + future_day + 1, '저가']))\n",
    "#for i in range(len(df[target_column]) -future_day):\n",
    "#    df.loc[i, '고가'] = a[i]\n",
    "#    df.loc[i, '저가'] = b[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>시가지수(포인트)</th>\n",
       "      <th>고가지수(포인트)</th>\n",
       "      <th>저가지수(포인트)</th>\n",
       "      <th>종가지수(포인트)</th>\n",
       "      <th>수익률(%)</th>\n",
       "      <th>수익률 (1주)(%)</th>\n",
       "      <th>수익률 (1개월)(%)</th>\n",
       "      <th>수익률 (3개월)(%)</th>\n",
       "      <th>수익률 (6개월)(%)</th>\n",
       "      <th>...</th>\n",
       "      <th>주요상품선물_천연가스(선물, NYMEX)($/mmBtu)</th>\n",
       "      <th>주요상품선물_금(선물)($/ounce)</th>\n",
       "      <th>주요상품선물_은(선물)($/ounce)</th>\n",
       "      <th>주요상품선물_알루미늄(선물)($/ton)</th>\n",
       "      <th>주요상품선물_옥수수(최근월물)(￠/bu)</th>\n",
       "      <th>대두박(￠/bu)</th>\n",
       "      <th>종가</th>\n",
       "      <th>시가</th>\n",
       "      <th>고가</th>\n",
       "      <th>저가</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-31</td>\n",
       "      <td>-0.218225</td>\n",
       "      <td>-0.493106</td>\n",
       "      <td>-0.400439</td>\n",
       "      <td>-0.223565</td>\n",
       "      <td>0.212872</td>\n",
       "      <td>1.352924</td>\n",
       "      <td>-0.851790</td>\n",
       "      <td>-0.382658</td>\n",
       "      <td>-0.162615</td>\n",
       "      <td>...</td>\n",
       "      <td>1.653438</td>\n",
       "      <td>-0.853010</td>\n",
       "      <td>0.974513</td>\n",
       "      <td>0.827681</td>\n",
       "      <td>0.373581</td>\n",
       "      <td>0.614542</td>\n",
       "      <td>-0.961538</td>\n",
       "      <td>116.55</td>\n",
       "      <td>120.10</td>\n",
       "      <td>116.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-02-01</td>\n",
       "      <td>-0.213200</td>\n",
       "      <td>-0.230417</td>\n",
       "      <td>-0.373954</td>\n",
       "      <td>-0.595373</td>\n",
       "      <td>-0.407635</td>\n",
       "      <td>1.808646</td>\n",
       "      <td>-1.074203</td>\n",
       "      <td>-1.707405</td>\n",
       "      <td>-0.490731</td>\n",
       "      <td>...</td>\n",
       "      <td>1.687061</td>\n",
       "      <td>-0.039689</td>\n",
       "      <td>0.193571</td>\n",
       "      <td>1.002058</td>\n",
       "      <td>0.594935</td>\n",
       "      <td>0.706918</td>\n",
       "      <td>3.381014</td>\n",
       "      <td>119.35</td>\n",
       "      <td>121.70</td>\n",
       "      <td>115.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-02-02</td>\n",
       "      <td>-0.208520</td>\n",
       "      <td>-0.448886</td>\n",
       "      <td>-0.367787</td>\n",
       "      <td>-0.200398</td>\n",
       "      <td>0.696677</td>\n",
       "      <td>2.107257</td>\n",
       "      <td>-0.769928</td>\n",
       "      <td>-1.558400</td>\n",
       "      <td>0.074389</td>\n",
       "      <td>...</td>\n",
       "      <td>1.809524</td>\n",
       "      <td>0.800355</td>\n",
       "      <td>0.031266</td>\n",
       "      <td>0.069626</td>\n",
       "      <td>0.430159</td>\n",
       "      <td>0.725768</td>\n",
       "      <td>3.292528</td>\n",
       "      <td>120.40</td>\n",
       "      <td>119.70</td>\n",
       "      <td>115.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-02-03</td>\n",
       "      <td>-0.204125</td>\n",
       "      <td>-0.216603</td>\n",
       "      <td>-0.062403</td>\n",
       "      <td>-0.021445</td>\n",
       "      <td>0.366314</td>\n",
       "      <td>1.488978</td>\n",
       "      <td>-0.625155</td>\n",
       "      <td>-1.268930</td>\n",
       "      <td>-0.079967</td>\n",
       "      <td>...</td>\n",
       "      <td>1.213642</td>\n",
       "      <td>1.515992</td>\n",
       "      <td>0.623370</td>\n",
       "      <td>0.378121</td>\n",
       "      <td>0.084769</td>\n",
       "      <td>0.352875</td>\n",
       "      <td>1.677149</td>\n",
       "      <td>122.90</td>\n",
       "      <td>120.25</td>\n",
       "      <td>116.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-02-07</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.378008</td>\n",
       "      <td>0.426748</td>\n",
       "      <td>0.592789</td>\n",
       "      <td>0.949260</td>\n",
       "      <td>1.101574</td>\n",
       "      <td>1.281908</td>\n",
       "      <td>-1.288431</td>\n",
       "      <td>1.273730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.689954</td>\n",
       "      <td>3.856851</td>\n",
       "      <td>0.568635</td>\n",
       "      <td>-0.322023</td>\n",
       "      <td>0.518339</td>\n",
       "      <td>0.702038</td>\n",
       "      <td>0.653862</td>\n",
       "      <td>122.40</td>\n",
       "      <td>123.45</td>\n",
       "      <td>119.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2000-02-08</td>\n",
       "      <td>-0.196110</td>\n",
       "      <td>0.353748</td>\n",
       "      <td>0.677387</td>\n",
       "      <td>0.254527</td>\n",
       "      <td>-0.366454</td>\n",
       "      <td>1.138729</td>\n",
       "      <td>0.998930</td>\n",
       "      <td>-1.329397</td>\n",
       "      <td>0.942469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.332052</td>\n",
       "      <td>2.657684</td>\n",
       "      <td>1.222974</td>\n",
       "      <td>-0.720402</td>\n",
       "      <td>0.474125</td>\n",
       "      <td>0.440887</td>\n",
       "      <td>1.319588</td>\n",
       "      <td>120.85</td>\n",
       "      <td>123.60</td>\n",
       "      <td>120.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2000-02-09</td>\n",
       "      <td>-0.192447</td>\n",
       "      <td>0.306497</td>\n",
       "      <td>0.819824</td>\n",
       "      <td>0.658684</td>\n",
       "      <td>0.641075</td>\n",
       "      <td>1.076862</td>\n",
       "      <td>1.233568</td>\n",
       "      <td>-1.637689</td>\n",
       "      <td>0.994675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557674</td>\n",
       "      <td>3.026664</td>\n",
       "      <td>2.114708</td>\n",
       "      <td>-0.407511</td>\n",
       "      <td>0.622228</td>\n",
       "      <td>0.721716</td>\n",
       "      <td>-1.867641</td>\n",
       "      <td>123.60</td>\n",
       "      <td>124.10</td>\n",
       "      <td>121.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000-02-10</td>\n",
       "      <td>0.330923</td>\n",
       "      <td>0.617187</td>\n",
       "      <td>0.552762</td>\n",
       "      <td>0.366383</td>\n",
       "      <td>-0.311193</td>\n",
       "      <td>0.651653</td>\n",
       "      <td>0.275024</td>\n",
       "      <td>-1.607973</td>\n",
       "      <td>0.269679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.812787</td>\n",
       "      <td>3.316674</td>\n",
       "      <td>1.871659</td>\n",
       "      <td>-0.157431</td>\n",
       "      <td>0.985272</td>\n",
       "      <td>1.379493</td>\n",
       "      <td>-5.575906</td>\n",
       "      <td>118.85</td>\n",
       "      <td>125.05</td>\n",
       "      <td>120.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2000-02-11</td>\n",
       "      <td>0.977388</td>\n",
       "      <td>0.576306</td>\n",
       "      <td>0.048738</td>\n",
       "      <td>-0.013448</td>\n",
       "      <td>-0.428351</td>\n",
       "      <td>0.326747</td>\n",
       "      <td>0.131530</td>\n",
       "      <td>-1.828185</td>\n",
       "      <td>0.546650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.675096</td>\n",
       "      <td>2.384344</td>\n",
       "      <td>0.988933</td>\n",
       "      <td>-0.154629</td>\n",
       "      <td>0.473525</td>\n",
       "      <td>0.907189</td>\n",
       "      <td>-8.274721</td>\n",
       "      <td>116.45</td>\n",
       "      <td>124.75</td>\n",
       "      <td>119.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2000-02-14</td>\n",
       "      <td>-0.649462</td>\n",
       "      <td>-0.806483</td>\n",
       "      <td>-0.954529</td>\n",
       "      <td>-1.222378</td>\n",
       "      <td>-1.533955</td>\n",
       "      <td>-1.217328</td>\n",
       "      <td>-0.075927</td>\n",
       "      <td>-2.486079</td>\n",
       "      <td>0.140712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.504701</td>\n",
       "      <td>1.935983</td>\n",
       "      <td>0.085505</td>\n",
       "      <td>-0.246361</td>\n",
       "      <td>0.529501</td>\n",
       "      <td>0.923675</td>\n",
       "      <td>-1.422414</td>\n",
       "      <td>112.05</td>\n",
       "      <td>119.00</td>\n",
       "      <td>115.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2000-02-15</td>\n",
       "      <td>-1.074607</td>\n",
       "      <td>-1.359121</td>\n",
       "      <td>-1.825901</td>\n",
       "      <td>-1.959771</td>\n",
       "      <td>-1.104848</td>\n",
       "      <td>-1.601324</td>\n",
       "      <td>-0.717085</td>\n",
       "      <td>-2.587555</td>\n",
       "      <td>-0.669097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.904886</td>\n",
       "      <td>1.222183</td>\n",
       "      <td>-0.051403</td>\n",
       "      <td>-1.024949</td>\n",
       "      <td>0.518408</td>\n",
       "      <td>1.160291</td>\n",
       "      <td>1.623816</td>\n",
       "      <td>112.35</td>\n",
       "      <td>116.80</td>\n",
       "      <td>110.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2000-02-16</td>\n",
       "      <td>-1.939741</td>\n",
       "      <td>-2.072462</td>\n",
       "      <td>-2.426875</td>\n",
       "      <td>-1.820694</td>\n",
       "      <td>0.171634</td>\n",
       "      <td>-1.786449</td>\n",
       "      <td>-0.703717</td>\n",
       "      <td>-2.563710</td>\n",
       "      <td>-0.401469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.590265</td>\n",
       "      <td>1.251580</td>\n",
       "      <td>0.487841</td>\n",
       "      <td>-1.605690</td>\n",
       "      <td>0.609070</td>\n",
       "      <td>0.911960</td>\n",
       "      <td>-2.186270</td>\n",
       "      <td>113.35</td>\n",
       "      <td>114.35</td>\n",
       "      <td>105.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2000-02-17</td>\n",
       "      <td>-2.062374</td>\n",
       "      <td>-1.576333</td>\n",
       "      <td>-1.627765</td>\n",
       "      <td>-1.281362</td>\n",
       "      <td>0.934836</td>\n",
       "      <td>-1.133411</td>\n",
       "      <td>-0.976304</td>\n",
       "      <td>-1.810490</td>\n",
       "      <td>1.006196</td>\n",
       "      <td>...</td>\n",
       "      <td>1.129172</td>\n",
       "      <td>1.099126</td>\n",
       "      <td>0.014284</td>\n",
       "      <td>-1.379601</td>\n",
       "      <td>0.356659</td>\n",
       "      <td>1.165770</td>\n",
       "      <td>-5.281846</td>\n",
       "      <td>109.80</td>\n",
       "      <td>113.55</td>\n",
       "      <td>109.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2000-02-18</td>\n",
       "      <td>-1.203728</td>\n",
       "      <td>-1.119424</td>\n",
       "      <td>-1.312861</td>\n",
       "      <td>-1.662198</td>\n",
       "      <td>-0.622664</td>\n",
       "      <td>-1.227625</td>\n",
       "      <td>-1.271035</td>\n",
       "      <td>-1.891012</td>\n",
       "      <td>0.605513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.911884</td>\n",
       "      <td>1.361637</td>\n",
       "      <td>0.227203</td>\n",
       "      <td>-1.264041</td>\n",
       "      <td>0.245730</td>\n",
       "      <td>1.381693</td>\n",
       "      <td>-4.112651</td>\n",
       "      <td>106.85</td>\n",
       "      <td>115.70</td>\n",
       "      <td>111.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2000-02-21</td>\n",
       "      <td>-2.011534</td>\n",
       "      <td>-2.360013</td>\n",
       "      <td>-2.117703</td>\n",
       "      <td>-2.493118</td>\n",
       "      <td>-1.243495</td>\n",
       "      <td>-1.039810</td>\n",
       "      <td>-0.905187</td>\n",
       "      <td>-2.286667</td>\n",
       "      <td>-0.665131</td>\n",
       "      <td>...</td>\n",
       "      <td>0.863101</td>\n",
       "      <td>1.274963</td>\n",
       "      <td>0.268968</td>\n",
       "      <td>-1.589226</td>\n",
       "      <td>0.185194</td>\n",
       "      <td>1.319954</td>\n",
       "      <td>3.608247</td>\n",
       "      <td>108.55</td>\n",
       "      <td>109.80</td>\n",
       "      <td>105.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2000-02-22</td>\n",
       "      <td>-2.301061</td>\n",
       "      <td>-2.151107</td>\n",
       "      <td>-2.119718</td>\n",
       "      <td>-2.142064</td>\n",
       "      <td>0.422501</td>\n",
       "      <td>-0.214320</td>\n",
       "      <td>-0.740215</td>\n",
       "      <td>-1.957417</td>\n",
       "      <td>-0.444290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121219</td>\n",
       "      <td>1.230409</td>\n",
       "      <td>0.364772</td>\n",
       "      <td>-1.172751</td>\n",
       "      <td>-0.103243</td>\n",
       "      <td>1.100675</td>\n",
       "      <td>1.491841</td>\n",
       "      <td>110.20</td>\n",
       "      <td>109.15</td>\n",
       "      <td>105.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2000-02-23</td>\n",
       "      <td>-1.670887</td>\n",
       "      <td>-1.656883</td>\n",
       "      <td>-1.569320</td>\n",
       "      <td>-1.267786</td>\n",
       "      <td>1.605126</td>\n",
       "      <td>0.506204</td>\n",
       "      <td>0.050075</td>\n",
       "      <td>-1.494619</td>\n",
       "      <td>-0.699133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160356</td>\n",
       "      <td>0.708148</td>\n",
       "      <td>-0.052186</td>\n",
       "      <td>-0.980503</td>\n",
       "      <td>-1.014657</td>\n",
       "      <td>0.126165</td>\n",
       "      <td>-1.492537</td>\n",
       "      <td>107.75</td>\n",
       "      <td>110.85</td>\n",
       "      <td>107.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2000-02-24</td>\n",
       "      <td>-1.366238</td>\n",
       "      <td>-1.422324</td>\n",
       "      <td>-1.358331</td>\n",
       "      <td>-1.490032</td>\n",
       "      <td>-0.528189</td>\n",
       "      <td>-0.294080</td>\n",
       "      <td>-0.319013</td>\n",
       "      <td>-1.472892</td>\n",
       "      <td>-1.661528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228028</td>\n",
       "      <td>0.530452</td>\n",
       "      <td>-1.233068</td>\n",
       "      <td>-0.941193</td>\n",
       "      <td>-0.976769</td>\n",
       "      <td>0.385420</td>\n",
       "      <td>-6.017455</td>\n",
       "      <td>105.95</td>\n",
       "      <td>111.60</td>\n",
       "      <td>107.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2000-02-25</td>\n",
       "      <td>-1.680249</td>\n",
       "      <td>-1.665735</td>\n",
       "      <td>-1.510077</td>\n",
       "      <td>-1.461250</td>\n",
       "      <td>0.033092</td>\n",
       "      <td>0.075897</td>\n",
       "      <td>0.493437</td>\n",
       "      <td>-1.430882</td>\n",
       "      <td>-2.424417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568840</td>\n",
       "      <td>-0.081697</td>\n",
       "      <td>-2.561532</td>\n",
       "      <td>-1.090127</td>\n",
       "      <td>0.174516</td>\n",
       "      <td>-0.148781</td>\n",
       "      <td>-5.463728</td>\n",
       "      <td>104.15</td>\n",
       "      <td>109.85</td>\n",
       "      <td>106.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2000-02-28</td>\n",
       "      <td>-1.697353</td>\n",
       "      <td>-2.045428</td>\n",
       "      <td>-2.011407</td>\n",
       "      <td>-2.307633</td>\n",
       "      <td>-1.966604</td>\n",
       "      <td>-0.289909</td>\n",
       "      <td>-1.729787</td>\n",
       "      <td>-1.563181</td>\n",
       "      <td>-3.097790</td>\n",
       "      <td>...</td>\n",
       "      <td>1.162350</td>\n",
       "      <td>-0.149500</td>\n",
       "      <td>-2.235868</td>\n",
       "      <td>-2.265504</td>\n",
       "      <td>-0.179609</td>\n",
       "      <td>0.061992</td>\n",
       "      <td>9.921799</td>\n",
       "      <td>106.50</td>\n",
       "      <td>106.55</td>\n",
       "      <td>101.95</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 814 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  시가지수(포인트)  고가지수(포인트)  저가지수(포인트)  종가지수(포인트)    수익률(%)  \\\n",
       "0   2000-01-31  -0.218225  -0.493106  -0.400439  -0.223565  0.212872   \n",
       "1   2000-02-01  -0.213200  -0.230417  -0.373954  -0.595373 -0.407635   \n",
       "2   2000-02-02  -0.208520  -0.448886  -0.367787  -0.200398  0.696677   \n",
       "3   2000-02-03  -0.204125  -0.216603  -0.062403  -0.021445  0.366314   \n",
       "4   2000-02-07  -0.200000   0.378008   0.426748   0.592789  0.949260   \n",
       "5   2000-02-08  -0.196110   0.353748   0.677387   0.254527 -0.366454   \n",
       "6   2000-02-09  -0.192447   0.306497   0.819824   0.658684  0.641075   \n",
       "7   2000-02-10   0.330923   0.617187   0.552762   0.366383 -0.311193   \n",
       "8   2000-02-11   0.977388   0.576306   0.048738  -0.013448 -0.428351   \n",
       "9   2000-02-14  -0.649462  -0.806483  -0.954529  -1.222378 -1.533955   \n",
       "10  2000-02-15  -1.074607  -1.359121  -1.825901  -1.959771 -1.104848   \n",
       "11  2000-02-16  -1.939741  -2.072462  -2.426875  -1.820694  0.171634   \n",
       "12  2000-02-17  -2.062374  -1.576333  -1.627765  -1.281362  0.934836   \n",
       "13  2000-02-18  -1.203728  -1.119424  -1.312861  -1.662198 -0.622664   \n",
       "14  2000-02-21  -2.011534  -2.360013  -2.117703  -2.493118 -1.243495   \n",
       "15  2000-02-22  -2.301061  -2.151107  -2.119718  -2.142064  0.422501   \n",
       "16  2000-02-23  -1.670887  -1.656883  -1.569320  -1.267786  1.605126   \n",
       "17  2000-02-24  -1.366238  -1.422324  -1.358331  -1.490032 -0.528189   \n",
       "18  2000-02-25  -1.680249  -1.665735  -1.510077  -1.461250  0.033092   \n",
       "19  2000-02-28  -1.697353  -2.045428  -2.011407  -2.307633 -1.966604   \n",
       "\n",
       "    수익률 (1주)(%)  수익률 (1개월)(%)  수익률 (3개월)(%)  수익률 (6개월)(%)  ...  \\\n",
       "0      1.352924     -0.851790     -0.382658     -0.162615  ...   \n",
       "1      1.808646     -1.074203     -1.707405     -0.490731  ...   \n",
       "2      2.107257     -0.769928     -1.558400      0.074389  ...   \n",
       "3      1.488978     -0.625155     -1.268930     -0.079967  ...   \n",
       "4      1.101574      1.281908     -1.288431      1.273730  ...   \n",
       "5      1.138729      0.998930     -1.329397      0.942469  ...   \n",
       "6      1.076862      1.233568     -1.637689      0.994675  ...   \n",
       "7      0.651653      0.275024     -1.607973      0.269679  ...   \n",
       "8      0.326747      0.131530     -1.828185      0.546650  ...   \n",
       "9     -1.217328     -0.075927     -2.486079      0.140712  ...   \n",
       "10    -1.601324     -0.717085     -2.587555     -0.669097  ...   \n",
       "11    -1.786449     -0.703717     -2.563710     -0.401469  ...   \n",
       "12    -1.133411     -0.976304     -1.810490      1.006196  ...   \n",
       "13    -1.227625     -1.271035     -1.891012      0.605513  ...   \n",
       "14    -1.039810     -0.905187     -2.286667     -0.665131  ...   \n",
       "15    -0.214320     -0.740215     -1.957417     -0.444290  ...   \n",
       "16     0.506204      0.050075     -1.494619     -0.699133  ...   \n",
       "17    -0.294080     -0.319013     -1.472892     -1.661528  ...   \n",
       "18     0.075897      0.493437     -1.430882     -2.424417  ...   \n",
       "19    -0.289909     -1.729787     -1.563181     -3.097790  ...   \n",
       "\n",
       "    주요상품선물_천연가스(선물, NYMEX)($/mmBtu)  주요상품선물_금(선물)($/ounce)  \\\n",
       "0                          1.653438              -0.853010   \n",
       "1                          1.687061              -0.039689   \n",
       "2                          1.809524               0.800355   \n",
       "3                          1.213642               1.515992   \n",
       "4                          0.689954               3.856851   \n",
       "5                          0.332052               2.657684   \n",
       "6                          0.557674               3.026664   \n",
       "7                          0.812787               3.316674   \n",
       "8                          0.675096               2.384344   \n",
       "9                          0.504701               1.935983   \n",
       "10                         0.904886               1.222183   \n",
       "11                         0.590265               1.251580   \n",
       "12                         1.129172               1.099126   \n",
       "13                         0.911884               1.361637   \n",
       "14                         0.863101               1.274963   \n",
       "15                         0.121219               1.230409   \n",
       "16                         0.160356               0.708148   \n",
       "17                         0.228028               0.530452   \n",
       "18                         0.568840              -0.081697   \n",
       "19                         1.162350              -0.149500   \n",
       "\n",
       "    주요상품선물_은(선물)($/ounce)  주요상품선물_알루미늄(선물)($/ton)  주요상품선물_옥수수(최근월물)(￠/bu)  \\\n",
       "0                0.974513                0.827681                0.373581   \n",
       "1                0.193571                1.002058                0.594935   \n",
       "2                0.031266                0.069626                0.430159   \n",
       "3                0.623370                0.378121                0.084769   \n",
       "4                0.568635               -0.322023                0.518339   \n",
       "5                1.222974               -0.720402                0.474125   \n",
       "6                2.114708               -0.407511                0.622228   \n",
       "7                1.871659               -0.157431                0.985272   \n",
       "8                0.988933               -0.154629                0.473525   \n",
       "9                0.085505               -0.246361                0.529501   \n",
       "10              -0.051403               -1.024949                0.518408   \n",
       "11               0.487841               -1.605690                0.609070   \n",
       "12               0.014284               -1.379601                0.356659   \n",
       "13               0.227203               -1.264041                0.245730   \n",
       "14               0.268968               -1.589226                0.185194   \n",
       "15               0.364772               -1.172751               -0.103243   \n",
       "16              -0.052186               -0.980503               -1.014657   \n",
       "17              -1.233068               -0.941193               -0.976769   \n",
       "18              -2.561532               -1.090127                0.174516   \n",
       "19              -2.235868               -2.265504               -0.179609   \n",
       "\n",
       "    대두박(￠/bu)        종가      시가      고가      저가  \n",
       "0    0.614542 -0.961538  116.55  120.10  116.65  \n",
       "1    0.706918  3.381014  119.35  121.70  115.35  \n",
       "2    0.725768  3.292528  120.40  119.70  115.95  \n",
       "3    0.352875  1.677149  122.90  120.25  116.80  \n",
       "4    0.702038  0.653862  122.40  123.45  119.75  \n",
       "5    0.440887  1.319588  120.85  123.60  120.95  \n",
       "6    0.721716 -1.867641  123.60  124.10  121.85  \n",
       "7    1.379493 -5.575906  118.85  125.05  120.40  \n",
       "8    0.907189 -8.274721  116.45  124.75  119.85  \n",
       "9    0.923675 -1.422414  112.05  119.00  115.25  \n",
       "10   1.160291  1.623816  112.35  116.80  110.45  \n",
       "11   0.911960 -2.186270  113.35  114.35  105.25  \n",
       "12   1.165770 -5.281846  109.80  113.55  109.95  \n",
       "13   1.381693 -4.112651  106.85  115.70  111.15  \n",
       "14   1.319954  3.608247  108.55  109.80  105.95  \n",
       "15   1.100675  1.491841  110.20  109.15  105.50  \n",
       "16   0.126165 -1.492537  107.75  110.85  107.10  \n",
       "17   0.385420 -6.017455  105.95  111.60  107.35  \n",
       "18  -0.148781 -5.463728  104.15  109.85  106.50  \n",
       "19   0.061992  9.921799  106.50  106.55  101.95  \n",
       "\n",
       "[20 rows x 814 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start_index = max(df.loc[df['date']<=train_start].index) + time_interval*(n_timestep-1) - 1\n",
    "train_end_index = max(df.loc[df['date']<=train_end].index)\n",
    "base_prices = tf.reduce_mean(df.loc[train_start_index:train_end_index+1, '시가'])  \n",
    "\n",
    "@tf.function\n",
    "def loss_fn_model1_1(targets, preds):\n",
    " \n",
    "    loss0 = tf.keras.losses.MSE(targets, preds)\n",
    "    \n",
    "    preds = tf.reshape(preds[:, -1, :], [-1])\n",
    "    targets = tf.reshape(targets[:, -1, :], [-1])\n",
    "    \n",
    "   \n",
    "    if alpha != 0:\n",
    "        # add RRL cost - maximize downside sharp ratio\n",
    "\n",
    "        # 1 if (pred - base) * (target - base) > 0, -1 otherwise\n",
    "        F = tf.math.sign(targets*preds)\n",
    "        F = tf.reshape(F, [-1])\n",
    "\n",
    "        # calc returns from each step in batches\n",
    "        R = tf.math.divide(tf.math.multiply(tf.math.abs(targets), (F - 0.00003)), base_prices)\n",
    "        R = tf.reshape(R, [-1])\n",
    "\n",
    "        # calc downside sharp ratio\n",
    "\n",
    "        # downside returns\n",
    "        DR = tf.minimum(0.0, R)\n",
    "        DR = tf.reshape(DR, [-1])\n",
    "\n",
    "        # calc. downside sharp ratio\n",
    "        #s = []\n",
    "        #for i in range(batch_size):\n",
    "        #   std =  tf.keras.backend.std(DR[i, :, 0])\n",
    "        #   s.append(tf.reduce_mean(R[i, :, 0])/tf.maximum(0.01, std))\n",
    "\n",
    "        # calc. downside sharp ratio\n",
    "        loss1 = tf.reduce_mean(R) / (tf.keras.backend.std(DR) + 0.001)\n",
    "    else:\n",
    "        loss1 = 0\n",
    "\n",
    "    \"\"\"\n",
    "    # average profits, loss\n",
    "    avg_plusR = [0.0]\n",
    "    avg_minusR = [0.0]\n",
    "\n",
    "    global num_of_profits\n",
    "    global num_of_losses\n",
    "\n",
    "    num_of_profits = 0\n",
    "    num_of_losses = 0\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        res = tf.cond(R[i, num_steps - 1, 0] > 0, lambda: return_one(), lambda: return_zero())\n",
    "        if res == 1:\n",
    "            avg_plusR.append(R[i, num_steps - 1, 0])\n",
    "        else:\n",
    "            avg_minusR.append(R[i, num_steps - 1, 0])\n",
    "    avg_profit = tf.reduce_mean(avg_plusR) \n",
    "    avg_loss = tf.reduce_mean(avg_minusR) \n",
    "    \"\"\"\n",
    "\n",
    "    if beta != 0:\n",
    "        #compute maximum drawdown\n",
    "\n",
    "        #accm_profit = [0.0]\n",
    "        #for i in range(batch_size):\n",
    "        #    for j in range(num_steps):\n",
    "        #        r = tf.cond((predict_prices[i, num_steps-1, 0] - base_prices[i, num_steps-1, 0]) *\n",
    "        #                   (target_prices[i, num_steps-1, 0] - base_prices[i, num_steps-1, 0]) > 0,\n",
    "        #                   lambda: return_one(),\n",
    "        #                   lambda: return_zero())\n",
    "        #        if r == 1: accm_profit.append(accm_profit[i*num_steps + j] + tf.abs(target_prices[i, j, 0] - base_prices[i, j, 0]))\n",
    "        #        else:      accm_profit.append(accm_profit[i*num_steps + j] - tf.abs(target_prices[i, j, 0] - base_prices[i, j, 0]))\n",
    "\n",
    "        accm_profit = [0.0 for i in range(batch_size)]\n",
    "        for i in range(batch_size):\n",
    "            if i == 0:\n",
    "                accm_profit[0] = tf.sign(preds * targets) * tf.math.abs(targets)\n",
    "            else:\n",
    "                accm_profit[i] = accm_profit[i-1] + tf.sign(preds * targets) * tf.math.abs(targets)\n",
    "        loss2 = (tf.reduce_max(accm_profit) - tf.reduce_min(accm_profit))/batch_size\n",
    "    else:\n",
    "        loss2 = 0\n",
    "\n",
    "    return loss0 + beta*loss2 - alpha*loss1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def loss_fn_model1(train_y, preds):\n",
    "    \n",
    "    #preds = tf.cast(preds, dtype=tf.float64)\n",
    "    #train_y = tf.cast(train_y, dtype=tf.float64)\n",
    "    rates = train_y / 100\n",
    "    updown = tf.math.sign(train_y)\n",
    "    profits = 1 + rates*tf.math.sign(preds)\n",
    "    \n",
    "    loss1 = keras.losses.MSE(train_y, preds)\n",
    "    \n",
    "    batches = tf.shape(train_y)[0]\n",
    "\n",
    "    return_plus = 0.000000000\n",
    "    return_minus = 0.000000000\n",
    "    for i in range(batches):\n",
    "        if profits[i, -1, 0] - 1 > 0.0: return_plus += profits[i, -1, 0]-1      \n",
    "        else: return_minus += 1-profits[i, -1, 0]    \n",
    "\n",
    "    loss2 = (return_plus - return_minus)/tf.cast(batches, dtype=tf.float32)\n",
    "    \n",
    "    return loss1 - loss2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accus = []\n",
    "predictions = []\n",
    "\n",
    "def objective_function(args):\n",
    "    \n",
    "    print(\"실험 시작\\nargs: {}\".format(args))\n",
    "    print(datetime.now().strftime(\"%Y-%m-%d:%H:%M:%S\"))\n",
    "    n_timestep = int(args[0, 0])\n",
    "    time_interval = int(args[0, 1])\n",
    "    n_units = int(args[0, 2])\n",
    "    batch_size = int(args[0, 3])\n",
    "    epochs = int(args[0, 4])\n",
    "    \"\"\"\n",
    "    model1 = tf.keras.Sequential([\n",
    "        tf.keras.Input(shape=(n_timestep, input_size)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(n_units, return_sequences=True, kernel_initializer='he_normal', kernel_regularizer=keras.regularizers.l2(0.01))),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(int(n_units/2), return_sequences=True, kernel_initializer='he_normal', kernel_regularizer=keras.regularizers.l2(0.01))),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "    \"\"\"\n",
    "\n",
    "    accu = np.zeros(2)\n",
    "    \n",
    "    for k in range(2):\n",
    "        \n",
    "        # 모델 생성\n",
    "        model = models.LSTM(n_timestep,input_size,n_unit,regularizers_alpha=0.01,drop_rate=0.5)\n",
    "\n",
    "        model.compile(optimizer='adam',\n",
    "                      loss=loss_fn_model1)\n",
    "                          #callbacks=[cp-callback]\n",
    "                      #metrics=['accuracy'])\n",
    "        model.save_weights(checkpoint_path)            \n",
    "\n",
    "        # 초기 train, test start-end date 설정\n",
    "        current_train_start = df.loc[prepro.date_to_index(df, train_start), 'date']\n",
    "        current_train_end = df.loc[prepro.date_to_index(df, train_end), 'date']\n",
    "        current_test_start = df.loc[prepro.date_to_index(df, test_start), 'date']\n",
    "        current_test_end = df.loc[prepro.date_to_index(df, test_start) + trans_day - 1, 'date']\n",
    "\n",
    "\n",
    "        #  각 transfer 구간의 예측값들을 합치기 위하여\n",
    "        test_prediction1 = []\n",
    "\n",
    "        while True:\n",
    "\n",
    "            gc.collect()\n",
    "\n",
    "            # train, test data 분리 생성 - 불칠요한 column 제거, 재배치\n",
    "            train_data, test_data = prepro.get_train_test_data(df, target_column, remove_columns, \n",
    "                                                                   current_train_start, current_train_end,\n",
    "                                                                   current_test_start, current_test_end,\n",
    "                                                                   future_day, n_timestep, time_interval)\n",
    "\n",
    "            # 전체 train, test dataset 생성\n",
    "            train_x, train_y = prepro.get_LSTM_dataset(train_data, n_timestep, time_interval, input_size, future_day)\n",
    "            \n",
    "            # the model1 training dataset\n",
    "            train_x = train_x[:train_end_back]\n",
    "            train_y = train_y[:train_end_back]\n",
    "\n",
    "            # random split\n",
    "            train_x, test_x, train_y, test_y = train_test_split(train_x, train_y, test_size=0.5)\n",
    "            \n",
    "            # test set 생성\n",
    "            test_x, test_y = prepro.get_LSTM_dataset(test_data, n_timestep, time_interval, input_size, future_day)\n",
    "            test_y_target = np.expand_dims(test_y[:, :, 3], axis=2)            \n",
    "\n",
    "            # 전체 dataset training\n",
    "            train_y_target = np.expand_dims(train_y[:, :, 3], axis=2)     \n",
    "            early_stopping1 = tf.keras.callbacks.EarlyStopping(patience=2, verbose=0)        \n",
    "            model.load_weights(checkpoint_path)\n",
    "            model.fit(train_x, train_y_target, verbose=0, batch_size=batch_size, epochs=epochs, callbacks=[early_stopping1], validation_data=(test_x, test_y_target))\n",
    "            model.save_weights(checkpoint_path)\n",
    "\n",
    "\n",
    "            #print('test dates ' + current_test_start + \"~\" + current_test_end)\n",
    "\n",
    "            # prediction1 accuracy\n",
    "            prediction1 = model.predict(test_x)[:, -1, 0].reshape(-1)\n",
    "            test_prediction1.append(prediction1)\n",
    "\n",
    "\n",
    "            if current_test_end == test_end:\n",
    "             break\n",
    "\n",
    "            #train, start dates shift\n",
    "            current_train_end = df.loc[prepro.date_to_index(df, current_train_end) + trans_day, 'date']\n",
    "            current_train_start = df.loc[prepro.date_to_index(df, current_train_end) - 2000, 'date']\n",
    "            current_test_start = df.loc[prepro.date_to_index(df, current_test_start) + trans_day, 'date']\n",
    "            if prepro.date_to_index(df, test_end) - prepro.date_to_index(df, current_test_start) < trans_day:\n",
    "                current_test_end = test_end\n",
    "            else:\n",
    "                current_test_end = df.loc[prepro.date_to_index(df, current_test_end) + trans_day, 'date']\n",
    "        #end while\n",
    "        \n",
    "        t = np.concatenate(test_prediction1) \n",
    "        if k == 0: t0 = t.copy()\n",
    "            \n",
    "        # 전체 test_oouput 생성\n",
    "        _, test_data = prepro.get_train_test_data(df, target_column, remove_columns,\n",
    "                                                           train_start, train_end,\n",
    "                                                           test_start, test_end,\n",
    "                                                           future_day, n_timestep, time_interval)\n",
    "        _, test_y = prepro.get_LSTM_dataset(test_data, n_timestep, time_interval, input_size, future_day)\n",
    "\n",
    "        # test accuracy 계산\n",
    "        test_start_index = df.loc[df['date'] <= test_start].index.max()\n",
    "        test_end_index = df.loc[df['date'] <= test_end].index.max()\n",
    "        test_open_prices = np.array(list(map(float, df.loc[test_start_index - future_day: test_end_index - future_day, '시가'])))\n",
    "        test_base_prices = np.array(list(map(float, dataframe.loc[test_start_index - future_day: test_end_index - future_day, '종가'])))\n",
    "        test_output_prices = np.array(list(map(float, dataframe.loc[test_start_index: test_end_index, '종가'])))   \n",
    "        test_predict_prices = test_base_prices * (np.array(t)/100 + 1)\n",
    "\n",
    "\n",
    "        # 예측확률 계산\n",
    "        cnt = 0\n",
    "        for i in range(len(test_predict_prices)):\n",
    "            if (test_predict_prices[i]-test_open_prices[i])*(test_output_prices[i]-test_open_prices[i]) > 0:\n",
    "                cnt += 1\n",
    "        accu[k] = cnt/len(test_predict_prices)\n",
    "        \n",
    "        print(\"실험 하나 종료! 결과\\nargs: {}, acc: {}, {}\".format(args, accu[k], k))\n",
    "        print(datetime.now().strftime(\"%Y-%m-%d:%H:%M:%S\"))\n",
    "    #end for\n",
    "    \n",
    "    accus.append(max(accu[0], accu[1]))\n",
    "    if accu[0] > accu[1]: predictions.append(t0)\n",
    "    else: predictions.append(t)\n",
    "    \n",
    "    return 1 - max(accu[0], accu[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실험 시작\n",
      "args: [[ 40.   2. 500.  10.   3.]]\n",
      "2020-06-27:01:28:41\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-f597ecdfdc9c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0maccus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmyBopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBayesianOptimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobjective_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_design_numdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mmyBopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_optimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mmyBopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_acquisition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/GPyOpt/methods/bayesian_optimization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, domain, constraints, cost_withGradients, model_type, X, Y, initial_design_numdata, initial_design_type, acquisition_type, normalize_Y, exact_feval, acquisition_optimizer_type, model_update_interval, evaluator_type, batch_size, num_cores, verbosity, verbosity_model, maximize, de_duplication, **kwargs)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_design_type\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0minitial_design_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_design_numdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_design_numdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_design_chooser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# --- CHOOSE the model type. If an instance of a GPyOpt model is passed (possibly user defined), it is used.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/GPyOpt/methods/bayesian_optimization.py\u001b[0m in \u001b[0;36m_init_design_chooser\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitial_design\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_design_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_design_numdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         \u001b[0;31m# Case 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mY\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/GPyOpt/core/task/objective.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_procs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mf_evals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcost_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eval_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/GPyOpt/core/task/objective.py\u001b[0m in \u001b[0;36m_eval_func\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mst_time\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mrlt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mf_evals\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf_evals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrlt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mcost_evals\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mst_time\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-596975a1daf4>\u001b[0m in \u001b[0;36mobjective_function\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mearly_stopping1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#!pip install GPyOpt\n",
    "from GPyOpt.methods import BayesianOptimization\n",
    "domain = [{'name': 'n_timestep', 'type': 'discrete', 'domain': (10, 15, 20, 25, 30, 35, 40, 45), 'dimensionality': 1},\n",
    "          {'name': 'time_interval', 'type': 'discrete', 'domain': (1, 2, 3), 'dimensionality': 1},\n",
    "          {'name': 'n_unit', 'type': 'discrete', 'domain': (128, 256, 500, 1024), 'dimensionality': 1},\n",
    "          {'name': 'batch_size', 'type': 'discrete', 'domain': (10, 15, 20, 25, 30, 35, 40, 45, 50), 'dimensionality': 1},\n",
    "          {'name': 'epochs', 'type': 'discrete', 'domain': (3, 5, 7, 10), 'dimensionality': 1},]\n",
    "accus = []\n",
    "predictions = []\n",
    "myBopt = BayesianOptimization(f=objective_function, domain=domain, initial_design_numdata=5)\n",
    "myBopt.run_optimization(max_iter=15)\n",
    "myBopt.plot_acquisition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(myBopt.x_opt)  # lr_rate, #layer, hidden_size, batch_size\n",
    "\n",
    "n_timestep = myBopt.x_opt[0]\n",
    "time_interval = myBopt.x_opt[1]\n",
    "n_unit = myBopt.x_opt[2]\n",
    "batch_sixe = myBopt.x_opt[3]\n",
    "\n",
    "N, _ = myBopt.X.shape\n",
    "for i in range(N):\n",
    "    if np.array_equal(myBopt.x_opt, myBopt.X[i, :]):\n",
    "        print(1 - myBopt.Y[i,:])  # accuracy 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_accu_idx = np.argmax(accus)\n",
    "t = predictions[best_accu_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "future_day = int(future_day)\n",
    "n_timestep = int(n_timestep)\n",
    "time_interval = int(time_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_prediction = np.concatenate(test_prediction, axis=0).reshape((-1, n_timestep, 1))\n",
    "#train_prediction = learn.predict_batch_test(model1, train_x[:batch_size], len(train_x[:batch_size]))\n",
    "\n",
    "# 원 지수, 날짜는 target conversion이 되기 전 dataframe에서 가져옴\n",
    "test_dates, test_base_prices, train_dates, train_base_prices = prepro.get_test_dates_prices(dataframe, test_start, test_end,\n",
    "                                                      train_start, train_end, n_timestep, time_interval, future_day, target_column)\n",
    "\n",
    "# 전체 test_oouput 생성\n",
    "_, test_data = prepro.get_train_test_data(df, target_column, remove_columns,\n",
    "                                                   train_start, train_end,\n",
    "                                                   test_start, test_end,\n",
    "                                                   future_day, n_timestep, time_interval)\n",
    "_, test_y = prepro.get_LSTM_dataset(test_data, n_timestep, time_interval, input_size, future_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updown = np.sign(test_y[:, -1, 3]).reshape((-1))\n",
    "#calculate accuracy\n",
    "temp = tf.math.multiply(updown, t.reshape((-1)))\n",
    "accu = tf.reduce_sum(list(map(lambda x: 1 if x > 0 else 0, temp)))/len(temp)\n",
    "print('accuracy = ', accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updown.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_base_prices = train_base_prices[:batch_size]\n",
    "#train_prediction = train_prediction[:batch_size]\n",
    "#train_y = train_y[:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = GenerateResult(t, t, test_y[:, -1, 3].reshape(-1), test_dates, n_timestep, future_day, train_end_back, trans_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start_index = prepro.date_to_index(df, test_start)\n",
    "test_end_index = prepro.date_to_index(df, test_end)\n",
    "test_open_prices = list(map(float, df.loc[test_start_index - future_day: test_end_index - future_day, '시가']))\n",
    "test_high_prices = list(map(float, df.loc[test_start_index - future_day: test_end_index - future_day, '고가']))\n",
    "test_low_prices = list(map(float, df.loc[test_start_index - future_day: test_end_index - future_day, '저가']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result.extract_last_output()\n",
    "result.convert_price(test_base_prices,conversion_type=target_type)\n",
    "# 손익 계산\n",
    "profits = np.zeros(len(test_dates))\n",
    "for i in range(len(test_dates)):\n",
    "    if result.test_predict_price[i]-test_open_prices[i] > 0:\n",
    "        profits[i] = result.test_output_price[i] - test_open_prices[i]\n",
    "    else:\n",
    "        profits[i] = test_open_prices[i] - result.test_output_price[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result.extract_last_output()\n",
    "result.convert_price(test_base_prices,conversion_type=target_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import operator\n",
    "preds = np.array(result.test_predict_price) - np.array(test_open_prices)\n",
    "trues = np.array(result.test_output_price) - np.array(test_open_prices)\n",
    "result.evaluation(preds, trues)\n",
    "result.table(test_open_prices, test_high_prices, test_low_prices, profits, profits)\n",
    "result.save_result(model_name,item_name,n_unit,target_type,batch_size,n_timestep,time_interval,epochs,str(alpha),comment)\n",
    "result.save_visualization()\n",
    "#result.save_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow)",
   "language": "python",
   "name": "conda_tensorflow"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
