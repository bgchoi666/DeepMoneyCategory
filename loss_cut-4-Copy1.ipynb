{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 Bimghi Choi. All Rights Reserved.\n",
    "# 예측 + 투자전략 시스템\n",
    "\n",
    "# _*_ coding: utf-8 _*_\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import os\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]='0, 1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import preprocess as prepro\n",
    "import models\n",
    "import learn\n",
    "from learn import GenerateResult\n",
    "import math\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # 텐서플로가 첫 번째 GPU에 1GB 메모리만 할당하도록 제한\n",
    "  try:\n",
    "#    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')    \n",
    "    tf.config.experimental.set_visible_devices(gpus[1], 'GPU')    \n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[1],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])\n",
    "  except RuntimeError as e:\n",
    "    # 프로그램 시작시에 가장 장치가 설정되어야만 합니다\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bayesian opt로 최적화된 parameter\n",
    "학습기간 2000-01-31 ~ 2017-01-02\n",
    "test기간 2017-01-02 ~ 2029-05-15\n",
    "\n",
    "2일 예측을 1일 예측으로 사용\n",
    "\n",
    "steps 30, interval 1, units 1024, batch size 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '~/Data/kospi200f_809_0515.csv'\n",
    "item_name = 'kospi200f_809_0515'\n",
    "train_start = '2000-01-31'\n",
    "train_end = '2017-01-02'\n",
    "test_start = '2017-01-02'\n",
    "test_end = '2020-05-15'\n",
    "\n",
    "remove_columns = ['date', '종가']\n",
    "target_column = '종가'\n",
    "input_columns = []\n",
    "target_type = 'rate'\n",
    "\n",
    "model_name = 'loss_cut-4'\n",
    "channel = False\n",
    "\n",
    "trans_day = 1\n",
    "\n",
    "target_alpha = 100\n",
    "future_day = 2\n",
    "train_end_back = -1\n",
    "n_timestep = 30\n",
    "time_interval = 1\n",
    "input_size = 809\n",
    "n_unit = 1024\n",
    "batch_size = 10\n",
    "learning_rate = 0.0005\n",
    "epochs = 2000\n",
    "\n",
    "alpha = 0.05\n",
    "beta = 0.01\n",
    "\n",
    "comment = \"2일 에측을 위해 1일 예측 학습, trainset을 2개로 분리, 예측 모델, 손절 모델 차례로 학습후 전체 trainset로 예측 모델 다시 학습\"\n",
    "\n",
    "from datetime import datetime\n",
    "now = datetime.now().strftime('%Y-%m-%d-%H:%M:%S')\n",
    "checkpoint_path = []\n",
    "for i in range(4):\n",
    "    checkpoint_path.append(model_name + \"/pred\"+str(future_day)+\":\"+now+\"-\"+str(i)+\".ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = util.read_datafile(file_name)\n",
    "df = dataframe.copy()\n",
    "df['close'] = df[target_column]\n",
    "\n",
    "#df = prepro.target_conversion(df, target_column, future_day, type=target_type)\n",
    "a = []\n",
    "b = []\n",
    "for i in range(len(df[target_column]) -future_day):\n",
    "    df.loc[i, target_column] = ((df.loc[i + future_day, target_column] - df.loc[i, target_column]) \n",
    "                                / df.loc[i, target_column]) * target_alpha \n",
    "    df.loc[i, '시가'] = df.loc[i + future_day + train_end_back + 1, '시가']\n",
    "    a.append(max(df.loc[i  + future_day + train_end_back + 1:i + future_day + 1, '고가']))\n",
    "    b.append(min(df.loc[i  + future_day + train_end_back + 1:i + future_day + 1, '저가']))\n",
    "for i in range(len(df['종가']) - future_day):\n",
    "    df.loc[i, '고가'] = a[i]\n",
    "    df.loc[i, '저가'] = b[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>시가지수(포인트)</th>\n",
       "      <th>고가지수(포인트)</th>\n",
       "      <th>저가지수(포인트)</th>\n",
       "      <th>종가지수(포인트)</th>\n",
       "      <th>수익률(%)</th>\n",
       "      <th>수익률 (1주)(%)</th>\n",
       "      <th>수익률 (1개월)(%)</th>\n",
       "      <th>수익률 (3개월)(%)</th>\n",
       "      <th>수익률 (6개월)(%)</th>\n",
       "      <th>...</th>\n",
       "      <th>주요상품선물_금(선물)($/ounce)</th>\n",
       "      <th>주요상품선물_은(선물)($/ounce)</th>\n",
       "      <th>주요상품선물_알루미늄(선물)($/ton)</th>\n",
       "      <th>주요상품선물_옥수수(최근월물)(￠/bu)</th>\n",
       "      <th>대두박(￠/bu)</th>\n",
       "      <th>종가</th>\n",
       "      <th>시가</th>\n",
       "      <th>고가</th>\n",
       "      <th>저가</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-31</td>\n",
       "      <td>-0.218225</td>\n",
       "      <td>-0.493106</td>\n",
       "      <td>-0.400439</td>\n",
       "      <td>-0.223565</td>\n",
       "      <td>0.212872</td>\n",
       "      <td>1.352924</td>\n",
       "      <td>-0.851790</td>\n",
       "      <td>-0.382658</td>\n",
       "      <td>-0.162615</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.853010</td>\n",
       "      <td>0.974513</td>\n",
       "      <td>0.827681</td>\n",
       "      <td>0.373581</td>\n",
       "      <td>0.614542</td>\n",
       "      <td>-0.961538</td>\n",
       "      <td>116.55</td>\n",
       "      <td>120.25</td>\n",
       "      <td>115.95</td>\n",
       "      <td>119.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-02-01</td>\n",
       "      <td>-0.213200</td>\n",
       "      <td>-0.230417</td>\n",
       "      <td>-0.373954</td>\n",
       "      <td>-0.595373</td>\n",
       "      <td>-0.407635</td>\n",
       "      <td>1.808646</td>\n",
       "      <td>-1.074203</td>\n",
       "      <td>-1.707405</td>\n",
       "      <td>-0.490731</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039689</td>\n",
       "      <td>0.193571</td>\n",
       "      <td>1.002058</td>\n",
       "      <td>0.594935</td>\n",
       "      <td>0.706918</td>\n",
       "      <td>3.381014</td>\n",
       "      <td>119.35</td>\n",
       "      <td>123.45</td>\n",
       "      <td>116.80</td>\n",
       "      <td>115.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-02-02</td>\n",
       "      <td>-0.208520</td>\n",
       "      <td>-0.448886</td>\n",
       "      <td>-0.367787</td>\n",
       "      <td>-0.200398</td>\n",
       "      <td>0.696677</td>\n",
       "      <td>2.107257</td>\n",
       "      <td>-0.769928</td>\n",
       "      <td>-1.558400</td>\n",
       "      <td>0.074389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.800355</td>\n",
       "      <td>0.031266</td>\n",
       "      <td>0.069626</td>\n",
       "      <td>0.430159</td>\n",
       "      <td>0.725768</td>\n",
       "      <td>3.292528</td>\n",
       "      <td>120.40</td>\n",
       "      <td>123.60</td>\n",
       "      <td>119.75</td>\n",
       "      <td>118.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-02-03</td>\n",
       "      <td>-0.204125</td>\n",
       "      <td>-0.216603</td>\n",
       "      <td>-0.062403</td>\n",
       "      <td>-0.021445</td>\n",
       "      <td>0.366314</td>\n",
       "      <td>1.488978</td>\n",
       "      <td>-0.625155</td>\n",
       "      <td>-1.268930</td>\n",
       "      <td>-0.079967</td>\n",
       "      <td>...</td>\n",
       "      <td>1.515992</td>\n",
       "      <td>0.623370</td>\n",
       "      <td>0.378121</td>\n",
       "      <td>0.084769</td>\n",
       "      <td>0.352875</td>\n",
       "      <td>1.677149</td>\n",
       "      <td>122.90</td>\n",
       "      <td>124.10</td>\n",
       "      <td>120.95</td>\n",
       "      <td>119.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-02-07</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.378008</td>\n",
       "      <td>0.426748</td>\n",
       "      <td>0.592789</td>\n",
       "      <td>0.949260</td>\n",
       "      <td>1.101574</td>\n",
       "      <td>1.281908</td>\n",
       "      <td>-1.288431</td>\n",
       "      <td>1.273730</td>\n",
       "      <td>...</td>\n",
       "      <td>3.856851</td>\n",
       "      <td>0.568635</td>\n",
       "      <td>-0.322023</td>\n",
       "      <td>0.518339</td>\n",
       "      <td>0.702038</td>\n",
       "      <td>0.653862</td>\n",
       "      <td>122.40</td>\n",
       "      <td>125.05</td>\n",
       "      <td>120.40</td>\n",
       "      <td>122.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2000-02-08</td>\n",
       "      <td>-0.196110</td>\n",
       "      <td>0.353748</td>\n",
       "      <td>0.677387</td>\n",
       "      <td>0.254527</td>\n",
       "      <td>-0.366454</td>\n",
       "      <td>1.138729</td>\n",
       "      <td>0.998930</td>\n",
       "      <td>-1.329397</td>\n",
       "      <td>0.942469</td>\n",
       "      <td>...</td>\n",
       "      <td>2.657684</td>\n",
       "      <td>1.222974</td>\n",
       "      <td>-0.720402</td>\n",
       "      <td>0.474125</td>\n",
       "      <td>0.440887</td>\n",
       "      <td>1.319588</td>\n",
       "      <td>120.85</td>\n",
       "      <td>125.05</td>\n",
       "      <td>119.85</td>\n",
       "      <td>121.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2000-02-09</td>\n",
       "      <td>-0.192447</td>\n",
       "      <td>0.306497</td>\n",
       "      <td>0.819824</td>\n",
       "      <td>0.658684</td>\n",
       "      <td>0.641075</td>\n",
       "      <td>1.076862</td>\n",
       "      <td>1.233568</td>\n",
       "      <td>-1.637689</td>\n",
       "      <td>0.994675</td>\n",
       "      <td>...</td>\n",
       "      <td>3.026664</td>\n",
       "      <td>2.114708</td>\n",
       "      <td>-0.407511</td>\n",
       "      <td>0.622228</td>\n",
       "      <td>0.721716</td>\n",
       "      <td>-1.867641</td>\n",
       "      <td>123.60</td>\n",
       "      <td>124.75</td>\n",
       "      <td>115.25</td>\n",
       "      <td>123.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000-02-10</td>\n",
       "      <td>0.330923</td>\n",
       "      <td>0.617187</td>\n",
       "      <td>0.552762</td>\n",
       "      <td>0.366383</td>\n",
       "      <td>-0.311193</td>\n",
       "      <td>0.651653</td>\n",
       "      <td>0.275024</td>\n",
       "      <td>-1.607973</td>\n",
       "      <td>0.269679</td>\n",
       "      <td>...</td>\n",
       "      <td>3.316674</td>\n",
       "      <td>1.871659</td>\n",
       "      <td>-0.157431</td>\n",
       "      <td>0.985272</td>\n",
       "      <td>1.379493</td>\n",
       "      <td>-5.575906</td>\n",
       "      <td>118.85</td>\n",
       "      <td>119.00</td>\n",
       "      <td>110.45</td>\n",
       "      <td>122.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2000-02-11</td>\n",
       "      <td>0.977388</td>\n",
       "      <td>0.576306</td>\n",
       "      <td>0.048738</td>\n",
       "      <td>-0.013448</td>\n",
       "      <td>-0.428351</td>\n",
       "      <td>0.326747</td>\n",
       "      <td>0.131530</td>\n",
       "      <td>-1.828185</td>\n",
       "      <td>0.546650</td>\n",
       "      <td>...</td>\n",
       "      <td>2.384344</td>\n",
       "      <td>0.988933</td>\n",
       "      <td>-0.154629</td>\n",
       "      <td>0.473525</td>\n",
       "      <td>0.907189</td>\n",
       "      <td>-8.274721</td>\n",
       "      <td>116.45</td>\n",
       "      <td>116.80</td>\n",
       "      <td>105.25</td>\n",
       "      <td>120.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2000-02-14</td>\n",
       "      <td>-0.649462</td>\n",
       "      <td>-0.806483</td>\n",
       "      <td>-0.954529</td>\n",
       "      <td>-1.222378</td>\n",
       "      <td>-1.533955</td>\n",
       "      <td>-1.217328</td>\n",
       "      <td>-0.075927</td>\n",
       "      <td>-2.486079</td>\n",
       "      <td>0.140712</td>\n",
       "      <td>...</td>\n",
       "      <td>1.935983</td>\n",
       "      <td>0.085505</td>\n",
       "      <td>-0.246361</td>\n",
       "      <td>0.529501</td>\n",
       "      <td>0.923675</td>\n",
       "      <td>-1.422414</td>\n",
       "      <td>112.05</td>\n",
       "      <td>114.35</td>\n",
       "      <td>105.25</td>\n",
       "      <td>116.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2000-02-15</td>\n",
       "      <td>-1.074607</td>\n",
       "      <td>-1.359121</td>\n",
       "      <td>-1.825901</td>\n",
       "      <td>-1.959771</td>\n",
       "      <td>-1.104848</td>\n",
       "      <td>-1.601324</td>\n",
       "      <td>-0.717085</td>\n",
       "      <td>-2.587555</td>\n",
       "      <td>-0.669097</td>\n",
       "      <td>...</td>\n",
       "      <td>1.222183</td>\n",
       "      <td>-0.051403</td>\n",
       "      <td>-1.024949</td>\n",
       "      <td>0.518408</td>\n",
       "      <td>1.160291</td>\n",
       "      <td>1.623816</td>\n",
       "      <td>112.35</td>\n",
       "      <td>115.70</td>\n",
       "      <td>109.95</td>\n",
       "      <td>110.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2000-02-16</td>\n",
       "      <td>-1.939741</td>\n",
       "      <td>-2.072462</td>\n",
       "      <td>-2.426875</td>\n",
       "      <td>-1.820694</td>\n",
       "      <td>0.171634</td>\n",
       "      <td>-1.786449</td>\n",
       "      <td>-0.703717</td>\n",
       "      <td>-2.563710</td>\n",
       "      <td>-0.401469</td>\n",
       "      <td>...</td>\n",
       "      <td>1.251580</td>\n",
       "      <td>0.487841</td>\n",
       "      <td>-1.605690</td>\n",
       "      <td>0.609070</td>\n",
       "      <td>0.911960</td>\n",
       "      <td>-2.186270</td>\n",
       "      <td>113.35</td>\n",
       "      <td>115.70</td>\n",
       "      <td>105.95</td>\n",
       "      <td>114.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2000-02-17</td>\n",
       "      <td>-2.062374</td>\n",
       "      <td>-1.576333</td>\n",
       "      <td>-1.627765</td>\n",
       "      <td>-1.281362</td>\n",
       "      <td>0.934836</td>\n",
       "      <td>-1.133411</td>\n",
       "      <td>-0.976304</td>\n",
       "      <td>-1.810490</td>\n",
       "      <td>1.006196</td>\n",
       "      <td>...</td>\n",
       "      <td>1.099126</td>\n",
       "      <td>0.014284</td>\n",
       "      <td>-1.379601</td>\n",
       "      <td>0.356659</td>\n",
       "      <td>1.165770</td>\n",
       "      <td>-5.281846</td>\n",
       "      <td>109.80</td>\n",
       "      <td>109.80</td>\n",
       "      <td>105.50</td>\n",
       "      <td>112.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2000-02-18</td>\n",
       "      <td>-1.203728</td>\n",
       "      <td>-1.119424</td>\n",
       "      <td>-1.312861</td>\n",
       "      <td>-1.662198</td>\n",
       "      <td>-0.622664</td>\n",
       "      <td>-1.227625</td>\n",
       "      <td>-1.271035</td>\n",
       "      <td>-1.891012</td>\n",
       "      <td>0.605513</td>\n",
       "      <td>...</td>\n",
       "      <td>1.361637</td>\n",
       "      <td>0.227203</td>\n",
       "      <td>-1.264041</td>\n",
       "      <td>0.245730</td>\n",
       "      <td>1.381693</td>\n",
       "      <td>-4.112651</td>\n",
       "      <td>106.85</td>\n",
       "      <td>110.85</td>\n",
       "      <td>105.50</td>\n",
       "      <td>111.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2000-02-21</td>\n",
       "      <td>-2.011534</td>\n",
       "      <td>-2.360013</td>\n",
       "      <td>-2.117703</td>\n",
       "      <td>-2.493118</td>\n",
       "      <td>-1.243495</td>\n",
       "      <td>-1.039810</td>\n",
       "      <td>-0.905187</td>\n",
       "      <td>-2.286667</td>\n",
       "      <td>-0.665131</td>\n",
       "      <td>...</td>\n",
       "      <td>1.274963</td>\n",
       "      <td>0.268968</td>\n",
       "      <td>-1.589226</td>\n",
       "      <td>0.185194</td>\n",
       "      <td>1.319954</td>\n",
       "      <td>3.608247</td>\n",
       "      <td>108.55</td>\n",
       "      <td>111.60</td>\n",
       "      <td>107.10</td>\n",
       "      <td>106.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2000-02-22</td>\n",
       "      <td>-2.301061</td>\n",
       "      <td>-2.151107</td>\n",
       "      <td>-2.119718</td>\n",
       "      <td>-2.142064</td>\n",
       "      <td>0.422501</td>\n",
       "      <td>-0.214320</td>\n",
       "      <td>-0.740215</td>\n",
       "      <td>-1.957417</td>\n",
       "      <td>-0.444290</td>\n",
       "      <td>...</td>\n",
       "      <td>1.230409</td>\n",
       "      <td>0.364772</td>\n",
       "      <td>-1.172751</td>\n",
       "      <td>-0.103243</td>\n",
       "      <td>1.100675</td>\n",
       "      <td>1.491841</td>\n",
       "      <td>110.20</td>\n",
       "      <td>111.60</td>\n",
       "      <td>106.50</td>\n",
       "      <td>107.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2000-02-23</td>\n",
       "      <td>-1.670887</td>\n",
       "      <td>-1.656883</td>\n",
       "      <td>-1.569320</td>\n",
       "      <td>-1.267786</td>\n",
       "      <td>1.605126</td>\n",
       "      <td>0.506204</td>\n",
       "      <td>0.050075</td>\n",
       "      <td>-1.494619</td>\n",
       "      <td>-0.699133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.708148</td>\n",
       "      <td>-0.052186</td>\n",
       "      <td>-0.980503</td>\n",
       "      <td>-1.014657</td>\n",
       "      <td>0.126165</td>\n",
       "      <td>-1.492537</td>\n",
       "      <td>107.75</td>\n",
       "      <td>109.85</td>\n",
       "      <td>101.95</td>\n",
       "      <td>110.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2000-02-24</td>\n",
       "      <td>-1.366238</td>\n",
       "      <td>-1.422324</td>\n",
       "      <td>-1.358331</td>\n",
       "      <td>-1.490032</td>\n",
       "      <td>-0.528189</td>\n",
       "      <td>-0.294080</td>\n",
       "      <td>-0.319013</td>\n",
       "      <td>-1.472892</td>\n",
       "      <td>-1.661528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.530452</td>\n",
       "      <td>-1.233068</td>\n",
       "      <td>-0.941193</td>\n",
       "      <td>-0.976769</td>\n",
       "      <td>0.385420</td>\n",
       "      <td>-6.017455</td>\n",
       "      <td>105.95</td>\n",
       "      <td>106.55</td>\n",
       "      <td>100.75</td>\n",
       "      <td>108.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2000-02-25</td>\n",
       "      <td>-1.680249</td>\n",
       "      <td>-1.665735</td>\n",
       "      <td>-1.510077</td>\n",
       "      <td>-1.461250</td>\n",
       "      <td>0.033092</td>\n",
       "      <td>0.075897</td>\n",
       "      <td>0.493437</td>\n",
       "      <td>-1.430882</td>\n",
       "      <td>-2.424417</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081697</td>\n",
       "      <td>-2.561532</td>\n",
       "      <td>-1.090127</td>\n",
       "      <td>0.174516</td>\n",
       "      <td>-0.148781</td>\n",
       "      <td>-5.463728</td>\n",
       "      <td>104.15</td>\n",
       "      <td>113.00</td>\n",
       "      <td>100.75</td>\n",
       "      <td>108.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2000-02-28</td>\n",
       "      <td>-1.697353</td>\n",
       "      <td>-2.045428</td>\n",
       "      <td>-2.011407</td>\n",
       "      <td>-2.307633</td>\n",
       "      <td>-1.966604</td>\n",
       "      <td>-0.289909</td>\n",
       "      <td>-1.729787</td>\n",
       "      <td>-1.563181</td>\n",
       "      <td>-3.097790</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.149500</td>\n",
       "      <td>-2.235868</td>\n",
       "      <td>-2.265504</td>\n",
       "      <td>-0.179609</td>\n",
       "      <td>0.061992</td>\n",
       "      <td>9.921799</td>\n",
       "      <td>106.50</td>\n",
       "      <td>113.75</td>\n",
       "      <td>106.35</td>\n",
       "      <td>102.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 815 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  시가지수(포인트)  고가지수(포인트)  저가지수(포인트)  종가지수(포인트)    수익률(%)  \\\n",
       "0   2000-01-31  -0.218225  -0.493106  -0.400439  -0.223565  0.212872   \n",
       "1   2000-02-01  -0.213200  -0.230417  -0.373954  -0.595373 -0.407635   \n",
       "2   2000-02-02  -0.208520  -0.448886  -0.367787  -0.200398  0.696677   \n",
       "3   2000-02-03  -0.204125  -0.216603  -0.062403  -0.021445  0.366314   \n",
       "4   2000-02-07  -0.200000   0.378008   0.426748   0.592789  0.949260   \n",
       "5   2000-02-08  -0.196110   0.353748   0.677387   0.254527 -0.366454   \n",
       "6   2000-02-09  -0.192447   0.306497   0.819824   0.658684  0.641075   \n",
       "7   2000-02-10   0.330923   0.617187   0.552762   0.366383 -0.311193   \n",
       "8   2000-02-11   0.977388   0.576306   0.048738  -0.013448 -0.428351   \n",
       "9   2000-02-14  -0.649462  -0.806483  -0.954529  -1.222378 -1.533955   \n",
       "10  2000-02-15  -1.074607  -1.359121  -1.825901  -1.959771 -1.104848   \n",
       "11  2000-02-16  -1.939741  -2.072462  -2.426875  -1.820694  0.171634   \n",
       "12  2000-02-17  -2.062374  -1.576333  -1.627765  -1.281362  0.934836   \n",
       "13  2000-02-18  -1.203728  -1.119424  -1.312861  -1.662198 -0.622664   \n",
       "14  2000-02-21  -2.011534  -2.360013  -2.117703  -2.493118 -1.243495   \n",
       "15  2000-02-22  -2.301061  -2.151107  -2.119718  -2.142064  0.422501   \n",
       "16  2000-02-23  -1.670887  -1.656883  -1.569320  -1.267786  1.605126   \n",
       "17  2000-02-24  -1.366238  -1.422324  -1.358331  -1.490032 -0.528189   \n",
       "18  2000-02-25  -1.680249  -1.665735  -1.510077  -1.461250  0.033092   \n",
       "19  2000-02-28  -1.697353  -2.045428  -2.011407  -2.307633 -1.966604   \n",
       "\n",
       "    수익률 (1주)(%)  수익률 (1개월)(%)  수익률 (3개월)(%)  수익률 (6개월)(%)  ...  \\\n",
       "0      1.352924     -0.851790     -0.382658     -0.162615  ...   \n",
       "1      1.808646     -1.074203     -1.707405     -0.490731  ...   \n",
       "2      2.107257     -0.769928     -1.558400      0.074389  ...   \n",
       "3      1.488978     -0.625155     -1.268930     -0.079967  ...   \n",
       "4      1.101574      1.281908     -1.288431      1.273730  ...   \n",
       "5      1.138729      0.998930     -1.329397      0.942469  ...   \n",
       "6      1.076862      1.233568     -1.637689      0.994675  ...   \n",
       "7      0.651653      0.275024     -1.607973      0.269679  ...   \n",
       "8      0.326747      0.131530     -1.828185      0.546650  ...   \n",
       "9     -1.217328     -0.075927     -2.486079      0.140712  ...   \n",
       "10    -1.601324     -0.717085     -2.587555     -0.669097  ...   \n",
       "11    -1.786449     -0.703717     -2.563710     -0.401469  ...   \n",
       "12    -1.133411     -0.976304     -1.810490      1.006196  ...   \n",
       "13    -1.227625     -1.271035     -1.891012      0.605513  ...   \n",
       "14    -1.039810     -0.905187     -2.286667     -0.665131  ...   \n",
       "15    -0.214320     -0.740215     -1.957417     -0.444290  ...   \n",
       "16     0.506204      0.050075     -1.494619     -0.699133  ...   \n",
       "17    -0.294080     -0.319013     -1.472892     -1.661528  ...   \n",
       "18     0.075897      0.493437     -1.430882     -2.424417  ...   \n",
       "19    -0.289909     -1.729787     -1.563181     -3.097790  ...   \n",
       "\n",
       "    주요상품선물_금(선물)($/ounce)  주요상품선물_은(선물)($/ounce)  주요상품선물_알루미늄(선물)($/ton)  \\\n",
       "0               -0.853010               0.974513                0.827681   \n",
       "1               -0.039689               0.193571                1.002058   \n",
       "2                0.800355               0.031266                0.069626   \n",
       "3                1.515992               0.623370                0.378121   \n",
       "4                3.856851               0.568635               -0.322023   \n",
       "5                2.657684               1.222974               -0.720402   \n",
       "6                3.026664               2.114708               -0.407511   \n",
       "7                3.316674               1.871659               -0.157431   \n",
       "8                2.384344               0.988933               -0.154629   \n",
       "9                1.935983               0.085505               -0.246361   \n",
       "10               1.222183              -0.051403               -1.024949   \n",
       "11               1.251580               0.487841               -1.605690   \n",
       "12               1.099126               0.014284               -1.379601   \n",
       "13               1.361637               0.227203               -1.264041   \n",
       "14               1.274963               0.268968               -1.589226   \n",
       "15               1.230409               0.364772               -1.172751   \n",
       "16               0.708148              -0.052186               -0.980503   \n",
       "17               0.530452              -1.233068               -0.941193   \n",
       "18              -0.081697              -2.561532               -1.090127   \n",
       "19              -0.149500              -2.235868               -2.265504   \n",
       "\n",
       "    주요상품선물_옥수수(최근월물)(￠/bu)  대두박(￠/bu)        종가      시가      고가      저가  \\\n",
       "0                 0.373581   0.614542 -0.961538  116.55  120.25  115.95   \n",
       "1                 0.594935   0.706918  3.381014  119.35  123.45  116.80   \n",
       "2                 0.430159   0.725768  3.292528  120.40  123.60  119.75   \n",
       "3                 0.084769   0.352875  1.677149  122.90  124.10  120.95   \n",
       "4                 0.518339   0.702038  0.653862  122.40  125.05  120.40   \n",
       "5                 0.474125   0.440887  1.319588  120.85  125.05  119.85   \n",
       "6                 0.622228   0.721716 -1.867641  123.60  124.75  115.25   \n",
       "7                 0.985272   1.379493 -5.575906  118.85  119.00  110.45   \n",
       "8                 0.473525   0.907189 -8.274721  116.45  116.80  105.25   \n",
       "9                 0.529501   0.923675 -1.422414  112.05  114.35  105.25   \n",
       "10                0.518408   1.160291  1.623816  112.35  115.70  109.95   \n",
       "11                0.609070   0.911960 -2.186270  113.35  115.70  105.95   \n",
       "12                0.356659   1.165770 -5.281846  109.80  109.80  105.50   \n",
       "13                0.245730   1.381693 -4.112651  106.85  110.85  105.50   \n",
       "14                0.185194   1.319954  3.608247  108.55  111.60  107.10   \n",
       "15               -0.103243   1.100675  1.491841  110.20  111.60  106.50   \n",
       "16               -1.014657   0.126165 -1.492537  107.75  109.85  101.95   \n",
       "17               -0.976769   0.385420 -6.017455  105.95  106.55  100.75   \n",
       "18                0.174516  -0.148781 -5.463728  104.15  113.00  100.75   \n",
       "19               -0.179609   0.061992  9.921799  106.50  113.75  106.35   \n",
       "\n",
       "     close  \n",
       "0   119.60  \n",
       "1   115.35  \n",
       "2   118.45  \n",
       "3   119.25  \n",
       "4   122.35  \n",
       "5   121.25  \n",
       "6   123.15  \n",
       "7   122.85  \n",
       "8   120.85  \n",
       "9   116.00  \n",
       "10  110.85  \n",
       "11  114.35  \n",
       "12  112.65  \n",
       "13  111.85  \n",
       "14  106.70  \n",
       "15  107.25  \n",
       "16  110.55  \n",
       "17  108.85  \n",
       "18  108.90  \n",
       "19  102.30  \n",
       "\n",
       "[20 rows x 815 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', '시가지수(포인트)', '고가지수(포인트)', '저가지수(포인트)', '종가지수(포인트)', '수익률(%)',\n",
       "       '수익률 (1주)(%)', '수익률 (1개월)(%)', '수익률 (3개월)(%)', '수익률 (6개월)(%)',\n",
       "       ...\n",
       "       '주요상품선물_금(선물)($/ounce)', '주요상품선물_은(선물)($/ounce)',\n",
       "       '주요상품선물_알루미늄(선물)($/ton)', '주요상품선물_옥수수(최근월물)(￠/bu)', '대두박(￠/bu)', '종가',\n",
       "       '시가', '고가', '저가', 'close'],\n",
       "      dtype='object', length=815)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_list = df.columns\n",
    "column_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas_profiling\n",
    "#data = df.loc[:, ['미국 SP 500 Index(종가)(Pt)', '거래량(주)', '외국인보유비중(%)', '종가']]\n",
    "#data = data[:-future_day]\n",
    "#pr = data.profile_report()\n",
    "#pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start_index = max(df.loc[df['date']<=train_start].index) + time_interval*(n_timestep-1) - 1\n",
    "train_end_index = max(df.loc[df['date']<=train_end].index)\n",
    "base_prices = tf.reduce_mean(df.loc[train_start_index:train_end_index+1, '시가'])  \n",
    "\n",
    "@tf.function\n",
    "def loss_fn_model1_1(targets, preds):\n",
    " \n",
    "    loss0 = tf.keras.losses.MSE(targets, preds)\n",
    "    \n",
    "    preds = tf.reshape(preds[:, n_timestep-1, :], [-1])\n",
    "    targets = tf.reshape(targets[:, n_timestep-1, :], [-1])\n",
    "    \n",
    "   \n",
    "    if alpha != 0:\n",
    "        # add RRL cost - maximize downside sharp ratio\n",
    "\n",
    "        # 1 if (pred - base) * (target - base) > 0, -1 otherwise\n",
    "        F = tf.math.sign(targets*preds)\n",
    "        F = tf.reshape(F, [-1])\n",
    "\n",
    "        # calc returns from each step in batches\n",
    "        R = tf.math.divide(tf.math.multiply(tf.math.abs(targets), (F - 0.00003)), base_prices)\n",
    "        R = tf.reshape(R, [-1])\n",
    "\n",
    "        # calc downside sharp ratio\n",
    "\n",
    "        # downside returns\n",
    "        DR = tf.minimum(0.0, R)\n",
    "        DR = tf.reshape(DR, [-1])\n",
    "\n",
    "        # calc. downside sharp ratio\n",
    "        #s = []\n",
    "        #for i in range(batch_size):\n",
    "        #   std =  tf.keras.backend.std(DR[i, :, 0])\n",
    "        #   s.append(tf.reduce_mean(R[i, :, 0])/tf.maximum(0.01, std))\n",
    "\n",
    "        # calc. downside sharp ratio\n",
    "        loss1 = tf.reduce_mean(R) / (tf.keras.backend.std(DR) + 0.001)\n",
    "    else:\n",
    "        loss1 = 0\n",
    "\n",
    "    \"\"\"\n",
    "    # average profits, loss\n",
    "    avg_plusR = [0.0]\n",
    "    avg_minusR = [0.0]\n",
    "\n",
    "    global num_of_profits\n",
    "    global num_of_losses\n",
    "\n",
    "    num_of_profits = 0\n",
    "    num_of_losses = 0\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        res = tf.cond(R[i, num_steps - 1, 0] > 0, lambda: return_one(), lambda: return_zero())\n",
    "        if res == 1:\n",
    "            avg_plusR.append(R[i, num_steps - 1, 0])\n",
    "        else:\n",
    "            avg_minusR.append(R[i, num_steps - 1, 0])\n",
    "    avg_profit = tf.reduce_mean(avg_plusR) \n",
    "    avg_loss = tf.reduce_mean(avg_minusR) \n",
    "    \"\"\"\n",
    "\n",
    "    if beta != 0:\n",
    "        #compute maximum drawdown\n",
    "\n",
    "        #accm_profit = [0.0]\n",
    "        #for i in range(batch_size):\n",
    "        #    for j in range(num_steps):\n",
    "        #        r = tf.cond((predict_prices[i, num_steps-1, 0] - base_prices[i, num_steps-1, 0]) *\n",
    "        #                   (target_prices[i, num_steps-1, 0] - base_prices[i, num_steps-1, 0]) > 0,\n",
    "        #                   lambda: return_one(),\n",
    "        #                   lambda: return_zero())\n",
    "        #        if r == 1: accm_profit.append(accm_profit[i*num_steps + j] + tf.abs(target_prices[i, j, 0] - base_prices[i, j, 0]))\n",
    "        #        else:      accm_profit.append(accm_profit[i*num_steps + j] - tf.abs(target_prices[i, j, 0] - base_prices[i, j, 0]))\n",
    "\n",
    "        accm_profit = [0.0 for i in range(batch_size)]\n",
    "        for i in range(batch_size):\n",
    "            if i == 0:\n",
    "                accm_profit[0] = tf.sign(preds * targets) * tf.math.abs(targets)\n",
    "            else:\n",
    "                accm_profit[i] = accm_profit[i-1] + tf.sign(preds * targets) * tf.math.abs(targets)\n",
    "        loss2 = (tf.reduce_max(accm_profit) - tf.reduce_min(accm_profit))/batch_size\n",
    "    else:\n",
    "        loss2 = 0\n",
    "\n",
    "    return loss0 + beta*loss2 - alpha*loss1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "손절이익이 최대가 되도록 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def loss_fn_model2(m1, m2, train_x, train_y):\n",
    "    \n",
    "    train_y_target = np.expand_dims(train_y[:, :, 4], axis=2)\n",
    "    \n",
    "    rates = train_y_target / 100\n",
    "    updown = tf.math.sign(train_y_target)\n",
    "    preds = tf.cast(tf.math.sign(m1(train_x)), dtype=tf.float64)\n",
    "    profits = tf.cast(1 + tf.convert_to_tensor(rates, dtype=tf.float64)*preds, dtype=tf.float64)\n",
    "    \n",
    "    batches = tf.cast(train_y.shape[0], dtype=tf.int32)\n",
    "    steps = train_y.shape[1]     \n",
    "    \n",
    "    # 시가, 고가, 저가\n",
    "    train_open = np.expand_dims(train_y[:, :, 0], axis=2)   \n",
    "    train_high = np.expand_dims(train_y[:, :, 1], axis=2)\n",
    "    train_low = np.expand_dims(train_y[:, :, 2], axis=2)\n",
    "    train_base = np.expand_dims(train_y[:, :, 3], axis=2)\n",
    "    train_real = train_base * (rates + 1)\n",
    "    \n",
    "    targets = train_y_target.copy()\n",
    "    targets[:, -max(1, int(future_day/time_interval)):, :] = 0\n",
    "    train_x_m2 = tf.concat([preds, targets, train_open, train_high, train_low], 2)\n",
    "    loss_cuts = tf.cast(tf.reshape(m2(train_x_m2)[:, -1, 0], [-1]), dtype=tf.float64)\n",
    "    \n",
    "    # 손절 손익 계산\n",
    "    loss_cut_targets = []\n",
    "    for i in range(batches):\n",
    "\n",
    "        if preds[i, -1, 0] > 0:\n",
    "            if rates[i, -1, 0] > 0: \n",
    "                loss_cut_targets.append(train_open[i, -1, 0] - train_low[i, -1, 0])\n",
    "            else:\n",
    "                loss_cut_targets.append((train_base[i, -1, 0] - train_real[i, -1, 0])*0.5)\n",
    "        else:\n",
    "            if rates[i, -1, 0] > 0: \n",
    "                loss_cut_targets.append((train_real[i, -1, 0] - train_base[i, -1, 0])*0.5)\n",
    "            else:\n",
    "                loss_cut_targets.append(train_high[i, -1, 0] - train_open[i, -1, 0])\n",
    "                    \n",
    "    return tf.keras.losses.MSE(loss_cuts, loss_cut_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def gradient1(model1, targets, preds):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn_model1_1(targets, preds)\n",
    "    return tape.gradient(loss, model1.trainable_variables)\n",
    "#@tf.function\n",
    "def gradient2(model1, model2, input_data, output_data):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn_model2(model1, model2, input_data, output_data)\n",
    "    return tape.gradient(loss, model2.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_cut-4/pred2:2020-06-17-02:23:34-0.ckpt\n",
      "loss_cut-4/pred2:2020-06-17-02:23:34-1.ckpt\n",
      "loss_cut-4/pred2:2020-06-17-02:23:34-2.ckpt\n",
      "loss_cut-4/pred2:2020-06-17-02:23:34-3.ckpt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#strategy = tf.distribute.MirroredStrategy()\n",
    "#with strategy.scope():\n",
    "\"\"\"\n",
    "model1 = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(n_timestep, input_size)),\n",
    "    tf.keras.layers.LSTM(n_unit, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(n_unit, return_sequences=True),    \n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model1.summary()\n",
    "\"\"\"\n",
    "model2 = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(n_timestep, 5)),\n",
    "    tf.keras.layers.LSTM(10, return_sequences=True, kernel_initializer='he_normal', kernel_regularizer=keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "#model2.summary()\n",
    "\n",
    "mo = [models.LSTM(n_timestep,input_size,n_unit,regularizers_alpha=0.01,drop_rate=0.5) for i in range(4)]\n",
    "#model2 = models.LSTM(n_timestep,2,10,regularizers_alpha=0.01,drop_rate=0.5)\n",
    "\n",
    "    #cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    #    checkpoint_path, verbose=1, save_weights_only=True,\n",
    "        # 다섯 번째 에포크마다 가중치를 저장합니다\n",
    "    #    save_freq=5)\n",
    "\n",
    "for i in range(4):\n",
    "    mo[i].compile(optimizer='adam',\n",
    "                  loss='mse')\n",
    "                      #callbacks=[cp-callback]\n",
    "                  #metrics=['accuracy'])\n",
    "    mo[i].save_weights(checkpoint_path[i]) \n",
    "    print(checkpoint_path[i])\n",
    "\n",
    "#    model2 = models.LSTM(n_timestep,input_size,n_unit,regularizers_alpha=0.01,drop_rate=0.5)\n",
    "#    model2.compile(optimizer='adam',\n",
    "#                  loss='mse')\n",
    "                  #callbacks=[cp-callback]\n",
    "              #metrics=['accuracy'])            \n",
    "\n",
    "#modle_name = model_name + \"tanh\"            \n",
    "#model2.save_weights(\"modle2_\"+ checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 시작\n",
      "2020-06-17:02:25:37\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process, Queue\n",
    "\n",
    "def work(idx, model, result):\n",
    "    current_train_start = df.loc[prepro.date_to_index(df, train_start), 'date']\n",
    "    current_train_end = df.loc[prepro.date_to_index(df, train_end), 'date']\n",
    "    current_test_start = df.loc[prepro.date_to_index(df, test_start), 'date']\n",
    "    current_test_end = df.loc[prepro.date_to_index(df, test_start) + trans_day - 1, 'date']\n",
    "\n",
    "\n",
    "    #  각 transfer 구간의 예측값들을 합치기 위하여\n",
    "    test_prediction1 = []\n",
    "    test_prediction2 = []\n",
    "    test_target = []\n",
    "\n",
    "    print(\"학습 시작\")\n",
    "    print(datetime.now().strftime(\"%Y-%m-%d:%H:%M:%S\"))\n",
    "    while True:\n",
    "\n",
    "        #gc.collect()\n",
    "\n",
    "        \n",
    "        train_data, test_data = prepro.get_train_test_data(df, target_column, remove_columns, \n",
    "                                                               current_train_start, current_train_end,\n",
    "                                                               current_test_start, current_test_end,\n",
    "                                                               future_day, n_timestep, time_interval)\n",
    "\n",
    "        # 전체 train, test dataset 생성\n",
    "        train_x, train_y = prepro.get_LSTM_dataset(train_data, n_timestep, time_interval, input_size, future_day)\n",
    "        train_x = train_x[:train_end_back]\n",
    "        train_y = train_y[:train_end_back]\n",
    "\n",
    "        test_x, test_y = prepro.get_LSTM_dataset(test_data, n_timestep, time_interval, input_size, future_day)\n",
    "        test_y_target = np.expand_dims(test_y[:, :, 4], axis=2)    \n",
    "\n",
    "        # train dataset를 random하게 4등분하여 하나만 사용\n",
    "        train_x, train2_x, train_y, train2_y = train_test_split(train_x, train_y, test_size=0.5)\n",
    "        train_x, train2_x, train_y, train2_y = train_test_split(train_x, train_y, test_size=0.5)\n",
    "        \n",
    "        \n",
    "        # the model2 training\n",
    "        train_y_target = np.expand_dims(train_y[:, :, 4], axis=2)\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)    \n",
    "        early_stopping2 = learn.EarlyStopping(patience=2, verbose=1)\n",
    "        iter = epochs\n",
    "        basic_epochs = tf.cast(epochs / 5, dtype=tf.int32)\n",
    "        for iteration in range(iter):\n",
    "            batch_input, batch_output = learn.next_random_batch(train_x, train_y_target, 10)\n",
    "\n",
    "            gradients1 = gradient1(model,  batch_output, model(batch_input))\n",
    "            optimizer.apply_gradients(zip(gradients1, model.trainable_variables))\n",
    "            if iteration % 100 == 0:\n",
    "\n",
    "                # model1의 test loss\n",
    "                loss1 = loss_fn_model1_1(test_y_target, model(test_x))\n",
    "                print('model loss =', loss1)  \n",
    "\n",
    "\n",
    "            if iteration > iter / 2 and early_stopping2.validate(loss2)==True:\n",
    "                break    \n",
    "\n",
    "        if iter > basic_epochs: iter -= basic_epochs\n",
    "        if iter < basic_epochs: iter = basic_epochs\n",
    "        \n",
    "      \n",
    "        print('test dates ' + current_test_start + \"~\" + current_test_end)\n",
    "\n",
    "        # prediction1 accuracy\n",
    "        updown = np.sign(test_y[:, -1, 4]).reshape((-1))\n",
    "        prediction1 = model.predict(test_x)[:, -1, 0].reshape(-1)\n",
    "        temp = tf.math.multiply(updown, prediction1)\n",
    "        accu = tf.reduce_sum(list(map(lambda x: 1 if x > 0 else 0, temp)))/len(temp)\n",
    "        print('prediction1 accuracy = ', accu)\n",
    "\n",
    "        test_prediction1.append(prediction1)\n",
    "        test_prediction2.append(prediction1)\n",
    "\n",
    "        # escape from while\n",
    "        if current_test_end == test_end:\n",
    "         break\n",
    "\n",
    "        #train, start dates shift\n",
    "        current_train_end = df.loc[prepro.date_to_index(df, current_train_end) + trans_day, 'date']\n",
    "        current_train_start = df.loc[prepro.date_to_index(df, current_train_end) - 1000, 'date']\n",
    "        current_test_start = df.loc[prepro.date_to_index(df, current_test_start) + trans_day, 'date']\n",
    "        if prepro.date_to_index(df, test_end) - prepro.date_to_index(df, current_test_start) < trans_day:\n",
    "            current_test_end = test_end\n",
    "        else:\n",
    "            current_test_end = df.loc[prepro.date_to_index(df, current_test_end) + trans_day, 'date']\n",
    "    print(\"학습 종료.......\")\n",
    "    print(datetime.now().strftime(\"%Y-%m-%d:%H:%M:%S\"))\n",
    "    \n",
    "    return result.put((idx, test_prediction1))\n",
    "\n",
    "    \n",
    "result = Queue()\n",
    "th1 = Process(target=work, args=(0, mo[0], result))\n",
    "#th2 = Process(target=work, args=(1, mo[1], result))\n",
    "#th3 = Process(target=work, args=(2, mo[2], result))\n",
    "#th4 = Process(target=work, args=(3, mo[3], result))\n",
    "\n",
    "th1.start()\n",
    "#th2.start()\n",
    "#th3.start()\n",
    "#th4.start()\n",
    "th1.join()\n",
    "#th2.join()\n",
    "#th3.join()\n",
    "#th4.join()\n",
    "\n",
    "test_preds = []\n",
    "#for i in range(4):\n",
    "#    tmp = result.get()\n",
    "#    test_preds.append(tmp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_prediction1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-1c98fc3ffb6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_prediction1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_prediction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_prediction1' is not defined"
     ]
    }
   ],
   "source": [
    "t1 = np.concatenate(test_prediction1)\n",
    "t2 = np.concatenate(test_prediction2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_prediction = np.concatenate(test_prediction, axis=0).reshape((-1, n_timestep, 1))\n",
    "#train_prediction = learn.predict_batch_test(model1, train_x[:batch_size], len(train_x[:batch_size]))\n",
    "\n",
    "# 원 지수, 날짜는 target conversion이 되기 전 dataframe에서 가져옴 - 종가를 test base price로 하는 경우\n",
    "test_dates, test_base_prices, train_dates, train_base_prices = prepro.get_test_dates_prices(dataframe, test_start, test_end,\n",
    "                                                      train_start, train_end, n_timestep, time_interval, future_day, target_column)\n",
    "\n",
    "\n",
    "# 전체 test_oouput 생성\n",
    "_, test_data = prepro.get_train_test_data(df, target_column, remove_columns,\n",
    "                                                   train_start, train_end,\n",
    "                                                   test_start, test_end,\n",
    "                                                   future_day, n_timestep, time_interval)\n",
    "_, test_y = prepro.get_LSTM_dataset(test_data, n_timestep, time_interval, input_size, future_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updown = np.sign(test_y[:, -1, 4]).reshape((-1))\n",
    "#calculate accuracy\n",
    "temp = tf.math.multiply(updown, t1.reshape((-1)))\n",
    "accu = tf.reduce_sum(list(map(lambda x: 1 if x > 0 else 0, temp)))/len(temp)\n",
    "print('accuracy = ', accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updown.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_base_prices = train_base_prices[:batch_size]\n",
    "#train_prediction = train_prediction[:batch_size]\n",
    "#train_y = train_y[:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = GenerateResult(t1, t2, test_y[:, -1, 4].reshape(-1), test_dates, n_timestep, future_day, train_end_back, trans_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start_index = prepro.date_to_index(df, test_start)\n",
    "test_end_index = prepro.date_to_index(df, test_end)\n",
    "test_open_prices = list(map(float, df.loc[test_start_index - future_day: test_end_index - future_day, '시가']))\n",
    "test_high_prices = list(map(float, df.loc[test_start_index - future_day: test_end_index - future_day, '고가']))\n",
    "test_low_prices = list(map(float, df.loc[test_start_index - future_day: test_end_index - future_day, '저가']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result.extract_last_output()\n",
    "result.convert_price(test_base_prices,conversion_type=target_type)\n",
    "# 손익 계산\n",
    "profits = np.zeros(len(test_dates))\n",
    "for i in range(len(test_dates)):\n",
    "    if result.test_predict_price[i]-test_open_prices[i] > 0:\n",
    "        profits[i] = result.test_output_price[i] - test_open_prices[i]\n",
    "    else:\n",
    "        profits[i] = test_open_prices[i] - result.test_output_price[i]\n",
    "# 손절 손익 계산\n",
    "profits2 = np.zeros(len(test_dates))\n",
    "for i in range(len(test_dates)):\n",
    "    if t2[i] <= 0: profits2[i] = profits[i]\n",
    "    else:\n",
    "        if result.test_predict_price[i] - test_open_prices[i] > 0:\n",
    "            if test_open_prices[i] - test_low_prices[i] > t2[i]:\n",
    "                profits2[i] = -t2[i]\n",
    "            else:\n",
    "                profits2[i] = profits[i]\n",
    "        else:\n",
    "            if test_high_prices[i] - test_open_prices[i] > t2[i]:\n",
    "                profits2[i] = -t2[i]\n",
    "            else:\n",
    "                profits2[i] = profits[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array(result.test_predict_price) - np.array(test_open_prices)\n",
    "trues = np.array(result.test_output_price) - np.array(test_open_prices)\n",
    "result.evaluation(preds, trues)\n",
    "result.table(test_open_prices, test_high_prices, test_low_prices, profits, profits2)\n",
    "result.save_result(model_name,item_name,n_unit,target_type,batch_size,n_timestep,time_interval,epochs,str(alpha),comment)\n",
    "result.save_visualization()\n",
    "result.save_model(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "current_train_start = df.loc[prepro.date_to_index(df, train_start), 'date']\n",
    "current_train_end = df.loc[prepro.date_to_index(df, train_end), 'date']\n",
    "current_test_start = df.loc[prepro.date_to_index(df, test_start), 'date']\n",
    "current_test_end = df.loc[prepro.date_to_index(df, test_start) + trans_day - 1, 'date']\n",
    "\n",
    "\n",
    "#  각 transfer 구간의 예측값들을 합치기 위하여\n",
    "test_prediction1 = []\n",
    "test_prediction2 = []\n",
    "test_target = []\n",
    "\n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(patience=3, verbose=1)\n",
    "early_stopping = learn.EarlyStopping(patience=2, verbose=1)\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "train_data, test_data = prepro.get_train_test_data(df, target_column, remove_columns, \n",
    "                                                       current_train_start, current_train_end,\n",
    "                                                       current_test_start, current_test_end,\n",
    "                                                       future_day, n_timestep, time_interval)\n",
    "\n",
    "# input_size, columns reset\n",
    "input_size = len(df.columns) - len(remove_columns)\n",
    "input_columns = df.columns.copy()\n",
    "\n",
    "train_x, train_y = prepro.get_LSTM_dataset(train_data, n_timestep, time_interval, input_size, future_day)\n",
    "test_x, test_y = prepro.get_LSTM_dataset(test_data, n_timestep, time_interval, input_size, future_day)\n",
    "\n",
    "#model.compile(optimizer='adam',\n",
    "#          loss=loss_fn)\n",
    "#          #callbacks=[cp-callback]\n",
    "#          #metrics=['accuracy'])\n",
    "\n",
    "# the firs training dataset\n",
    "train_x = train_x[:-future_day]\n",
    "train_y = train_y[:-future_day]    \n",
    "\n",
    "#global_step = tf.train.get_or_create_global_step()\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "#lr_decay = tf.train.exponential_decay(learning_rate, global_step,\n",
    "#                                      train_input.shape[0]/batch_size*5, 0.5, staircase=True)\n",
    "lr_decay = tf.compat.v1.train.exponential_decay(learning_rate,global_step, int(len(train_y)/batch_size), 0.96, staircase=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "#optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "updown = np.sign(test_y[:, -1, 0]).reshape((-1))    \n",
    "epochs = len(train_y)\n",
    "for iteration in range(399):\n",
    "    batch_input, batch_output = learn.next_random_interval_batch(train_x, train_y, batch_size, future_day)\n",
    "\n",
    "    #noise = 2*np.random.randn(batch_size,n_timestep,1)\n",
    "    #batch_output = batch_output+noise\n",
    "    #batch_input = encoder(train_input[idx])\n",
    "    gradients1 = gradient1(model1, model2, batch_input, batch_output)\n",
    "    optimizer.apply_gradients(zip(gradients1, model1.trainable_variables))\n",
    "    \n",
    "    targets = tf.reshape(train_y[:, -1, 0], [-1])\n",
    "    rates = targets / 100\n",
    "    preds = tf.reshape(model1(train_x, training=False)[:, -1, 0], [-1])\n",
    "    \n",
    "    n = len(targets)    \n",
    "    returns = [1.0]\n",
    "    losses = []\n",
    "    for i in range(n - 1):\n",
    "       \n",
    "        # average_return, std of returns, remaining days, preds[0] \n",
    "        state = []\n",
    "        \n",
    "        random_rates = []\n",
    "        for k in range(i+1):\n",
    "            random_rates.append(rates[k])\n",
    "        # 실재 수익률을 기반으로 random 수익률 생성, 예측에 의한 porfits 생성 \n",
    "        for k in range(i+1, n):\n",
    "            random_rates.append(tf.random.normal((), mean=targets[k] / 100, stddev=0.1, dtype=tf.float64))\n",
    "        profits = tf.convert_to_tensor(random_rates, dtype=tf.float64)*tf.cast(preds, dtype=tf.float64) + 1\n",
    "        \n",
    "        # 현재까지의 예측에 의한 수익률 기하평균 구하기\n",
    "        returns_past = []\n",
    "        for k in range(i+1):\n",
    "            returns_past.append(profits[k])\n",
    "        avg_return = tf.cast(tf.math.reduce_prod(returns_past)**(1/(i+1)), dtype=tf.float64)\n",
    "        \n",
    "        state.append(avg_return)\n",
    "        state.append(tf.math.reduce_std(returns))\n",
    "        state.append((n - i) / n)\n",
    "        state.append(preds[i])\n",
    "        state = np.array(state).reshape((1, 4))\n",
    "     \n",
    "        # 목표일까지의 기대 기하 평균 수익률 구하기 \n",
    "        returns_future = []\n",
    "        for j in range(i+1, n):\n",
    "            returns_future.append(profits[j])\n",
    "        avg_return_future = tf.math.reduce_prod(returns_future)**(1/(n-i-1))\n",
    "\n",
    "        # 예측 기하 평균 수익률과 기대 기하 평균 수익률의 MSE\n",
    "        value = tf.cast(model2(state, training=True)[0, 0], dtype=tf.float64)\n",
    "        losses.append((value - avg_return_future)**2)\n",
    "        if n == 3: break\n",
    "    print(\"losses\", losses)\n",
    "    print(\"value\", value)    \n",
    "    with tf.GradientTape() as tape:\n",
    "        gradients2 = tape.gradient(tf.math.reduce_sum(losses), model2.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients2, model2.trainable_variables))\n",
    "    \n",
    "\n",
    "    \n",
    "    if iteration % 100 == 0:\n",
    "        #test_MSE = model.evaluate(test_x, test_y)\n",
    "        prediction = model1.predict(test_x)\n",
    "        prediction_MSE = sum((updown - np.sign(prediction[:, -1, 0]).reshape(-1))**2)/len(test_y)\n",
    "        print('prediction_MSE =', prediction_MSE)\n",
    "\n",
    "    if iteration > epochs / 2 and early_stopping.validate(prediction_MSE)==True:\n",
    "        break\n",
    "epochs -= epochs / 5\n",
    "if epochs <= 0: epochs = 100\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow)",
   "language": "python",
   "name": "conda_tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
