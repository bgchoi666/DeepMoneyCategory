{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of preprocessing failed: Traceback (most recent call last):\n",
      "  File \"/home/sunggil/anaconda3/envs/tf1/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/sunggil/anaconda3/envs/tf1/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 450, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/home/sunggil/anaconda3/envs/tf1/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 387, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/sunggil/anaconda3/envs/tf1/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 357, in update_class\n",
      "    update_instances(old, new)\n",
      "  File \"/home/sunggil/anaconda3/envs/tf1/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 312, in update_instances\n",
      "    update_instances(old, new, obj.__dict__, visited)\n",
      "  File \"/home/sunggil/anaconda3/envs/tf1/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 317, in update_instances\n",
      "    update_instances(old, new, obj, visited)\n",
      "  File \"/home/sunggil/anaconda3/envs/tf1/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 317, in update_instances\n",
      "    update_instances(old, new, obj, visited)\n",
      "  File \"/home/sunggil/anaconda3/envs/tf1/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 312, in update_instances\n",
      "    update_instances(old, new, obj.__dict__, visited)\n",
      "  File \"/home/sunggil/anaconda3/envs/tf1/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 317, in update_instances\n",
      "    update_instances(old, new, obj, visited)\n",
      "  File \"/home/sunggil/anaconda3/envs/tf1/lib/python3.7/site-packages/IPython/extensions/autoreload.py\", line 312, in update_instances\n",
      "    update_instances(old, new, obj.__dict__, visited)\n",
      "AttributeError: 'dict' object has no attribute 'iterkeys'\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "from postprocessing import restore_future_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunggil/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sunggil/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sunggil/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sunggil/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sunggil/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sunggil/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/sunggil/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/sunggil/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/sunggil/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/sunggil/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/sunggil/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/sunggil/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocessing\n",
    "from preprocessing import data_load\n",
    "from preprocessing import remove_dummy_columns\n",
    "from preprocessing import corss_train_test_split\n",
    "from preprocessing import create_time_table_X\n",
    "from preprocessing import CreateTarget\n",
    "from preprocessing import next_batch\n",
    "from preprocessing import load_important_variables\n",
    "from preprocessing import load_important_train_test\n",
    "from preprocessing import load_timeseries_tensor_train_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valuation(path, confu_mat,MSE):\n",
    "    recall = round(confus_mat[0][0]/sum(confus_mat[:,0]),3)\n",
    "    precision = round(confus_mat[0][0]/sum(confus_mat[0]),3)\n",
    "    accu = round((confus_mat[0][0]+confus_mat[1][1])/sum(sum(confus_mat)),3)\n",
    "    \n",
    "    wb = load_workbook(file_path+file_name, data_only=True)\n",
    "    sheet1 = wb.active\n",
    "    \n",
    "    sheet1.cell(2,7,'TRUE')\n",
    "    sheet1.cell(3,7,'0')\n",
    "    sheet1.cell(3,8,'1')\n",
    "\n",
    "    sheet1.cell(3,6,'PREDICTION')\n",
    "    sheet1.cell(4,6,'0')\n",
    "    sheet1.cell(5,6,'1')\n",
    "\n",
    "    sheet1.cell(4,7,confus_mat[0][0])\n",
    "    sheet1.cell(4,8,confus_mat[0][1])\n",
    "    sheet1.cell(5,7,confus_mat[1][0])\n",
    "    sheet1.cell(5,8,confus_mat[1][1])\n",
    "\n",
    "\n",
    "    sheet1.cell(3,9,'accuracy')\n",
    "    sheet1.cell(3,10,accu)\n",
    "\n",
    "    sheet1.cell(4,9,'precision(하락)')\n",
    "    sheet1.cell(4,10,precision)\n",
    "\n",
    "    sheet1.cell(5,9,'recall(하락)')\n",
    "    sheet1.cell(5,10,recall)\n",
    "\n",
    "    sheet1.cell(6,9,'MSE')\n",
    "    sheet1.cell(6,10,MSE)\n",
    "    \n",
    "    wb.save(file_path+file_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_data_4D_part(training_part,part, target_name, timestep, future_day):\n",
    "    train = training_part[part][0]\n",
    "    test = training_part[part][1]\n",
    "    \n",
    "    train_price = np.array(train[target_name])\n",
    "    test_price = np.array(test[target_name])\n",
    "    train_labels = CreateTarget.semi_trend_ratio(train_price, future_day, timestep)\n",
    "    test_labels = CreateTarget.semi_trend_ratio(test_price, future_day, timestep)\n",
    "    \n",
    "    train_labels = np.expand_dims(train_labels, axis = -1)\n",
    "    test_labels = np.expand_dims(test_labels, axis = -1)\n",
    "    \n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    train_scaled = scaler.fit_transform(train)\n",
    "    test_scaled = scaler.transform(test)\n",
    "    \n",
    "    train_X = create_time_table_X(train_scaled,timestep)\n",
    "    test_X = create_time_table_X(test_scaled,timestep)\n",
    "\n",
    "    train_X = np.expand_dims(train_X, axis = -1)\n",
    "    test_X = np.expand_dims(test_X, axis = -1)\n",
    "\n",
    "    train_X = train_X[:len(train_labels)]\n",
    "    test_X = test_X[:len(test_labels)]\n",
    "    \n",
    "    return (train_X, train_labels, test_X, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_learning_dataset(item_name, train_data, test_data, future_day,time_step, price_col,n_variable = None):\n",
    "    train_price = np.array(train_data[price_col]) #train/test 데이터의 종가 추출\n",
    "    test_price = np.array(test_data[price_col])\n",
    "    \n",
    "    y_train = CreateTarget.semi_trend_ratio(train_price, future_day,time_step) #train/test 수익률 계산하여 label생성\n",
    "    y_test = CreateTarget.semi_trend_ratio(test_price, future_day,time_step)\n",
    "    \n",
    "    y_train= np.expand_dims(y_train, axis = -1)\n",
    "    y_test = np.expand_dims(y_test, axis = -1)\n",
    "    \n",
    "    if n_variable !=None:\n",
    "        train_data = load_important_variables(train_data, item_name, future_day, n_variable)\n",
    "        test_data = load_important_variables(test_data, item_name, future_day, n_variable)\n",
    "        \n",
    "    scaler = StandardScaler()  \n",
    "    train_data_scaled = scaler.fit_transform(train_data) #input data 정규화\n",
    "    test_data_scaled = scaler.transform(test_data) \n",
    "    \n",
    "    X_train = create_time_table_X(train_data_scaled,time_step) #input 데이터 time slice 하여 3차원 input 데이터 생성\n",
    "    X_test = create_time_table_X(test_data_scaled,time_step)\n",
    "    \n",
    "    return (X_train[:-future_day], y_train, X_test[:-future_day], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, X_data, labels):\n",
    "    logits = model(X_data, training = True)\n",
    "    loss = tf.reduce_mean(tf.square(logits - labels))\n",
    "    return loss\n",
    "\n",
    "def grad(model, X_data, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn(model, X_data, labels)\n",
    "    \n",
    "    return tape.gradient(loss, model.variables)\n",
    "\n",
    "def evaluate(model, X_data, labels):\n",
    "    logits = model(X_data, training = False)\n",
    "    MES = tf.reduce_mean(tf.square(logits - labels)) \n",
    "    return MES\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(model, X_train,y_train, X_test, y_test, batch_size, n_epochs ):\n",
    "    for epoch in range(n_epochs):\n",
    "        X, labels = next_batch(X_train, y_train, batch_size)\n",
    "\n",
    "        grads = grad(model, X, labels)\n",
    "        optimizer.apply_gradients(zip(grads, model.variables))\n",
    "        if epoch%100 == 0:\n",
    "            loss = loss_fn(model, X, labels)\n",
    "            train_MSE = evaluate(model, X, labels)\n",
    "\n",
    "            test_MSE =  evaluate(model, X_test, y_test)\n",
    "\n",
    "            print('Epoch :', epoch+1, ' loss =', loss.numpy(), ' train MSE =', train_MSE.numpy(),' test MSE =', test_MSE.numpy())\n",
    "    return  model(X_test, training = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_LSTM(time_step):\n",
    "    with tf.device('/gpu:2') :\n",
    "        inputs = keras.Input(shape = (time_step,3917,1))\n",
    "        conv1 = keras.layers.Conv2D(filters = 1, kernel_size = (1,3917), strides = (1,1),padding = 'SAME', activation = tf.nn.elu,kernel_regularizer=keras.regularizers.l2(0.001))(inputs)\n",
    "        pool1 = keras.layers.MaxPool2D(pool_size = (10,10),padding = 'SAME')(conv1)\n",
    "        pool2 = tf.reshape (pool1,(-1,pool1.shape[1],pool1.shape[2]))\n",
    "        #pool3_flat = keras.layers.Flatten()(pool3)\n",
    "        #dense4 = keras.layers.Dense(units= 50,kernel_regularizer=keras.regularizers.l2(0.001), activation = tf.nn.elu)(pool2)\n",
    "        #drop4 = keras.layers.Dropout(rate = 0.9)(dense4)\n",
    "        #CNN_output = keras.layers.Dense(units = 50)(drop4)\n",
    "        \n",
    "        \n",
    "        lstm5 = keras.layers.LSTM(units = 50, activation = tf.nn.relu6,return_sequences=True)(pool2)\n",
    "        drop5 = keras.layers.TimeDistributed(keras.layers.Dropout(rate = .5))(lstm5)\n",
    "        lstm6 = keras.layers.LSTM(units = 50, activation = tf.nn.relu6)(drop5)\n",
    "        drop6 = keras.layers.Dropout(rate = .9)(lstm6)\n",
    "\n",
    "        dense8 = keras.layers.Dense(units = 64)(drop6)\n",
    "        drop8 = keras.layers.Dropout(rate = .9)(dense8)\n",
    "        logits = keras.layers.Dense(units = 1)(drop8)\n",
    "\n",
    "    return keras.Model(inputs = inputs, outputs = logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "38*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_LSTM(time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_CNN(time_step,n_input):\n",
    "    with tf.device('/gpu:0') :\n",
    "        inputs = keras.Input(shape = (time_step,n_input,1))\n",
    "        conv1 = keras.layers.Conv2D(filters = 32, kernel_size = 3, padding = 'SAME', activation = tf.nn.relu)(inputs)\n",
    "        pool1 = keras.layers.MaxPool2D(padding = 'SAME')(conv1)\n",
    "        conv2 = keras.layers.Conv2D(filters = 64, kernel_size = 3, padding = 'SAME', activation =tf.nn.relu)(pool1)\n",
    "        pool2 = keras.layers.MaxPool2D(padding = 'same')(conv2)\n",
    "        conv3 = keras.layers.Conv2D(filters = 128, kernel_size = 3, padding = 'SAME', activation = tf.nn.relu)(pool2)\n",
    "        pool3 = keras.layers.MaxPool2D(padding = 'SAME')(conv3)\n",
    "        pool3_flat = keras.layers.Flatten()(pool3)\n",
    "        dense4 = keras.layers.Dense(units= 256, activation = tf.nn.relu)(pool3_flat)\n",
    "        drop4 = keras.layers.Dropout(rate = 0.4)(dense4)\n",
    "        logits = keras.layers.Dense(units = 1)(drop4)\n",
    "\n",
    "    return keras.Model(inputs = inputs, outputs = logits)\n",
    "\n",
    "#,kernel_regularizer=keras.regularizers.l2(0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 200, 100, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 200, 100, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 100, 50, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 100, 50, 64)       18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 50, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 50, 25, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 25, 13, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 41600)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               10649856  \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 10,742,785\n",
      "Trainable params: 10,742,785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_CNN = create_CNN(200,100)\n",
    "model_CNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_LSTM(time_step,n_input):\n",
    "    #with tf.device('/gpu:1') :\n",
    "    inputs = keras.Input(shape = (time_step,n_input))    \n",
    "    #lstm1 = keras.layers.LSTM(units = 300, activation = tf.nn.relu6, return_sequences=True)(inputs)\n",
    "    lstm1 = keras.layers.CuDNNLSTM(units = 300, return_sequences=True)(inputs)\n",
    "\n",
    "    #drop1 = keras.layers.TimeDistributed(keras.layers.Dropout(rate = .5))(lstm1)\n",
    "    drop1 = keras.layers.Dropout(rate = .3)(lstm1)\n",
    "\n",
    "    #lstm2 = keras.layers.LSTM(units = 300, activation = tf.nn.relu6,return_sequences=True)(drop1)\n",
    "    lstm2 = keras.layers.CuDNNLSTM(units = 300,return_sequences=True)(drop1)\n",
    "    drop2 = keras.layers.Dropout(rate = .3)(lstm2)\n",
    "\n",
    "    #lstm3 = keras.layers.LSTM(units = 300, activation = tf.nn.relu6,return_sequences=True)(drop2)\n",
    "    lstm3 = keras.layers.CuDNNLSTM(units = 300,return_sequences=True)(drop2)\n",
    "    drop3 = keras.layers.Dropout(rate = .3)(lstm3)\n",
    "\n",
    "    stacked_rnn_outputs = tf.reshape(drop3,[-1,300])\n",
    "    stacked_outputs = keras.layers.Dense(units = 1)(stacked_rnn_outputs)\n",
    "    logits = tf.reshape(stacked_outputs,[-1,time_step, 1])\n",
    "    #dense3 = keras.layers.Dense(units = 64)(drop2)\n",
    "    #drop3 = keras.layers.Dropout(rate = .9)(dense3)\n",
    "    #logits = keras.layers.TimeDistributed(keras.layers.Dense(units = 1))(drop3)\n",
    "\n",
    "    return keras.Model(inputs = inputs, outputs = logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  2.6645393 ],\n",
       "       [ -1.6320925 ],\n",
       "       [ -7.2528944 ],\n",
       "       [ -4.394966  ],\n",
       "       [ -4.10991   ],\n",
       "       [ -7.5743537 ],\n",
       "       [-12.021382  ],\n",
       "       [-12.159553  ],\n",
       "       [ -3.4653382 ],\n",
       "       [  5.661732  ],\n",
       "       [  6.8147616 ],\n",
       "       [  7.2207837 ],\n",
       "       [  9.414165  ],\n",
       "       [ 12.075141  ],\n",
       "       [ 12.870138  ],\n",
       "       [ 14.164834  ],\n",
       "       [ 14.056041  ],\n",
       "       [ 16.214132  ],\n",
       "       [ 17.885708  ],\n",
       "       [ 18.816132  ],\n",
       "       [ 18.397877  ],\n",
       "       [ 16.78931   ],\n",
       "       [ 15.563253  ],\n",
       "       [ 14.36325   ],\n",
       "       [ 13.651658  ],\n",
       "       [ 13.160334  ],\n",
       "       [ 12.912563  ],\n",
       "       [ 13.268429  ],\n",
       "       [ 13.724841  ],\n",
       "       [ 13.64328   ],\n",
       "       [ 13.423542  ],\n",
       "       [ 14.231275  ],\n",
       "       [ 15.659476  ],\n",
       "       [ 17.024817  ],\n",
       "       [ 15.4416685 ],\n",
       "       [ 13.764699  ],\n",
       "       [ 13.621931  ],\n",
       "       [ 13.563576  ],\n",
       "       [ 14.297652  ],\n",
       "       [ 14.330974  ],\n",
       "       [ 13.354017  ],\n",
       "       [ 12.027668  ],\n",
       "       [ 12.477649  ],\n",
       "       [ 12.24249   ],\n",
       "       [ 13.096021  ],\n",
       "       [ 13.670037  ],\n",
       "       [ 11.602283  ],\n",
       "       [ 10.668046  ],\n",
       "       [ 10.358687  ],\n",
       "       [ 10.096217  ],\n",
       "       [  9.624346  ],\n",
       "       [ 11.3186035 ],\n",
       "       [ 12.962843  ],\n",
       "       [ 13.803306  ],\n",
       "       [ 11.999941  ],\n",
       "       [ 11.000945  ],\n",
       "       [ 12.10473   ],\n",
       "       [ 12.923548  ],\n",
       "       [ 13.1923895 ],\n",
       "       [ 13.024882  ],\n",
       "       [ 13.127059  ],\n",
       "       [ 13.857085  ],\n",
       "       [ 12.530092  ],\n",
       "       [ 11.309236  ],\n",
       "       [ 11.161672  ],\n",
       "       [ 10.308289  ],\n",
       "       [ 11.950405  ],\n",
       "       [ 13.491647  ],\n",
       "       [ 14.310425  ],\n",
       "       [ 14.549371  ],\n",
       "       [ 14.708074  ],\n",
       "       [ 14.408448  ],\n",
       "       [ 15.276662  ],\n",
       "       [ 15.509779  ],\n",
       "       [ 15.305184  ],\n",
       "       [ 14.988273  ],\n",
       "       [ 13.848755  ],\n",
       "       [ 10.895539  ],\n",
       "       [  9.040278  ],\n",
       "       [ 10.9909935 ],\n",
       "       [ 13.148362  ],\n",
       "       [ 15.405247  ],\n",
       "       [ 15.433592  ],\n",
       "       [ 13.414787  ],\n",
       "       [  6.3257785 ],\n",
       "       [  1.1896126 ],\n",
       "       [ -5.0157275 ],\n",
       "       [-10.140374  ],\n",
       "       [-14.218853  ],\n",
       "       [-13.423304  ],\n",
       "       [-12.167554  ],\n",
       "       [-14.148674  ],\n",
       "       [-14.032958  ],\n",
       "       [-12.776007  ],\n",
       "       [-12.8339    ],\n",
       "       [-11.974172  ],\n",
       "       [-10.294707  ],\n",
       "       [-11.038164  ],\n",
       "       [-11.791036  ],\n",
       "       [-11.40421   ],\n",
       "       [-11.902183  ],\n",
       "       [-13.088177  ],\n",
       "       [-10.148708  ],\n",
       "       [ -6.765539  ],\n",
       "       [ -6.844967  ],\n",
       "       [ -5.235593  ],\n",
       "       [ -6.254071  ],\n",
       "       [ -6.6446285 ],\n",
       "       [ -7.868572  ],\n",
       "       [ -9.140582  ],\n",
       "       [-10.282928  ],\n",
       "       [ -9.881617  ],\n",
       "       [ -8.570125  ],\n",
       "       [ -9.425135  ],\n",
       "       [-10.821555  ],\n",
       "       [-11.286655  ],\n",
       "       [-10.161157  ],\n",
       "       [ -9.407524  ],\n",
       "       [ -9.41392   ],\n",
       "       [ -9.173993  ],\n",
       "       [ -7.6656218 ],\n",
       "       [ -6.4493446 ],\n",
       "       [ -4.076365  ],\n",
       "       [ -1.4213359 ],\n",
       "       [ -0.02387869],\n",
       "       [  0.9307122 ],\n",
       "       [  0.37468344],\n",
       "       [ -0.29892766],\n",
       "       [  0.5927944 ],\n",
       "       [  0.05529767],\n",
       "       [ -0.368859  ],\n",
       "       [  0.28870052],\n",
       "       [  0.22989553],\n",
       "       [ -1.4968925 ],\n",
       "       [ -2.7942364 ],\n",
       "       [ -3.4004123 ],\n",
       "       [ -3.900517  ],\n",
       "       [ -3.9360242 ],\n",
       "       [ -1.5410669 ],\n",
       "       [  0.5294662 ],\n",
       "       [ -0.07639691],\n",
       "       [ -0.25162864],\n",
       "       [  0.09597665],\n",
       "       [  0.25097093],\n",
       "       [ -0.8081663 ],\n",
       "       [ -1.9623585 ],\n",
       "       [ -3.4960723 ],\n",
       "       [ -5.9087467 ],\n",
       "       [ -7.717868  ],\n",
       "       [ -7.0335026 ],\n",
       "       [ -6.5046124 ],\n",
       "       [ -5.1323686 ],\n",
       "       [ -4.03511   ],\n",
       "       [ -3.7732506 ],\n",
       "       [ -2.7558212 ],\n",
       "       [ -1.9891877 ],\n",
       "       [ -3.0244493 ],\n",
       "       [ -2.786699  ],\n",
       "       [ -1.5114636 ],\n",
       "       [ -0.1892978 ],\n",
       "       [  0.43067276],\n",
       "       [ -0.29992962],\n",
       "       [  0.84226   ],\n",
       "       [  1.7516036 ],\n",
       "       [  1.9806149 ],\n",
       "       [  1.8611772 ],\n",
       "       [ -0.5220384 ],\n",
       "       [ -0.9283511 ],\n",
       "       [ -2.0558872 ],\n",
       "       [ -2.8494465 ],\n",
       "       [ -2.4282637 ],\n",
       "       [ -1.3854272 ],\n",
       "       [ -2.1140075 ],\n",
       "       [ -1.0680301 ],\n",
       "       [ -2.0347989 ],\n",
       "       [ -1.9475484 ],\n",
       "       [ -1.0600247 ],\n",
       "       [ -0.48875707],\n",
       "       [ -0.9937715 ],\n",
       "       [ -1.3070185 ],\n",
       "       [ -1.0570021 ],\n",
       "       [ -2.8204527 ],\n",
       "       [ -3.962926  ],\n",
       "       [ -2.667047  ],\n",
       "       [ -2.9211862 ],\n",
       "       [ -3.7553105 ],\n",
       "       [ -2.7142558 ],\n",
       "       [ -0.35428518],\n",
       "       [  1.6081364 ],\n",
       "       [  2.2746382 ],\n",
       "       [  1.6328056 ],\n",
       "       [ -0.25180626],\n",
       "       [ -0.07300985],\n",
       "       [  1.0345395 ],\n",
       "       [  2.4004977 ],\n",
       "       [  2.1351411 ],\n",
       "       [  1.0394317 ],\n",
       "       [  0.0194689 ],\n",
       "       [ -0.15292674],\n",
       "       [  1.6718247 ]], dtype=float32)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(predict_LSTM[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Database connection successful!\n",
      "[success] enterprise_absolute_value_indicator\n",
      "[success] enterprise_consensus_estimate\n",
      "[success] enterprise_dividend\n",
      "[success] enterprise_financial_ratio\n",
      "[success] enterprise_financial_statements\n",
      "[success] enterprise_fixed_result_news\n",
      "[success] enterprise_relative_value_indicator\n",
      "[success] enterprise_stock\n",
      "[success] enterprise_tentative_result\n",
      "[success] market_financial_institution_reception_trend\n",
      "[success] market_fund_investors\n",
      "[success] market_money_around_market\n",
      "[success] market_retirement_pension\n",
      "[success] market_stock_cam\n",
      "[success] market_trends_elw\n",
      "[success] market_trends_investor_bond\n",
      "[success] market_trends_investor_dollar\n",
      "[success] market_trends_investor_eur\n",
      "[success] market_trends_investor_gold\n",
      "[success] market_trends_investor_index\n",
      "[success] market_trends_investor_jpy\n",
      "[success] market_trends_investor_pork\n",
      "[success] market_trends_investor_rates\n",
      "[success] market_trends_investor_stock\n",
      "[success] market_trends_program\n",
      "[success] economy_employ\n",
      "[success] economy_finance\n",
      "[success] economy_income\n",
      "[success] economy_industry\n",
      "[success] economy_oversea_statistics\n",
      "[success] economy_prices\n",
      "[success] economy_trade\n"
     ]
    }
   ],
   "source": [
    "item_name = 'SK텔레콤'\n",
    "data = data_load(item_name)\n",
    "data = remove_dummy_columns(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name = '수정주가(원)'\n",
    "time_step = 200\n",
    "future_day = 60\n",
    "n_epochs = 5000\n",
    "batch_size = 50\n",
    "learning_rate = 0.001\n",
    "n_input = 100\n",
    "var_method = 'RF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_16 (InputLayer)        [(None, 200, 100)]        0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_21 (CuDNNLSTM)    (None, 200, 300)          482400    \n",
      "_________________________________________________________________\n",
      "dropout_31 (Dropout)         (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_22 (CuDNNLSTM)    (None, 200, 300)          722400    \n",
      "_________________________________________________________________\n",
      "dropout_32 (Dropout)         (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "cu_dnnlstm_23 (CuDNNLSTM)    (None, 200, 300)          722400    \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Reshape_20 (Tens [(None, 300)]             0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 301       \n",
      "_________________________________________________________________\n",
      "tf_op_layer_Reshape_21 (Tens [(None, 200, 1)]          0         \n",
      "=================================================================\n",
      "Total params: 1,927,501\n",
      "Trainable params: 1,927,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "model_LSTM = create_LSTM(time_step,n_input)\n",
    "model_LSTM.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = round(len(data)*80/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[:idx]\n",
    "test_data = data[idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunggil/project/market_prediction/code/preprocessing.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['target'] = target\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size : 4660 test size : 1165\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = load_important_train_test(data,item_name, future_day, target_name, 'trend_ratio',n_input,var_method)    \n",
    "X_train, y_train, X_test, y_test = load_timeseries_tensor_train_test(X_train, y_train, X_test, y_test,time_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fecf04b6210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fecf04b6210>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fecf04b6210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fecf04b6210>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fecf04252d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fecf04252d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fecf04252d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fecf04252d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fecf04b6210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fecf04b6210>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fecf04b6210>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fecf04b6210>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fecf04252d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fecf04252d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fecf04252d0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method TensorFlowOpLayer._defun_call of <tensorflow.python.eager.function.TfMethodTarget object at 0x7fecf04252d0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "Epoch : 1  loss = 335.97983  train MSE = 335.69547  test MSE = 67.41674\n",
      "Epoch : 101  loss = 83.285065  train MSE = 81.620995  test MSE = 93.18936\n",
      "Epoch : 201  loss = 80.11048  train MSE = 78.38063  test MSE = 92.51435\n",
      "Epoch : 301  loss = 143.44095  train MSE = 140.27745  test MSE = 89.14215\n",
      "Epoch : 401  loss = 108.80809  train MSE = 107.16847  test MSE = 89.37231\n",
      "Epoch : 501  loss = 107.6175  train MSE = 104.29323  test MSE = 86.47764\n",
      "Epoch : 601  loss = 66.97924  train MSE = 64.772896  test MSE = 87.21824\n",
      "Epoch : 701  loss = 22.43886  train MSE = 20.337885  test MSE = 87.9647\n",
      "Epoch : 801  loss = 21.933382  train MSE = 19.224348  test MSE = 87.921364\n",
      "Epoch : 901  loss = 26.588362  train MSE = 23.294085  test MSE = 89.50172\n",
      "Epoch : 1001  loss = 21.707743  train MSE = 18.545902  test MSE = 89.4998\n",
      "Epoch : 1101  loss = 27.714115  train MSE = 24.023678  test MSE = 90.74405\n",
      "Epoch : 1201  loss = 10.941653  train MSE = 8.486635  test MSE = 87.343735\n",
      "Epoch : 1301  loss = 12.307472  train MSE = 10.399238  test MSE = 89.56576\n",
      "Epoch : 1401  loss = 24.244783  train MSE = 20.759716  test MSE = 91.05288\n",
      "Epoch : 1501  loss = 16.89733  train MSE = 13.319209  test MSE = 90.08645\n",
      "Epoch : 1601  loss = 6.125066  train MSE = 4.466321  test MSE = 88.211914\n",
      "Epoch : 1701  loss = 7.1966076  train MSE = 5.1110954  test MSE = 86.91092\n",
      "Epoch : 1801  loss = 9.744928  train MSE = 6.6935306  test MSE = 89.0176\n",
      "Epoch : 1901  loss = 7.833981  train MSE = 5.1581335  test MSE = 86.84553\n",
      "Epoch : 2001  loss = 4.80952  train MSE = 2.262832  test MSE = 85.43448\n",
      "Epoch : 2101  loss = 5.96029  train MSE = 3.1431875  test MSE = 89.56826\n",
      "Epoch : 2201  loss = 5.2313447  train MSE = 2.6967316  test MSE = 87.9837\n",
      "Epoch : 2301  loss = 2.8422804  train MSE = 1.2874955  test MSE = 89.99373\n",
      "Epoch : 2401  loss = 9.183231  train MSE = 5.4146624  test MSE = 84.5585\n",
      "Epoch : 2501  loss = 3.3565192  train MSE = 1.0683999  test MSE = 85.875046\n",
      "Epoch : 2601  loss = 5.2369213  train MSE = 2.299604  test MSE = 86.87834\n",
      "Epoch : 2701  loss = 6.158811  train MSE = 2.8053524  test MSE = 85.833725\n",
      "Epoch : 2801  loss = 1.7161252  train MSE = 0.41079843  test MSE = 83.02415\n",
      "Epoch : 2901  loss = 3.594757  train MSE = 1.5193089  test MSE = 85.81719\n",
      "Epoch : 3001  loss = 3.0278952  train MSE = 1.0164782  test MSE = 84.99998\n",
      "Epoch : 3101  loss = 3.1586542  train MSE = 1.6296066  test MSE = 84.81877\n",
      "Epoch : 3201  loss = 2.7320614  train MSE = 0.731976  test MSE = 83.77972\n",
      "Epoch : 3301  loss = 2.8485596  train MSE = 1.0772176  test MSE = 84.12453\n",
      "Epoch : 3401  loss = 3.5312266  train MSE = 1.1962273  test MSE = 83.604675\n",
      "Epoch : 3501  loss = 3.2357917  train MSE = 0.9978945  test MSE = 83.70475\n",
      "Epoch : 3601  loss = 2.0879  train MSE = 0.5279148  test MSE = 83.19958\n",
      "Epoch : 3701  loss = 2.8996193  train MSE = 0.7842225  test MSE = 85.339516\n",
      "Epoch : 3801  loss = 2.4123359  train MSE = 0.5752666  test MSE = 84.23135\n",
      "Epoch : 3901  loss = 3.4702406  train MSE = 1.3977648  test MSE = 86.41108\n",
      "Epoch : 4001  loss = 1.5494133  train MSE = 0.32292545  test MSE = 82.702126\n",
      "Epoch : 4101  loss = 2.1233878  train MSE = 0.3786133  test MSE = 82.68351\n",
      "Epoch : 4201  loss = 2.7680418  train MSE = 0.59802735  test MSE = 83.51101\n",
      "Epoch : 4301  loss = 1.5786184  train MSE = 0.3884896  test MSE = 81.63165\n",
      "Epoch : 4401  loss = 2.1223109  train MSE = 0.37655482  test MSE = 83.123726\n",
      "Epoch : 4501  loss = 2.0770419  train MSE = 0.48120117  test MSE = 83.49508\n",
      "Epoch : 4601  loss = 2.2882755  train MSE = 0.4997493  test MSE = 84.4609\n",
      "Epoch : 4701  loss = 2.5383945  train MSE = 0.37361738  test MSE = 84.22462\n",
      "Epoch : 4801  loss = 1.8314402  train MSE = 0.33434954  test MSE = 84.87428\n",
      "Epoch : 4901  loss = 1.3501536  train MSE = 0.32821378  test MSE = 83.87981\n"
     ]
    }
   ],
   "source": [
    "predict_LSTM = training(model_LSTM, X_train,y_train, X_test, y_test, batch_size, n_epochs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_time = np.array(predict_LSTM)[0:,-1]\n",
    "y_test_time = y_test[0:,-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE = round((sum((y_pred_time-y_test_time)**2)/len(y_pred_time))[0],3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_test_price = data[len(X_train):][target_name]\n",
    "\n",
    "y_test_price = restore_future_price(X_test_price[time_step-1:], y_test_time.reshape(-1), future_day)\n",
    "y_pred_price = restore_future_price(X_test_price[time_step-1:], y_pred_time.reshape(-1), future_day)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '/futureday'+str(future_day)+'_timestep'+str(time_step)+'_nInput'+str(n_input)+'.xlsx'\n",
    "file_path = '../result/'+'LSTM_'+var_method+'/'+item_name\n",
    "\n",
    "result_df = pd.DataFrame({\"ratio_r\":y_test_time.reshape(-1), \"ratio_p\":y_pred_time.reshape(-1), \"price_r\":y_test_price, \"price_p\":y_pred_price})\n",
    "result_df.to_excel(file_path+file_name)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_up_down = list()\n",
    "y_test_up_down = list()\n",
    "for i in range(len(y_pred_time)):\n",
    "\n",
    "    if y_pred_time[i] < 0 :\n",
    "        y_pred_up_down.append(0)\n",
    "    else:\n",
    "        y_pred_up_down.append(1)\n",
    "\n",
    "for i in range(len(y_test_time)):    \n",
    "    if y_test_time[i] < 0 :\n",
    "        y_test_up_down.append(0)\n",
    "    else:\n",
    "        y_test_up_down.append(1)  \n",
    "\n",
    "confus_mat = confusion_matrix(y_pred_up_down,y_test_up_down)\n",
    "\n",
    "valuation(file_path+file_name,confus_mat,MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var_model : Lasso , train model : LSTM , MSE : 98.76 , Accuracy : 0.555 , Precision : 0.506 , Recall : 0.293\n"
     ]
    }
   ],
   "source": [
    "print('var_model :', var_method, ', train model : LSTM' ,', MSE :', MSE,', Accuracy :',accu, ', Precision :', precision, ', Recall :', recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = create_learning_dataset(item_name, train_data, test_data, future_day,time_step, target_name,n_variable = n_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4449, 1)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(918, 200, 100)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4509-4449"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_15\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_19 (InputLayer)        [(None, 200, 100)]        0         \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 200, 300)          481200    \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 200, 300)          721200    \n",
      "_________________________________________________________________\n",
      "dropout_16 (Dropout)         (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "lstm_14 (LSTM)               (None, 300)               721200    \n",
      "_________________________________________________________________\n",
      "dropout_17 (Dropout)         (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 1,923,901\n",
      "Trainable params: 1,923,901\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_LSTM = create_LSTM(time_step,n_input)\n",
    "model_LSTM.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_18 (InputLayer)        [(None, 200, 100, 1)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 200, 100, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 100, 50, 32)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 100, 50, 64)       18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 50, 25, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 50, 25, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 25, 13, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_10 (Flatten)         (None, 41600)             0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 256)               10649856  \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 10,742,785\n",
      "Trainable params: 10,742,785\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_CNN = create_CNN(time_step,n_input)\n",
    "model_CNN.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4449, 1)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunggil/project/market_prediction/code/preprocessing.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['target'] = target\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training size : 4660 test size : 1165\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sunggil/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/ops/math_grad.py:1220: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Epoch : 1  loss = 586.57007  train MSE = 586.82263  test MSE = 68.548065\n",
      "Epoch : 2  loss = 830.85095  train MSE = 831.0014  test MSE = 68.41507\n",
      "Epoch : 3  loss = 810.11255  train MSE = 808.1434  test MSE = 67.70547\n",
      "Epoch : 4  loss = 986.6304  train MSE = 988.39685  test MSE = 66.93026\n",
      "Epoch : 5  loss = 540.8073  train MSE = 535.50433  test MSE = 66.917114\n",
      "Epoch : 6  loss = 985.46985  train MSE = 975.9773  test MSE = 65.83372\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-4247cbff20dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredict_LSTM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_LSTM2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-9-2b4216ccbd80>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(model, X_train, y_train, X_test, y_test, batch_size, n_epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mtrain_MSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mtest_MSE\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch :'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' loss ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' train MSE ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_MSE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' test MSE ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_MSE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-69e42ec110f9>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, X_data, labels)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mMES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mMES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    677\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    678\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[0;32m--> 679\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    749\u001b[0m                                 ' implement a `call` method.')\n\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_internal_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, initial_state, constants, **kwargs)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minitial_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconstants\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 619\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m     \u001b[0;31m# If any of `initial_state` or `constants` are specified and are Keras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    677\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    678\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[0;32m--> 679\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state)\u001b[0m\n\u001b[1;32m   2531\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_recurrent_dropout_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2532\u001b[0m     return super(LSTM, self).call(\n\u001b[0;32m-> 2533\u001b[0;31m         inputs, mask=mask, training=training, initial_state=initial_state)\n\u001b[0m\u001b[1;32m   2534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2535\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask, training, initial_state, constants)\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m         \u001b[0mtime_major\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime_major\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 743\u001b[0;31m         zero_output_for_mask=self.zero_output_for_mask)\n\u001b[0m\u001b[1;32m    744\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    745\u001b[0m       \u001b[0mupdates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mrnn\u001b[0;34m(step_function, inputs, initial_states, go_backwards, mask, constants, unroll, input_length, time_major, zero_output_for_mask)\u001b[0m\n\u001b[1;32m   3850\u001b[0m           \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_step\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3851\u001b[0m           \u001b[0mloop_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_ta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3852\u001b[0;31m           **while_loop_kwargs)\n\u001b[0m\u001b[1;32m   3853\u001b[0m       \u001b[0mnew_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinal_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36mwhile_loop\u001b[0;34m(cond, body, loop_vars, shape_invariants, parallel_iterations, back_prop, swap_memory, name, maximum_iterations, return_same_structure)\u001b[0m\n\u001b[1;32m   3461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3462\u001b[0m       \u001b[0;32mwhile\u001b[0m \u001b[0mcond\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3463\u001b[0;31m         \u001b[0mloop_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3464\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtry_to_pack\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_basetuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3465\u001b[0m           \u001b[0mpacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/ops/control_flow_ops.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(i, lv)\u001b[0m\n\u001b[1;32m   3454\u001b[0m         cond = lambda i, lv: (  # pylint: disable=g-long-lambda\n\u001b[1;32m   3455\u001b[0m             math_ops.logical_and(i < maximum_iterations, orig_cond(*lv)))\n\u001b[0;32m-> 3456\u001b[0;31m         \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlv\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_step\u001b[0;34m(time, output_ta_t, *states)\u001b[0m\n\u001b[1;32m   3834\u001b[0m         \u001b[0mcurrent_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_sequence_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3835\u001b[0m         output, new_states = step_function(current_input,\n\u001b[0;32m-> 3836\u001b[0;31m                                            tuple(states) + tuple(constants))\n\u001b[0m\u001b[1;32m   3837\u001b[0m         \u001b[0mflat_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3838\u001b[0m         \u001b[0mflat_new_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(inputs, states)\u001b[0m\n\u001b[1;32m    726\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mstates\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_tf_rnn_cell\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    729\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m           \u001b[0mnew_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnew_states\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, states, training)\u001b[0m\n\u001b[1;32m   2238\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_o\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2239\u001b[0m       \u001b[0mh_tm1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mh_tm1_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_tm1_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_tm1_c\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_tm1_o\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2240\u001b[0;31m       \u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compute_carry_and_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh_tm1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_tm1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2241\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2242\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;36m0.\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m_compute_carry_and_output\u001b[0;34m(self, x, h_tm1, c_tm1)\u001b[0m\n\u001b[1;32m   2177\u001b[0m         x_i + K.dot(h_tm1_i, self.recurrent_kernel[:, :self.units]))\n\u001b[1;32m   2178\u001b[0m     f = self.recurrent_activation(x_f + K.dot(\n\u001b[0;32m-> 2179\u001b[0;31m         h_tm1_f, self.recurrent_kernel[:, self.units:self.units * 2]))\n\u001b[0m\u001b[1;32m   2180\u001b[0m     c = f * c_tm1 + i * self.activation(x_c + K.dot(\n\u001b[1;32m   2181\u001b[0m         h_tm1_c, self.recurrent_kernel[:, self.units * 2:self.units * 3]))\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mdot\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m   1606\u001b[0m   \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1607\u001b[0m   \"\"\"\n\u001b[0;32m-> 1608\u001b[0;31m   \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1609\u001b[0m     \u001b[0mx_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1610\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mndim\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1150\u001b[0m   \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m   \"\"\"\n\u001b[0;32m-> 1152\u001b[0;31m   \u001b[0mdims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mdims\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mshape\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    902\u001b[0m       \u001b[0;31m# `_tensor_shape` is declared and defined in the definition of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m       \u001b[0;31m# `EagerTensor`, in C.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorShape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shape_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dims)\u001b[0m\n\u001b[1;32m    772\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0;31m# Got a list of dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    772\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0;31m# Got a list of dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mas_dimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdims_iter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36mas_dimension\u001b[0;34m(value)\u001b[0m\n\u001b[1;32m    714\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mDimension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDimension\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDType\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot convert %s to Dimension\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "predict_LSTM = training(model_LSTM2, X_train,y_train, X_test, y_test, batch_size, n_epochs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, labels = next_batch(X_train, y_train, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model_LSTM2(X, training = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.reduce_mean(tf.square(logits - labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.121651], dtype=float32)"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(predict_LSTM)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_time = np.array(predict_LSTM)[0:,-1]\n",
    "y_test_time = np.array(y_test)[0:,-1]\n",
    "MSE = round((sum((y_pred_time-y_test_time)**2)/len(y_pred_time))[0],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.76"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_up_down = list()\n",
    "y_test_up_down = list()\n",
    "for i in range(len(y_pred_time)):\n",
    "\n",
    "    if y_pred_time[i] < 0 :\n",
    "        y_pred_up_down.append(0)\n",
    "    else:\n",
    "        y_pred_up_down.append(1)\n",
    "\n",
    "for i in range(len(y_test_time)):    \n",
    "    if y_test_time[i] < 0 :\n",
    "        y_test_up_down.append(0)\n",
    "    else:\n",
    "        y_test_up_down.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "confus_mat = confusion_matrix(y_pred_up_down,y_test_up_down)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[127, 124],\n",
       "       [306, 409]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confus_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = round(confus_mat[0][0]/sum(confus_mat[:,0]),3)\n",
    "precision = round(confus_mat[0][0]/sum(confus_mat[0]),3)\n",
    "accu = round((confus_mat[0][0]+confus_mat[1][1])/sum(sum(confus_mat)),3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.293"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.506"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.555"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_4D = np.expand_dims(X_train, axis = -1)\n",
    "X_test_4D = np.expand_dims(X_test, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1  loss = 1422.6422  train MSE = 1350.2759  test MSE = 1491.9822\n",
      "Epoch : 2  loss = 1599.4623  train MSE = 1713.4609  test MSE = 802.5197\n",
      "Epoch : 3  loss = 273.25903  train MSE = 272.452  test MSE = 65.05144\n",
      "Epoch : 4  loss = 1057.2542  train MSE = 1055.5339  test MSE = 59.748894\n",
      "Epoch : 5  loss = 433.2846  train MSE = 429.73325  test MSE = 82.54547\n",
      "Epoch : 6  loss = 1272.3541  train MSE = 1290.0338  test MSE = 188.33023\n",
      "Epoch : 7  loss = 436.41278  train MSE = 431.13745  test MSE = 77.31992\n",
      "Epoch : 8  loss = 403.35345  train MSE = 392.33466  test MSE = 59.310394\n",
      "Epoch : 9  loss = 105.8286  train MSE = 107.3104  test MSE = 63.992935\n",
      "Epoch : 10  loss = 145.90205  train MSE = 140.49435  test MSE = 68.6866\n",
      "Epoch : 11  loss = 139.21053  train MSE = 136.80629  test MSE = 70.71861\n",
      "Epoch : 12  loss = 723.1004  train MSE = 724.77625  test MSE = 64.522285\n",
      "Epoch : 13  loss = 430.7093  train MSE = 430.51993  test MSE = 60.35758\n",
      "Epoch : 14  loss = 1976.926  train MSE = 1976.9926  test MSE = 60.100796\n",
      "Epoch : 15  loss = 682.38464  train MSE = 682.3723  test MSE = 60.025352\n",
      "Epoch : 16  loss = 279.87396  train MSE = 279.9353  test MSE = 59.86846\n",
      "Epoch : 17  loss = 353.76886  train MSE = 353.4308  test MSE = 60.27385\n",
      "Epoch : 18  loss = 1412.4729  train MSE = 1411.5613  test MSE = 59.822994\n",
      "Epoch : 19  loss = 642.2158  train MSE = 642.0807  test MSE = 59.89167\n",
      "Epoch : 20  loss = 570.96985  train MSE = 570.9295  test MSE = 59.90764\n",
      "Epoch : 21  loss = 1081.0896  train MSE = 1081.5059  test MSE = 59.900845\n",
      "Epoch : 22  loss = 1079.3713  train MSE = 1079.323  test MSE = 59.88231\n",
      "Epoch : 23  loss = 917.6776  train MSE = 917.626  test MSE = 59.860725\n",
      "Epoch : 24  loss = 873.46423  train MSE = 873.6366  test MSE = 59.830673\n",
      "Epoch : 25  loss = 268.11874  train MSE = 268.2848  test MSE = 59.80378\n",
      "Epoch : 26  loss = 316.70135  train MSE = 316.73938  test MSE = 59.774033\n",
      "Epoch : 27  loss = 125.65859  train MSE = 125.59104  test MSE = 59.741215\n",
      "Epoch : 28  loss = 561.9883  train MSE = 561.9784  test MSE = 59.708374\n",
      "Epoch : 29  loss = 425.8647  train MSE = 425.86118  test MSE = 59.68434\n",
      "Epoch : 30  loss = 89.387344  train MSE = 89.31114  test MSE = 59.65569\n",
      "Epoch : 31  loss = 178.81679  train MSE = 178.7828  test MSE = 59.635017\n",
      "Epoch : 32  loss = 259.17575  train MSE = 258.83295  test MSE = 59.62103\n",
      "Epoch : 33  loss = 823.7785  train MSE = 823.8341  test MSE = 59.598244\n",
      "Epoch : 34  loss = 114.9375  train MSE = 114.97701  test MSE = 59.576458\n",
      "Epoch : 35  loss = 76.25502  train MSE = 76.02133  test MSE = 59.568092\n",
      "Epoch : 36  loss = 108.527565  train MSE = 108.19192  test MSE = 59.5689\n",
      "Epoch : 37  loss = 337.05734  train MSE = 336.82007  test MSE = 59.564816\n",
      "Epoch : 38  loss = 808.2546  train MSE = 807.5744  test MSE = 59.560482\n",
      "Epoch : 39  loss = 2413.386  train MSE = 2412.99  test MSE = 59.506184\n",
      "Epoch : 40  loss = 365.50958  train MSE = 366.5763  test MSE = 59.44742\n",
      "Epoch : 41  loss = 292.9619  train MSE = 292.56802  test MSE = 59.46111\n",
      "Epoch : 42  loss = 551.94275  train MSE = 551.26135  test MSE = 59.570988\n",
      "Epoch : 43  loss = 316.29062  train MSE = 315.63303  test MSE = 60.223167\n",
      "Epoch : 44  loss = 170.16074  train MSE = 169.11201  test MSE = 66.403145\n",
      "Epoch : 45  loss = 946.87646  train MSE = 969.9901  test MSE = 78.85975\n",
      "Epoch : 46  loss = 290.10635  train MSE = 285.8622  test MSE = 64.45737\n",
      "Epoch : 47  loss = 277.52042  train MSE = 275.51617  test MSE = 60.850506\n",
      "Epoch : 48  loss = 1932.7605  train MSE = 1935.0967  test MSE = 61.189117\n",
      "Epoch : 49  loss = 736.7274  train MSE = 728.68536  test MSE = 62.550404\n",
      "Epoch : 50  loss = 321.13275  train MSE = 331.2204  test MSE = 64.63627\n",
      "Epoch : 51  loss = 601.28125  train MSE = 633.66064  test MSE = 69.381714\n",
      "Epoch : 52  loss = 677.83966  train MSE = 660.1019  test MSE = 84.72204\n",
      "Epoch : 53  loss = 476.21698  train MSE = 473.25848  test MSE = 118.297874\n",
      "Epoch : 54  loss = 493.3069  train MSE = 523.0278  test MSE = 197.29509\n",
      "Epoch : 55  loss = 259.81665  train MSE = 218.74246  test MSE = 161.66306\n",
      "Epoch : 56  loss = 174.4808  train MSE = 156.71725  test MSE = 76.3321\n",
      "Epoch : 57  loss = 478.40244  train MSE = 478.66806  test MSE = 63.63915\n",
      "Epoch : 58  loss = 217.78984  train MSE = 209.00371  test MSE = 84.65656\n",
      "Epoch : 59  loss = 2895.2227  train MSE = 3001.856  test MSE = 74.17813\n",
      "Epoch : 60  loss = 250.50198  train MSE = 271.53018  test MSE = 66.4663\n",
      "Epoch : 61  loss = 97.39903  train MSE = 118.358986  test MSE = 64.31306\n",
      "Epoch : 62  loss = 129.13922  train MSE = 129.07697  test MSE = 64.31924\n",
      "Epoch : 63  loss = 179.10294  train MSE = 166.90427  test MSE = 64.807884\n",
      "Epoch : 64  loss = 223.28836  train MSE = 229.96603  test MSE = 65.33744\n",
      "Epoch : 65  loss = 116.54445  train MSE = 119.49211  test MSE = 66.11779\n",
      "Epoch : 66  loss = 40.110115  train MSE = 41.725822  test MSE = 66.86788\n",
      "Epoch : 67  loss = 410.94458  train MSE = 413.6862  test MSE = 66.69359\n",
      "Epoch : 68  loss = 135.09001  train MSE = 123.74314  test MSE = 66.408066\n",
      "Epoch : 69  loss = 281.79462  train MSE = 276.16733  test MSE = 66.5187\n",
      "Epoch : 70  loss = 656.22974  train MSE = 639.8035  test MSE = 66.6671\n",
      "Epoch : 71  loss = 148.53018  train MSE = 145.65744  test MSE = 66.77295\n",
      "Epoch : 72  loss = 633.41895  train MSE = 598.74866  test MSE = 67.09427\n",
      "Epoch : 73  loss = 489.9962  train MSE = 491.58862  test MSE = 67.88454\n",
      "Epoch : 74  loss = 393.0685  train MSE = 366.49854  test MSE = 69.038124\n",
      "Epoch : 75  loss = 81.44532  train MSE = 87.05468  test MSE = 70.008156\n",
      "Epoch : 76  loss = 112.24833  train MSE = 117.443726  test MSE = 71.32242\n",
      "Epoch : 77  loss = 518.21985  train MSE = 518.00305  test MSE = 73.35576\n",
      "Epoch : 78  loss = 159.53352  train MSE = 115.49978  test MSE = 75.64785\n",
      "Epoch : 79  loss = 2927.2876  train MSE = 2542.727  test MSE = 78.56436\n",
      "Epoch : 80  loss = 129.1663  train MSE = 90.37655  test MSE = 82.98479\n",
      "Epoch : 81  loss = 911.52264  train MSE = 814.68445  test MSE = 85.89111\n",
      "Epoch : 82  loss = 216.92827  train MSE = 222.57426  test MSE = 88.3823\n",
      "Epoch : 83  loss = 351.02792  train MSE = 360.6325  test MSE = 96.62014\n",
      "Epoch : 84  loss = 191.5588  train MSE = 194.31772  test MSE = 111.24528\n",
      "Epoch : 85  loss = 403.1595  train MSE = 371.01263  test MSE = 127.79998\n",
      "Epoch : 86  loss = 334.89728  train MSE = 308.33307  test MSE = 132.37856\n",
      "Epoch : 87  loss = 403.22253  train MSE = 414.0196  test MSE = 131.67952\n",
      "Epoch : 88  loss = 396.30624  train MSE = 395.51196  test MSE = 122.7802\n",
      "Epoch : 89  loss = 491.4857  train MSE = 568.211  test MSE = 112.6056\n",
      "Epoch : 90  loss = 207.11055  train MSE = 181.08481  test MSE = 106.15036\n",
      "Epoch : 91  loss = 475.94345  train MSE = 537.86914  test MSE = 104.010254\n",
      "Epoch : 92  loss = 245.71407  train MSE = 237.8002  test MSE = 98.905464\n",
      "Epoch : 93  loss = 1452.5953  train MSE = 1616.3335  test MSE = 101.23133\n",
      "Epoch : 94  loss = 1890.4797  train MSE = 2387.8452  test MSE = 114.01939\n",
      "Epoch : 95  loss = 247.25244  train MSE = 222.32903  test MSE = 145.25505\n",
      "Epoch : 96  loss = 385.70416  train MSE = 296.02197  test MSE = 149.23247\n",
      "Epoch : 97  loss = 345.36502  train MSE = 359.54233  test MSE = 147.83826\n",
      "Epoch : 98  loss = 279.0213  train MSE = 240.55173  test MSE = 145.81607\n",
      "Epoch : 99  loss = 282.27692  train MSE = 235.9692  test MSE = 153.20413\n",
      "Epoch : 100  loss = 184.23541  train MSE = 190.6824  test MSE = 158.01073\n",
      "Epoch : 101  loss = 367.62347  train MSE = 386.3678  test MSE = 148.73308\n",
      "Epoch : 102  loss = 343.52057  train MSE = 435.0341  test MSE = 143.50296\n",
      "Epoch : 103  loss = 849.33905  train MSE = 1010.2845  test MSE = 140.48495\n",
      "Epoch : 104  loss = 253.85703  train MSE = 220.36482  test MSE = 137.8903\n",
      "Epoch : 105  loss = 398.27222  train MSE = 410.36826  test MSE = 136.28876\n",
      "Epoch : 106  loss = 125.477455  train MSE = 125.31575  test MSE = 134.75676\n",
      "Epoch : 107  loss = 274.6916  train MSE = 237.56018  test MSE = 136.85684\n",
      "Epoch : 108  loss = 235.06357  train MSE = 311.49176  test MSE = 143.07704\n",
      "Epoch : 109  loss = 624.5542  train MSE = 554.4999  test MSE = 154.62701\n",
      "Epoch : 110  loss = 176.29361  train MSE = 184.38689  test MSE = 170.96298\n",
      "Epoch : 111  loss = 164.6523  train MSE = 158.32124  test MSE = 183.7189\n",
      "Epoch : 112  loss = 294.00732  train MSE = 231.46176  test MSE = 184.53171\n",
      "Epoch : 113  loss = 265.83185  train MSE = 244.73853  test MSE = 181.83243\n",
      "Epoch : 114  loss = 109.59424  train MSE = 108.50987  test MSE = 180.04466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 115  loss = 136.24477  train MSE = 116.71997  test MSE = 173.52295\n",
      "Epoch : 116  loss = 191.99588  train MSE = 269.56622  test MSE = 160.91373\n",
      "Epoch : 117  loss = 172.31985  train MSE = 145.1985  test MSE = 149.94322\n",
      "Epoch : 118  loss = 103.70083  train MSE = 103.23863  test MSE = 140.87868\n",
      "Epoch : 119  loss = 77.119255  train MSE = 63.658375  test MSE = 133.24347\n",
      "Epoch : 120  loss = 235.41219  train MSE = 226.4307  test MSE = 127.93069\n",
      "Epoch : 121  loss = 126.22544  train MSE = 105.81288  test MSE = 125.36485\n",
      "Epoch : 122  loss = 272.48126  train MSE = 321.4215  test MSE = 126.18845\n",
      "Epoch : 123  loss = 533.99304  train MSE = 528.14307  test MSE = 134.5804\n",
      "Epoch : 124  loss = 152.59354  train MSE = 151.51959  test MSE = 142.0101\n",
      "Epoch : 125  loss = 166.2496  train MSE = 156.9393  test MSE = 147.9304\n",
      "Epoch : 126  loss = 105.58933  train MSE = 89.673546  test MSE = 155.10077\n",
      "Epoch : 127  loss = 152.21935  train MSE = 157.02101  test MSE = 159.0513\n",
      "Epoch : 128  loss = 346.27545  train MSE = 395.54776  test MSE = 166.0523\n",
      "Epoch : 129  loss = 169.83165  train MSE = 156.43674  test MSE = 174.69969\n",
      "Epoch : 130  loss = 225.62988  train MSE = 239.0018  test MSE = 188.57861\n",
      "Epoch : 131  loss = 625.9489  train MSE = 536.9107  test MSE = 219.18132\n",
      "Epoch : 132  loss = 314.05786  train MSE = 409.8019  test MSE = 230.91084\n",
      "Epoch : 133  loss = 97.65753  train MSE = 120.533875  test MSE = 239.30608\n",
      "Epoch : 134  loss = 252.27847  train MSE = 224.52425  test MSE = 237.15114\n",
      "Epoch : 135  loss = 42.928715  train MSE = 35.22373  test MSE = 241.4374\n",
      "Epoch : 136  loss = 92.43855  train MSE = 67.95158  test MSE = 250.90997\n",
      "Epoch : 137  loss = 254.61728  train MSE = 242.8381  test MSE = 251.96211\n",
      "Epoch : 138  loss = 269.0284  train MSE = 205.20955  test MSE = 252.56186\n",
      "Epoch : 139  loss = 489.86597  train MSE = 430.46356  test MSE = 240.65099\n",
      "Epoch : 140  loss = 166.75282  train MSE = 105.15918  test MSE = 234.1207\n",
      "Epoch : 141  loss = 246.92119  train MSE = 244.23763  test MSE = 212.09746\n",
      "Epoch : 142  loss = 502.41718  train MSE = 418.55  test MSE = 185.06868\n",
      "Epoch : 143  loss = 98.91683  train MSE = 95.30708  test MSE = 160.42719\n",
      "Epoch : 144  loss = 125.49685  train MSE = 132.62045  test MSE = 138.82948\n",
      "Epoch : 145  loss = 163.24817  train MSE = 142.26509  test MSE = 122.182495\n",
      "Epoch : 146  loss = 38.68581  train MSE = 36.33787  test MSE = 110.67551\n",
      "Epoch : 147  loss = 230.49178  train MSE = 227.00822  test MSE = 103.40184\n",
      "Epoch : 148  loss = 1011.284  train MSE = 1005.9971  test MSE = 106.85334\n",
      "Epoch : 149  loss = 380.11368  train MSE = 377.88654  test MSE = 115.388115\n",
      "Epoch : 150  loss = 98.08285  train MSE = 84.25672  test MSE = 125.28272\n",
      "Epoch : 151  loss = 186.0596  train MSE = 200.90047  test MSE = 136.0223\n",
      "Epoch : 152  loss = 191.05722  train MSE = 176.69121  test MSE = 145.71571\n",
      "Epoch : 153  loss = 740.5703  train MSE = 795.3315  test MSE = 172.06091\n",
      "Epoch : 154  loss = 241.67728  train MSE = 230.80722  test MSE = 198.81248\n",
      "Epoch : 155  loss = 578.9755  train MSE = 595.50256  test MSE = 212.6969\n",
      "Epoch : 156  loss = 247.45987  train MSE = 223.35649  test MSE = 210.31348\n",
      "Epoch : 157  loss = 330.25385  train MSE = 274.58514  test MSE = 212.19855\n",
      "Epoch : 158  loss = 277.26126  train MSE = 148.68869  test MSE = 220.76918\n",
      "Epoch : 159  loss = 315.4477  train MSE = 361.12167  test MSE = 234.58122\n",
      "Epoch : 160  loss = 356.7649  train MSE = 256.66974  test MSE = 256.1965\n",
      "Epoch : 161  loss = 189.70726  train MSE = 212.64548  test MSE = 274.9891\n",
      "Epoch : 162  loss = 104.87575  train MSE = 99.69898  test MSE = 292.71118\n",
      "Epoch : 163  loss = 194.2072  train MSE = 148.49521  test MSE = 312.61935\n",
      "Epoch : 164  loss = 304.51724  train MSE = 244.08223  test MSE = 327.8079\n",
      "Epoch : 165  loss = 178.30875  train MSE = 233.87935  test MSE = 334.15314\n",
      "Epoch : 166  loss = 184.14458  train MSE = 137.75214  test MSE = 319.52863\n",
      "Epoch : 167  loss = 68.222916  train MSE = 55.55525  test MSE = 297.87463\n",
      "Epoch : 168  loss = 241.64456  train MSE = 186.85349  test MSE = 275.51865\n",
      "Epoch : 169  loss = 342.04865  train MSE = 355.2847  test MSE = 250.81165\n",
      "Epoch : 170  loss = 89.3644  train MSE = 113.084045  test MSE = 226.40501\n",
      "Epoch : 171  loss = 206.86307  train MSE = 223.2664  test MSE = 212.13371\n",
      "Epoch : 172  loss = 707.85565  train MSE = 280.28595  test MSE = 196.79517\n",
      "Epoch : 173  loss = 368.93073  train MSE = 467.57925  test MSE = 189.49272\n",
      "Epoch : 174  loss = 238.37924  train MSE = 210.51811  test MSE = 186.71774\n",
      "Epoch : 175  loss = 151.78964  train MSE = 136.01466  test MSE = 189.14493\n",
      "Epoch : 176  loss = 100.945946  train MSE = 96.90794  test MSE = 193.7136\n",
      "Epoch : 177  loss = 419.7133  train MSE = 406.204  test MSE = 200.9673\n",
      "Epoch : 178  loss = 286.96307  train MSE = 259.6909  test MSE = 202.12106\n",
      "Epoch : 179  loss = 330.89862  train MSE = 286.14075  test MSE = 207.89479\n",
      "Epoch : 180  loss = 94.303696  train MSE = 69.89798  test MSE = 211.64871\n",
      "Epoch : 181  loss = 329.1956  train MSE = 190.21233  test MSE = 214.7867\n",
      "Epoch : 182  loss = 111.915405  train MSE = 91.49083  test MSE = 212.28224\n",
      "Epoch : 183  loss = 216.99734  train MSE = 200.38663  test MSE = 209.5831\n",
      "Epoch : 184  loss = 120.96251  train MSE = 115.334145  test MSE = 210.72333\n",
      "Epoch : 185  loss = 114.29043  train MSE = 114.369606  test MSE = 214.449\n",
      "Epoch : 186  loss = 210.01851  train MSE = 236.59395  test MSE = 215.67296\n",
      "Epoch : 187  loss = 143.50221  train MSE = 156.63388  test MSE = 213.27515\n",
      "Epoch : 188  loss = 227.04703  train MSE = 221.96199  test MSE = 211.45868\n",
      "Epoch : 189  loss = 49.07452  train MSE = 51.34449  test MSE = 208.37633\n",
      "Epoch : 190  loss = 265.25284  train MSE = 247.68929  test MSE = 205.64304\n",
      "Epoch : 191  loss = 381.25897  train MSE = 392.84418  test MSE = 201.45041\n",
      "Epoch : 192  loss = 293.7587  train MSE = 208.51614  test MSE = 197.42079\n",
      "Epoch : 193  loss = 192.27145  train MSE = 159.50664  test MSE = 192.57616\n",
      "Epoch : 194  loss = 204.86636  train MSE = 267.43637  test MSE = 195.42381\n",
      "Epoch : 195  loss = 84.58392  train MSE = 76.91701  test MSE = 198.09947\n",
      "Epoch : 196  loss = 248.00266  train MSE = 292.47656  test MSE = 201.31654\n",
      "Epoch : 197  loss = 75.684586  train MSE = 51.901314  test MSE = 203.35713\n",
      "Epoch : 198  loss = 182.25272  train MSE = 172.78105  test MSE = 205.5176\n",
      "Epoch : 199  loss = 155.7714  train MSE = 122.159134  test MSE = 204.66003\n",
      "Epoch : 200  loss = 226.08496  train MSE = 238.33081  test MSE = 203.21475\n",
      "Epoch : 201  loss = 171.76826  train MSE = 150.82498  test MSE = 200.55458\n",
      "Epoch : 202  loss = 185.91322  train MSE = 176.64474  test MSE = 199.34303\n",
      "Epoch : 203  loss = 84.586426  train MSE = 82.84434  test MSE = 200.97269\n",
      "Epoch : 204  loss = 632.3267  train MSE = 553.2212  test MSE = 216.8489\n",
      "Epoch : 205  loss = 78.3557  train MSE = 83.04899  test MSE = 229.90314\n",
      "Epoch : 206  loss = 192.87946  train MSE = 185.01031  test MSE = 244.2986\n",
      "Epoch : 207  loss = 133.4707  train MSE = 102.65237  test MSE = 257.6296\n",
      "Epoch : 208  loss = 87.72298  train MSE = 141.57732  test MSE = 268.94025\n",
      "Epoch : 209  loss = 188.61201  train MSE = 220.80655  test MSE = 280.6899\n",
      "Epoch : 210  loss = 148.47363  train MSE = 113.80855  test MSE = 289.07773\n",
      "Epoch : 211  loss = 91.07571  train MSE = 131.75919  test MSE = 294.00046\n",
      "Epoch : 212  loss = 128.4662  train MSE = 129.899  test MSE = 299.46198\n",
      "Epoch : 213  loss = 56.08026  train MSE = 50.915016  test MSE = 303.51517\n",
      "Epoch : 214  loss = 150.47461  train MSE = 110.96448  test MSE = 307.1743\n",
      "Epoch : 215  loss = 85.66667  train MSE = 77.03105  test MSE = 316.69635\n",
      "Epoch : 216  loss = 169.42702  train MSE = 114.76003  test MSE = 333.9827\n",
      "Epoch : 217  loss = 202.54227  train MSE = 188.80823  test MSE = 354.94598\n",
      "Epoch : 218  loss = 56.749626  train MSE = 50.649178  test MSE = 375.63235\n",
      "Epoch : 219  loss = 80.64878  train MSE = 74.39418  test MSE = 377.09012\n",
      "Epoch : 220  loss = 192.16565  train MSE = 167.90149  test MSE = 377.75345\n",
      "Epoch : 221  loss = 361.4367  train MSE = 246.92212  test MSE = 376.29343\n",
      "Epoch : 222  loss = 1752.6377  train MSE = 854.522  test MSE = 332.6144\n",
      "Epoch : 223  loss = 82.41222  train MSE = 55.98354  test MSE = 295.13443\n",
      "Epoch : 224  loss = 235.77597  train MSE = 276.8044  test MSE = 275.55566\n",
      "Epoch : 225  loss = 170.80803  train MSE = 210.85751  test MSE = 253.64204\n",
      "Epoch : 226  loss = 66.17343  train MSE = 73.02199  test MSE = 226.47462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 227  loss = 221.28836  train MSE = 158.90463  test MSE = 202.04799\n",
      "Epoch : 228  loss = 120.82605  train MSE = 120.472244  test MSE = 186.9827\n",
      "Epoch : 229  loss = 147.47371  train MSE = 137.12343  test MSE = 171.92868\n",
      "Epoch : 230  loss = 296.28326  train MSE = 282.08725  test MSE = 165.80998\n",
      "Epoch : 231  loss = 100.00676  train MSE = 102.011185  test MSE = 161.62668\n",
      "Epoch : 232  loss = 175.96774  train MSE = 194.78365  test MSE = 160.1443\n",
      "Epoch : 233  loss = 112.81553  train MSE = 115.00828  test MSE = 161.63324\n",
      "Epoch : 234  loss = 97.51466  train MSE = 84.65485  test MSE = 166.06\n",
      "Epoch : 235  loss = 58.341278  train MSE = 53.163757  test MSE = 172.15076\n",
      "Epoch : 236  loss = 212.9047  train MSE = 207.03862  test MSE = 182.957\n",
      "Epoch : 237  loss = 161.7698  train MSE = 127.37064  test MSE = 194.59885\n",
      "Epoch : 238  loss = 44.701553  train MSE = 33.620247  test MSE = 204.99745\n",
      "Epoch : 239  loss = 81.3315  train MSE = 81.58029  test MSE = 215.44629\n",
      "Epoch : 240  loss = 165.20749  train MSE = 131.17018  test MSE = 223.70885\n",
      "Epoch : 241  loss = 189.89066  train MSE = 129.51355  test MSE = 223.53336\n",
      "Epoch : 242  loss = 56.64933  train MSE = 56.39469  test MSE = 224.89917\n",
      "Epoch : 243  loss = 224.83525  train MSE = 189.27861  test MSE = 221.20653\n",
      "Epoch : 244  loss = 59.450184  train MSE = 60.479774  test MSE = 219.89293\n",
      "Epoch : 245  loss = 61.664513  train MSE = 47.338005  test MSE = 222.10794\n",
      "Epoch : 246  loss = 53.865486  train MSE = 48.49386  test MSE = 225.5936\n",
      "Epoch : 247  loss = 108.34192  train MSE = 125.30464  test MSE = 231.51088\n",
      "Epoch : 248  loss = 109.90409  train MSE = 73.72601  test MSE = 231.43736\n",
      "Epoch : 249  loss = 127.12266  train MSE = 136.19322  test MSE = 227.26808\n",
      "Epoch : 250  loss = 49.554302  train MSE = 45.98569  test MSE = 223.5317\n",
      "Epoch : 251  loss = 113.48102  train MSE = 119.36247  test MSE = 219.23006\n",
      "Epoch : 252  loss = 121.0599  train MSE = 121.792336  test MSE = 219.41913\n",
      "Epoch : 253  loss = 80.52346  train MSE = 84.01176  test MSE = 219.69014\n",
      "Epoch : 254  loss = 45.146152  train MSE = 48.483223  test MSE = 222.2948\n",
      "Epoch : 255  loss = 94.92131  train MSE = 148.86105  test MSE = 233.62524\n",
      "Epoch : 256  loss = 63.120617  train MSE = 57.036907  test MSE = 242.56084\n",
      "Epoch : 257  loss = 47.736156  train MSE = 33.215435  test MSE = 247.00215\n",
      "Epoch : 258  loss = 97.79811  train MSE = 79.353165  test MSE = 250.5495\n",
      "Epoch : 259  loss = 56.540977  train MSE = 52.556072  test MSE = 250.61218\n",
      "Epoch : 260  loss = 189.6855  train MSE = 144.82187  test MSE = 251.49829\n",
      "Epoch : 261  loss = 114.150085  train MSE = 86.04397  test MSE = 249.03635\n",
      "Epoch : 262  loss = 155.60399  train MSE = 83.76381  test MSE = 253.93971\n",
      "Epoch : 263  loss = 147.93637  train MSE = 169.7941  test MSE = 257.49338\n",
      "Epoch : 264  loss = 105.29985  train MSE = 91.20607  test MSE = 262.68533\n",
      "Epoch : 265  loss = 38.379116  train MSE = 47.016594  test MSE = 265.83548\n",
      "Epoch : 266  loss = 123.9619  train MSE = 175.3197  test MSE = 266.13498\n",
      "Epoch : 267  loss = 166.66986  train MSE = 169.75494  test MSE = 270.29202\n",
      "Epoch : 268  loss = 43.893677  train MSE = 28.322361  test MSE = 277.97745\n",
      "Epoch : 269  loss = 161.00508  train MSE = 93.61711  test MSE = 287.4551\n",
      "Epoch : 270  loss = 76.264084  train MSE = 65.09914  test MSE = 302.8906\n",
      "Epoch : 271  loss = 91.25679  train MSE = 96.592636  test MSE = 309.4396\n",
      "Epoch : 272  loss = 67.52057  train MSE = 52.7765  test MSE = 306.84048\n",
      "Epoch : 273  loss = 101.70694  train MSE = 93.325485  test MSE = 289.17654\n",
      "Epoch : 274  loss = 74.95998  train MSE = 78.63303  test MSE = 272.76746\n",
      "Epoch : 275  loss = 152.89287  train MSE = 53.649162  test MSE = 257.08945\n",
      "Epoch : 276  loss = 104.353745  train MSE = 95.833115  test MSE = 242.6816\n",
      "Epoch : 277  loss = 64.13666  train MSE = 67.60444  test MSE = 226.38638\n",
      "Epoch : 278  loss = 84.299095  train MSE = 66.016525  test MSE = 210.00014\n",
      "Epoch : 279  loss = 339.4274  train MSE = 259.82907  test MSE = 186.89395\n",
      "Epoch : 280  loss = 73.28904  train MSE = 79.844505  test MSE = 171.06003\n",
      "Epoch : 281  loss = 178.4784  train MSE = 219.52205  test MSE = 163.86777\n",
      "Epoch : 282  loss = 113.9691  train MSE = 120.39421  test MSE = 158.81784\n",
      "Epoch : 283  loss = 51.922844  train MSE = 48.33744  test MSE = 153.72157\n",
      "Epoch : 284  loss = 68.517426  train MSE = 63.22863  test MSE = 150.70625\n",
      "Epoch : 285  loss = 77.56081  train MSE = 82.08061  test MSE = 148.74483\n",
      "Epoch : 286  loss = 16.79694  train MSE = 104.9979  test MSE = 151.75299\n",
      "Epoch : 287  loss = 63.74893  train MSE = 65.15733  test MSE = 157.54544\n",
      "Epoch : 288  loss = 127.20534  train MSE = 124.71886  test MSE = 164.50717\n",
      "Epoch : 289  loss = 228.56038  train MSE = 229.49629  test MSE = 171.72061\n",
      "Epoch : 290  loss = 94.495125  train MSE = 90.281845  test MSE = 178.85449\n",
      "Epoch : 291  loss = 158.6138  train MSE = 162.53014  test MSE = 186.5651\n",
      "Epoch : 292  loss = 271.19354  train MSE = 128.80045  test MSE = 198.45035\n",
      "Epoch : 293  loss = 145.88077  train MSE = 250.81851  test MSE = 202.73698\n",
      "Epoch : 294  loss = 258.1651  train MSE = 213.79204  test MSE = 202.70982\n",
      "Epoch : 295  loss = 116.3765  train MSE = 122.34169  test MSE = 203.79213\n",
      "Epoch : 296  loss = 54.774525  train MSE = 48.168747  test MSE = 205.3467\n",
      "Epoch : 297  loss = 76.50522  train MSE = 76.35507  test MSE = 207.93373\n",
      "Epoch : 298  loss = 209.32114  train MSE = 139.89468  test MSE = 210.15994\n",
      "Epoch : 299  loss = 314.07562  train MSE = 270.123  test MSE = 215.2122\n",
      "Epoch : 300  loss = 61.006874  train MSE = 63.100414  test MSE = 216.68056\n",
      "Epoch : 301  loss = 54.636803  train MSE = 48.274723  test MSE = 213.47754\n",
      "Epoch : 302  loss = 42.07225  train MSE = 45.707794  test MSE = 208.62695\n",
      "Epoch : 303  loss = 411.15167  train MSE = 429.32217  test MSE = 202.70886\n",
      "Epoch : 304  loss = 67.9092  train MSE = 59.559742  test MSE = 200.18164\n",
      "Epoch : 305  loss = 75.5693  train MSE = 53.80005  test MSE = 198.81526\n",
      "Epoch : 306  loss = 87.29595  train MSE = 83.67489  test MSE = 196.80235\n",
      "Epoch : 307  loss = 89.54183  train MSE = 62.982323  test MSE = 195.5646\n",
      "Epoch : 308  loss = 72.32283  train MSE = 56.998516  test MSE = 195.05177\n",
      "Epoch : 309  loss = 112.37697  train MSE = 118.24959  test MSE = 196.30737\n",
      "Epoch : 310  loss = 128.96223  train MSE = 120.219505  test MSE = 198.47046\n",
      "Epoch : 311  loss = 38.931313  train MSE = 42.467415  test MSE = 202.52269\n",
      "Epoch : 312  loss = 59.13368  train MSE = 43.787617  test MSE = 208.37267\n",
      "Epoch : 313  loss = 45.42523  train MSE = 48.788307  test MSE = 216.23155\n",
      "Epoch : 314  loss = 83.472435  train MSE = 79.14813  test MSE = 220.08777\n",
      "Epoch : 315  loss = 30.439753  train MSE = 29.560223  test MSE = 223.96397\n",
      "Epoch : 316  loss = 181.20346  train MSE = 47.599976  test MSE = 223.04915\n",
      "Epoch : 317  loss = 72.60918  train MSE = 65.61671  test MSE = 221.81393\n",
      "Epoch : 318  loss = 30.239801  train MSE = 20.138626  test MSE = 217.86537\n",
      "Epoch : 319  loss = 86.41237  train MSE = 80.57861  test MSE = 216.15408\n",
      "Epoch : 320  loss = 60.862976  train MSE = 84.04692  test MSE = 216.29471\n",
      "Epoch : 321  loss = 61.34812  train MSE = 59.07792  test MSE = 216.58536\n",
      "Epoch : 322  loss = 137.97212  train MSE = 100.52174  test MSE = 219.07445\n",
      "Epoch : 323  loss = 60.452374  train MSE = 56.88376  test MSE = 221.12431\n",
      "Epoch : 324  loss = 18.986088  train MSE = 27.181585  test MSE = 223.14662\n",
      "Epoch : 325  loss = 367.54752  train MSE = 378.3818  test MSE = 226.82625\n",
      "Epoch : 326  loss = 168.86038  train MSE = 173.06711  test MSE = 231.41339\n",
      "Epoch : 327  loss = 104.37821  train MSE = 46.213837  test MSE = 238.91806\n",
      "Epoch : 328  loss = 160.46382  train MSE = 211.31189  test MSE = 248.2988\n",
      "Epoch : 329  loss = 64.35974  train MSE = 82.67442  test MSE = 254.17188\n",
      "Epoch : 330  loss = 139.27646  train MSE = 79.79118  test MSE = 257.74442\n",
      "Epoch : 331  loss = 63.05979  train MSE = 58.054985  test MSE = 264.53726\n",
      "Epoch : 332  loss = 168.3992  train MSE = 115.72117  test MSE = 276.33276\n",
      "Epoch : 333  loss = 230.4847  train MSE = 184.21982  test MSE = 285.82727\n",
      "Epoch : 334  loss = 134.2795  train MSE = 109.00364  test MSE = 291.01862\n",
      "Epoch : 335  loss = 40.32445  train MSE = 36.576866  test MSE = 293.9034\n",
      "Epoch : 336  loss = 70.346275  train MSE = 68.204994  test MSE = 296.71503\n",
      "Epoch : 337  loss = 60.231926  train MSE = 45.097534  test MSE = 299.28455\n",
      "Epoch : 338  loss = 297.16983  train MSE = 161.51508  test MSE = 279.90442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 339  loss = 113.39357  train MSE = 80.04214  test MSE = 264.4622\n",
      "Epoch : 340  loss = 63.535973  train MSE = 54.687855  test MSE = 250.40282\n",
      "Epoch : 341  loss = 49.824196  train MSE = 46.473427  test MSE = 237.76862\n",
      "Epoch : 342  loss = 163.36533  train MSE = 292.3852  test MSE = 232.2991\n",
      "Epoch : 343  loss = 230.21822  train MSE = 247.1189  test MSE = 226.02786\n",
      "Epoch : 344  loss = 87.56529  train MSE = 75.98921  test MSE = 218.952\n",
      "Epoch : 345  loss = 55.584583  train MSE = 63.3406  test MSE = 211.9453\n",
      "Epoch : 346  loss = 165.63538  train MSE = 172.25616  test MSE = 208.12433\n",
      "Epoch : 347  loss = 54.657776  train MSE = 48.163776  test MSE = 203.97832\n",
      "Epoch : 348  loss = 180.6801  train MSE = 163.30807  test MSE = 199.99693\n",
      "Epoch : 349  loss = 76.873055  train MSE = 92.65615  test MSE = 198.36525\n",
      "Epoch : 350  loss = 211.26004  train MSE = 193.91063  test MSE = 197.91606\n",
      "Epoch : 351  loss = 107.00037  train MSE = 98.79756  test MSE = 198.3822\n",
      "Epoch : 352  loss = 123.99609  train MSE = 128.83215  test MSE = 200.00851\n",
      "Epoch : 353  loss = 162.81937  train MSE = 204.13277  test MSE = 204.39706\n",
      "Epoch : 354  loss = 81.06557  train MSE = 80.80684  test MSE = 209.55577\n",
      "Epoch : 355  loss = 87.746994  train MSE = 59.596516  test MSE = 215.95386\n",
      "Epoch : 356  loss = 172.01758  train MSE = 219.21164  test MSE = 225.8339\n",
      "Epoch : 357  loss = 60.064198  train MSE = 74.04549  test MSE = 235.91113\n",
      "Epoch : 358  loss = 339.62747  train MSE = 309.5761  test MSE = 241.86209\n",
      "Epoch : 359  loss = 125.15831  train MSE = 88.396545  test MSE = 246.73735\n",
      "Epoch : 360  loss = 186.45285  train MSE = 130.53871  test MSE = 241.61545\n",
      "Epoch : 361  loss = 31.634201  train MSE = 23.750645  test MSE = 236.67783\n",
      "Epoch : 362  loss = 45.82377  train MSE = 32.666363  test MSE = 232.35172\n",
      "Epoch : 363  loss = 29.393866  train MSE = 25.90086  test MSE = 229.93208\n",
      "Epoch : 364  loss = 60.64158  train MSE = 49.70968  test MSE = 229.5184\n",
      "Epoch : 365  loss = 109.820595  train MSE = 42.939034  test MSE = 231.62253\n",
      "Epoch : 366  loss = 245.25883  train MSE = 204.6031  test MSE = 235.59909\n",
      "Epoch : 367  loss = 88.48957  train MSE = 86.34745  test MSE = 238.94783\n",
      "Epoch : 368  loss = 126.17546  train MSE = 160.56038  test MSE = 245.78032\n",
      "Epoch : 369  loss = 202.87097  train MSE = 204.77861  test MSE = 253.42706\n",
      "Epoch : 370  loss = 88.00964  train MSE = 61.80409  test MSE = 259.13892\n",
      "Epoch : 371  loss = 242.14412  train MSE = 180.5167  test MSE = 263.31137\n",
      "Epoch : 372  loss = 83.147865  train MSE = 77.66994  test MSE = 269.0382\n",
      "Epoch : 373  loss = 48.951057  train MSE = 49.74407  test MSE = 274.7532\n",
      "Epoch : 374  loss = 145.12645  train MSE = 134.55766  test MSE = 277.91724\n",
      "Epoch : 375  loss = 84.42116  train MSE = 27.676228  test MSE = 282.2442\n",
      "Epoch : 376  loss = 102.28908  train MSE = 53.103752  test MSE = 281.51166\n",
      "Epoch : 377  loss = 83.64513  train MSE = 70.68443  test MSE = 279.63303\n",
      "Epoch : 378  loss = 102.144775  train MSE = 65.69444  test MSE = 276.16602\n",
      "Epoch : 379  loss = 40.5784  train MSE = 34.429615  test MSE = 275.99448\n",
      "Epoch : 380  loss = 72.8207  train MSE = 61.131145  test MSE = 276.49786\n",
      "Epoch : 381  loss = 45.367233  train MSE = 46.89834  test MSE = 276.87167\n",
      "Epoch : 382  loss = 59.270245  train MSE = 46.792725  test MSE = 279.76733\n",
      "Epoch : 383  loss = 50.33318  train MSE = 47.055355  test MSE = 274.35046\n",
      "Epoch : 384  loss = 33.83375  train MSE = 24.644321  test MSE = 269.14838\n",
      "Epoch : 385  loss = 106.83119  train MSE = 93.95455  test MSE = 266.0981\n",
      "Epoch : 386  loss = 47.207905  train MSE = 41.36435  test MSE = 262.89075\n",
      "Epoch : 387  loss = 211.55994  train MSE = 242.74002  test MSE = 264.5263\n",
      "Epoch : 388  loss = 67.89191  train MSE = 66.12233  test MSE = 269.4469\n",
      "Epoch : 389  loss = 409.7191  train MSE = 119.41478  test MSE = 274.20456\n",
      "Epoch : 390  loss = 83.00884  train MSE = 74.23283  test MSE = 277.62717\n",
      "Epoch : 391  loss = 104.28507  train MSE = 99.90229  test MSE = 278.13348\n",
      "Epoch : 392  loss = 105.90487  train MSE = 26.295559  test MSE = 278.79056\n",
      "Epoch : 393  loss = 108.78471  train MSE = 112.59202  test MSE = 280.39514\n",
      "Epoch : 394  loss = 29.113037  train MSE = 21.270468  test MSE = 278.6988\n",
      "Epoch : 395  loss = 27.251337  train MSE = 30.085337  test MSE = 277.8313\n",
      "Epoch : 396  loss = 29.48457  train MSE = 32.11179  test MSE = 276.58737\n",
      "Epoch : 397  loss = 95.059494  train MSE = 104.0974  test MSE = 277.87048\n",
      "Epoch : 398  loss = 67.034935  train MSE = 41.357124  test MSE = 281.48502\n",
      "Epoch : 399  loss = 270.4273  train MSE = 21.67056  test MSE = 279.98984\n",
      "Epoch : 400  loss = 30.010452  train MSE = 27.218754  test MSE = 277.26242\n",
      "Epoch : 401  loss = 79.15981  train MSE = 88.13595  test MSE = 275.2231\n",
      "Epoch : 402  loss = 74.45172  train MSE = 71.0782  test MSE = 270.72644\n",
      "Epoch : 403  loss = 52.16814  train MSE = 57.325294  test MSE = 263.39832\n",
      "Epoch : 404  loss = 42.43239  train MSE = 72.95234  test MSE = 262.49094\n",
      "Epoch : 405  loss = 46.611084  train MSE = 42.89868  test MSE = 266.07727\n",
      "Epoch : 406  loss = 46.890846  train MSE = 43.685116  test MSE = 270.31506\n",
      "Epoch : 407  loss = 21.371761  train MSE = 21.062063  test MSE = 277.07684\n",
      "Epoch : 408  loss = 121.44082  train MSE = 47.215958  test MSE = 284.88568\n",
      "Epoch : 409  loss = 44.687393  train MSE = 41.372368  test MSE = 288.54733\n",
      "Epoch : 410  loss = 35.876194  train MSE = 48.62105  test MSE = 292.03247\n",
      "Epoch : 411  loss = 95.53813  train MSE = 85.56833  test MSE = 291.61176\n",
      "Epoch : 412  loss = 159.00194  train MSE = 42.626225  test MSE = 277.6768\n",
      "Epoch : 413  loss = 197.12662  train MSE = 71.28079  test MSE = 267.55106\n",
      "Epoch : 414  loss = 59.952576  train MSE = 47.91398  test MSE = 256.7853\n",
      "Epoch : 415  loss = 71.70815  train MSE = 75.265335  test MSE = 247.29964\n",
      "Epoch : 416  loss = 42.936268  train MSE = 27.66987  test MSE = 237.602\n",
      "Epoch : 417  loss = 55.7873  train MSE = 49.848255  test MSE = 227.798\n",
      "Epoch : 418  loss = 92.99559  train MSE = 106.51133  test MSE = 222.69913\n",
      "Epoch : 419  loss = 28.823923  train MSE = 27.914967  test MSE = 218.58485\n",
      "Epoch : 420  loss = 49.445885  train MSE = 44.36747  test MSE = 214.42274\n",
      "Epoch : 421  loss = 32.021263  train MSE = 34.803207  test MSE = 210.26883\n",
      "Epoch : 422  loss = 110.0563  train MSE = 111.96588  test MSE = 203.19058\n",
      "Epoch : 423  loss = 74.59638  train MSE = 66.49516  test MSE = 201.09395\n",
      "Epoch : 424  loss = 67.877335  train MSE = 71.42216  test MSE = 199.31206\n",
      "Epoch : 425  loss = 78.38646  train MSE = 64.413925  test MSE = 197.71051\n",
      "Epoch : 426  loss = 48.548668  train MSE = 41.32142  test MSE = 198.23088\n",
      "Epoch : 427  loss = 53.502098  train MSE = 60.987938  test MSE = 199.51291\n",
      "Epoch : 428  loss = 63.105125  train MSE = 85.54273  test MSE = 201.43044\n",
      "Epoch : 429  loss = 57.92286  train MSE = 42.93507  test MSE = 204.56247\n",
      "Epoch : 430  loss = 95.324936  train MSE = 112.986435  test MSE = 208.77733\n",
      "Epoch : 431  loss = 41.53658  train MSE = 36.653313  test MSE = 213.32741\n",
      "Epoch : 432  loss = 114.11647  train MSE = 90.32265  test MSE = 217.29721\n",
      "Epoch : 433  loss = 35.226803  train MSE = 28.186829  test MSE = 220.87683\n",
      "Epoch : 434  loss = 61.89808  train MSE = 62.207573  test MSE = 223.52553\n",
      "Epoch : 435  loss = 143.34943  train MSE = 155.01443  test MSE = 227.26944\n",
      "Epoch : 436  loss = 66.229294  train MSE = 34.251625  test MSE = 230.5849\n",
      "Epoch : 437  loss = 113.58003  train MSE = 87.16863  test MSE = 234.37872\n",
      "Epoch : 438  loss = 79.09099  train MSE = 25.063417  test MSE = 237.8015\n",
      "Epoch : 439  loss = 57.96485  train MSE = 48.26804  test MSE = 240.492\n",
      "Epoch : 440  loss = 36.646446  train MSE = 22.744076  test MSE = 243.04897\n",
      "Epoch : 441  loss = 358.49127  train MSE = 177.33234  test MSE = 234.44614\n",
      "Epoch : 442  loss = 59.55359  train MSE = 79.52157  test MSE = 228.45508\n",
      "Epoch : 443  loss = 73.79831  train MSE = 71.033035  test MSE = 223.57152\n",
      "Epoch : 444  loss = 48.94946  train MSE = 42.86417  test MSE = 219.9035\n",
      "Epoch : 445  loss = 125.96228  train MSE = 167.59781  test MSE = 214.87534\n",
      "Epoch : 446  loss = 93.836426  train MSE = 95.91961  test MSE = 213.18066\n",
      "Epoch : 447  loss = 120.88867  train MSE = 96.90773  test MSE = 211.09622\n",
      "Epoch : 448  loss = 36.774036  train MSE = 36.23989  test MSE = 209.96225\n",
      "Epoch : 449  loss = 211.58562  train MSE = 129.60445  test MSE = 210.05807\n",
      "Epoch : 450  loss = 24.133095  train MSE = 21.157633  test MSE = 209.83238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 451  loss = 19.671925  train MSE = 21.607973  test MSE = 209.28455\n",
      "Epoch : 452  loss = 44.969257  train MSE = 46.19841  test MSE = 207.72157\n",
      "Epoch : 453  loss = 51.84304  train MSE = 56.13872  test MSE = 207.28685\n",
      "Epoch : 454  loss = 81.520004  train MSE = 50.578743  test MSE = 208.98413\n",
      "Epoch : 455  loss = 32.235  train MSE = 35.633877  test MSE = 210.6371\n",
      "Epoch : 456  loss = 46.027916  train MSE = 34.88559  test MSE = 213.84259\n",
      "Epoch : 457  loss = 61.31846  train MSE = 54.22234  test MSE = 218.21623\n",
      "Epoch : 458  loss = 56.13849  train MSE = 48.94593  test MSE = 226.65556\n",
      "Epoch : 459  loss = 141.78181  train MSE = 92.733086  test MSE = 236.22923\n",
      "Epoch : 460  loss = 41.84375  train MSE = 35.294212  test MSE = 247.33612\n",
      "Epoch : 461  loss = 161.89613  train MSE = 138.59866  test MSE = 261.91092\n",
      "Epoch : 462  loss = 44.10285  train MSE = 48.180714  test MSE = 274.4532\n",
      "Epoch : 463  loss = 132.30402  train MSE = 77.19159  test MSE = 286.8992\n",
      "Epoch : 464  loss = 36.435825  train MSE = 37.80863  test MSE = 296.5\n",
      "Epoch : 465  loss = 95.861984  train MSE = 123.78971  test MSE = 304.34454\n",
      "Epoch : 466  loss = 35.656677  train MSE = 38.86111  test MSE = 310.5113\n",
      "Epoch : 467  loss = 128.86148  train MSE = 94.91127  test MSE = 302.222\n",
      "Epoch : 468  loss = 90.31084  train MSE = 54.344738  test MSE = 295.78497\n",
      "Epoch : 469  loss = 354.4315  train MSE = 267.28552  test MSE = 291.10767\n",
      "Epoch : 470  loss = 33.894127  train MSE = 30.919552  test MSE = 283.6474\n",
      "Epoch : 471  loss = 89.385994  train MSE = 98.93109  test MSE = 277.25278\n",
      "Epoch : 472  loss = 61.69618  train MSE = 91.11052  test MSE = 269.63647\n",
      "Epoch : 473  loss = 153.12541  train MSE = 64.10996  test MSE = 261.49365\n",
      "Epoch : 474  loss = 102.16368  train MSE = 47.118538  test MSE = 260.26007\n",
      "Epoch : 475  loss = 42.058693  train MSE = 35.337406  test MSE = 259.31287\n",
      "Epoch : 476  loss = 51.26179  train MSE = 41.27478  test MSE = 258.99695\n",
      "Epoch : 477  loss = 57.760754  train MSE = 123.97136  test MSE = 255.01518\n",
      "Epoch : 478  loss = 51.5401  train MSE = 26.687353  test MSE = 252.60855\n",
      "Epoch : 479  loss = 54.182056  train MSE = 54.773945  test MSE = 251.90929\n",
      "Epoch : 480  loss = 110.522766  train MSE = 44.910088  test MSE = 248.5449\n",
      "Epoch : 481  loss = 224.95358  train MSE = 167.61511  test MSE = 245.48021\n",
      "Epoch : 482  loss = 130.86171  train MSE = 106.28408  test MSE = 243.44075\n",
      "Epoch : 483  loss = 75.98783  train MSE = 71.95941  test MSE = 241.17384\n",
      "Epoch : 484  loss = 54.54119  train MSE = 40.232346  test MSE = 236.90755\n",
      "Epoch : 485  loss = 55.021893  train MSE = 30.003239  test MSE = 235.85777\n",
      "Epoch : 486  loss = 19.00703  train MSE = 13.450292  test MSE = 235.71002\n",
      "Epoch : 487  loss = 105.92444  train MSE = 46.530773  test MSE = 234.95683\n",
      "Epoch : 488  loss = 125.62512  train MSE = 95.57643  test MSE = 231.8134\n",
      "Epoch : 489  loss = 78.466934  train MSE = 68.827995  test MSE = 228.79703\n",
      "Epoch : 490  loss = 48.350475  train MSE = 52.50564  test MSE = 223.15875\n",
      "Epoch : 491  loss = 29.118744  train MSE = 34.651672  test MSE = 219.20607\n",
      "Epoch : 492  loss = 49.287556  train MSE = 21.328753  test MSE = 208.62726\n",
      "Epoch : 493  loss = 51.948803  train MSE = 36.136646  test MSE = 197.18037\n",
      "Epoch : 494  loss = 49.411114  train MSE = 45.047375  test MSE = 185.48642\n",
      "Epoch : 495  loss = 38.994987  train MSE = 30.790253  test MSE = 176.33582\n",
      "Epoch : 496  loss = 73.15318  train MSE = 56.213184  test MSE = 169.99652\n",
      "Epoch : 497  loss = 130.80838  train MSE = 95.41123  test MSE = 165.91643\n",
      "Epoch : 498  loss = 235.58562  train MSE = 160.01462  test MSE = 167.39204\n",
      "Epoch : 499  loss = 32.160027  train MSE = 76.002075  test MSE = 171.38681\n",
      "Epoch : 500  loss = 26.306433  train MSE = 22.73122  test MSE = 175.77931\n",
      "Epoch : 501  loss = 44.5442  train MSE = 29.91808  test MSE = 180.40872\n",
      "Epoch : 502  loss = 68.14772  train MSE = 58.463978  test MSE = 188.16022\n",
      "Epoch : 503  loss = 30.953197  train MSE = 21.59634  test MSE = 195.86488\n",
      "Epoch : 504  loss = 28.612738  train MSE = 25.687206  test MSE = 204.1713\n",
      "Epoch : 505  loss = 44.181396  train MSE = 17.323967  test MSE = 212.78421\n",
      "Epoch : 506  loss = 54.467346  train MSE = 46.596718  test MSE = 222.57167\n",
      "Epoch : 507  loss = 32.588463  train MSE = 23.652288  test MSE = 233.4827\n",
      "Epoch : 508  loss = 176.94559  train MSE = 192.9145  test MSE = 244.2385\n",
      "Epoch : 509  loss = 46.74532  train MSE = 25.119623  test MSE = 252.64551\n",
      "Epoch : 510  loss = 33.506844  train MSE = 24.94928  test MSE = 262.10696\n",
      "Epoch : 511  loss = 79.70728  train MSE = 91.90029  test MSE = 263.0975\n",
      "Epoch : 512  loss = 65.88251  train MSE = 55.899853  test MSE = 261.79465\n",
      "Epoch : 513  loss = 44.835438  train MSE = 33.08418  test MSE = 260.46872\n",
      "Epoch : 514  loss = 746.7582  train MSE = 409.57492  test MSE = 243.5121\n",
      "Epoch : 515  loss = 492.20914  train MSE = 119.737305  test MSE = 220.87694\n",
      "Epoch : 516  loss = 39.689766  train MSE = 41.153107  test MSE = 202.80267\n",
      "Epoch : 517  loss = 159.44089  train MSE = 169.31168  test MSE = 187.67029\n",
      "Epoch : 518  loss = 65.8461  train MSE = 60.46762  test MSE = 174.97409\n",
      "Epoch : 519  loss = 38.38672  train MSE = 36.149025  test MSE = 164.49374\n",
      "Epoch : 520  loss = 210.82031  train MSE = 131.56693  test MSE = 157.90825\n",
      "Epoch : 521  loss = 99.80058  train MSE = 73.61915  test MSE = 152.78651\n",
      "Epoch : 522  loss = 80.32783  train MSE = 73.791794  test MSE = 149.11029\n",
      "Epoch : 523  loss = 108.68945  train MSE = 91.716385  test MSE = 146.83943\n",
      "Epoch : 524  loss = 33.399498  train MSE = 34.374573  test MSE = 146.11684\n",
      "Epoch : 525  loss = 242.40768  train MSE = 145.55513  test MSE = 148.0184\n",
      "Epoch : 526  loss = 100.48029  train MSE = 111.38927  test MSE = 150.07497\n",
      "Epoch : 527  loss = 102.0281  train MSE = 87.16122  test MSE = 153.87595\n",
      "Epoch : 528  loss = 327.23724  train MSE = 288.54477  test MSE = 159.68784\n",
      "Epoch : 529  loss = 89.50822  train MSE = 92.886925  test MSE = 166.40703\n",
      "Epoch : 530  loss = 66.73689  train MSE = 46.033886  test MSE = 174.02577\n",
      "Epoch : 531  loss = 18.27386  train MSE = 16.345867  test MSE = 182.34071\n",
      "Epoch : 532  loss = 35.838596  train MSE = 35.799725  test MSE = 191.12057\n",
      "Epoch : 533  loss = 115.635056  train MSE = 127.9891  test MSE = 200.61684\n",
      "Epoch : 534  loss = 235.3995  train MSE = 128.78052  test MSE = 211.50392\n",
      "Epoch : 535  loss = 51.11225  train MSE = 36.24341  test MSE = 221.36568\n",
      "Epoch : 536  loss = 56.960835  train MSE = 45.319702  test MSE = 231.42862\n",
      "Epoch : 537  loss = 19.578514  train MSE = 27.127161  test MSE = 237.58585\n",
      "Epoch : 538  loss = 99.000465  train MSE = 36.32896  test MSE = 244.2224\n",
      "Epoch : 539  loss = 114.379135  train MSE = 108.590904  test MSE = 248.57193\n",
      "Epoch : 540  loss = 46.47801  train MSE = 45.505474  test MSE = 252.44374\n",
      "Epoch : 541  loss = 146.87808  train MSE = 98.73069  test MSE = 256.97696\n",
      "Epoch : 542  loss = 27.95191  train MSE = 42.043304  test MSE = 255.37575\n",
      "Epoch : 543  loss = 120.84695  train MSE = 80.45623  test MSE = 257.67853\n",
      "Epoch : 544  loss = 59.288795  train MSE = 29.241444  test MSE = 255.91394\n",
      "Epoch : 545  loss = 75.50359  train MSE = 55.552197  test MSE = 249.87082\n",
      "Epoch : 546  loss = 71.22034  train MSE = 59.28227  test MSE = 243.24155\n",
      "Epoch : 547  loss = 150.27744  train MSE = 149.4187  test MSE = 238.82248\n",
      "Epoch : 548  loss = 100.108315  train MSE = 31.481867  test MSE = 234.2038\n",
      "Epoch : 549  loss = 55.063667  train MSE = 55.59857  test MSE = 230.56967\n",
      "Epoch : 550  loss = 45.680275  train MSE = 35.903645  test MSE = 227.16866\n",
      "Epoch : 551  loss = 48.63148  train MSE = 44.15186  test MSE = 224.65953\n",
      "Epoch : 552  loss = 211.22527  train MSE = 74.26647  test MSE = 224.03705\n",
      "Epoch : 553  loss = 140.84238  train MSE = 82.304535  test MSE = 220.622\n",
      "Epoch : 554  loss = 159.37047  train MSE = 71.34456  test MSE = 217.89879\n",
      "Epoch : 555  loss = 133.0188  train MSE = 51.394005  test MSE = 215.28552\n",
      "Epoch : 556  loss = 86.73271  train MSE = 176.53429  test MSE = 216.89334\n",
      "Epoch : 557  loss = 57.79128  train MSE = 42.877033  test MSE = 219.69188\n",
      "Epoch : 558  loss = 78.92284  train MSE = 100.3379  test MSE = 222.87086\n",
      "Epoch : 559  loss = 39.117638  train MSE = 31.691513  test MSE = 225.11024\n",
      "Epoch : 560  loss = 39.943226  train MSE = 25.966633  test MSE = 227.96103\n",
      "Epoch : 561  loss = 29.519705  train MSE = 24.94169  test MSE = 230.34657\n",
      "Epoch : 562  loss = 92.88088  train MSE = 68.7146  test MSE = 234.0643\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 563  loss = 26.56222  train MSE = 28.589834  test MSE = 237.77121\n",
      "Epoch : 564  loss = 64.530106  train MSE = 39.974464  test MSE = 242.26457\n",
      "Epoch : 565  loss = 96.127365  train MSE = 51.735374  test MSE = 248.99861\n",
      "Epoch : 566  loss = 104.145615  train MSE = 44.169228  test MSE = 254.31859\n",
      "Epoch : 567  loss = 18.642765  train MSE = 20.293896  test MSE = 258.95108\n",
      "Epoch : 568  loss = 24.664808  train MSE = 24.567764  test MSE = 261.0629\n",
      "Epoch : 569  loss = 36.917297  train MSE = 20.828295  test MSE = 261.5875\n",
      "Epoch : 570  loss = 75.29323  train MSE = 43.086624  test MSE = 258.63037\n",
      "Epoch : 571  loss = 45.70324  train MSE = 26.992273  test MSE = 257.2519\n",
      "Epoch : 572  loss = 138.02351  train MSE = 79.55748  test MSE = 254.22313\n",
      "Epoch : 573  loss = 57.453175  train MSE = 43.226692  test MSE = 244.20645\n",
      "Epoch : 574  loss = 24.303263  train MSE = 18.636808  test MSE = 233.10211\n",
      "Epoch : 575  loss = 28.79946  train MSE = 25.04509  test MSE = 223.87766\n",
      "Epoch : 576  loss = 64.53743  train MSE = 35.683983  test MSE = 216.51947\n",
      "Epoch : 577  loss = 28.797436  train MSE = 17.373163  test MSE = 210.9406\n",
      "Epoch : 578  loss = 133.45294  train MSE = 153.22049  test MSE = 208.80473\n",
      "Epoch : 579  loss = 68.3362  train MSE = 69.85229  test MSE = 207.39505\n",
      "Epoch : 580  loss = 173.7471  train MSE = 148.81966  test MSE = 208.54689\n",
      "Epoch : 581  loss = 36.993996  train MSE = 24.094534  test MSE = 209.90155\n",
      "Epoch : 582  loss = 98.932785  train MSE = 63.291424  test MSE = 212.02702\n",
      "Epoch : 583  loss = 26.878775  train MSE = 24.973108  test MSE = 215.1682\n",
      "Epoch : 584  loss = 18.52092  train MSE = 9.474958  test MSE = 220.01062\n",
      "Epoch : 585  loss = 137.87888  train MSE = 54.753265  test MSE = 223.0881\n",
      "Epoch : 586  loss = 32.620815  train MSE = 23.333416  test MSE = 227.25311\n",
      "Epoch : 587  loss = 219.02902  train MSE = 60.78077  test MSE = 227.19772\n",
      "Epoch : 588  loss = 39.902935  train MSE = 37.889965  test MSE = 227.44145\n",
      "Epoch : 589  loss = 60.771004  train MSE = 126.667496  test MSE = 225.91226\n",
      "Epoch : 590  loss = 57.913055  train MSE = 58.408813  test MSE = 225.90067\n",
      "Epoch : 591  loss = 64.86177  train MSE = 70.393364  test MSE = 228.12376\n",
      "Epoch : 592  loss = 214.55313  train MSE = 261.34027  test MSE = 230.17256\n",
      "Epoch : 593  loss = 52.0165  train MSE = 33.542347  test MSE = 232.91574\n",
      "Epoch : 594  loss = 45.56282  train MSE = 39.40508  test MSE = 233.8273\n",
      "Epoch : 595  loss = 314.55182  train MSE = 158.93456  test MSE = 233.77711\n",
      "Epoch : 596  loss = 23.57462  train MSE = 19.679361  test MSE = 233.94589\n",
      "Epoch : 597  loss = 46.586693  train MSE = 43.186806  test MSE = 232.25551\n",
      "Epoch : 598  loss = 211.57846  train MSE = 164.99768  test MSE = 231.66916\n",
      "Epoch : 599  loss = 148.31946  train MSE = 78.25535  test MSE = 233.10732\n",
      "Epoch : 600  loss = 58.10401  train MSE = 41.553505  test MSE = 232.85472\n",
      "Epoch : 601  loss = 63.874165  train MSE = 66.47265  test MSE = 233.5824\n",
      "Epoch : 602  loss = 107.37907  train MSE = 109.03176  test MSE = 234.87804\n",
      "Epoch : 603  loss = 50.69822  train MSE = 42.885933  test MSE = 237.3796\n",
      "Epoch : 604  loss = 31.701283  train MSE = 33.687004  test MSE = 239.60565\n",
      "Epoch : 605  loss = 85.122116  train MSE = 43.35546  test MSE = 242.16881\n",
      "Epoch : 606  loss = 62.218895  train MSE = 40.12685  test MSE = 248.41957\n",
      "Epoch : 607  loss = 64.11351  train MSE = 44.773407  test MSE = 254.44699\n",
      "Epoch : 608  loss = 23.164942  train MSE = 12.5501  test MSE = 259.88004\n",
      "Epoch : 609  loss = 36.45893  train MSE = 21.70137  test MSE = 265.93164\n",
      "Epoch : 610  loss = 108.286415  train MSE = 73.837746  test MSE = 271.53223\n",
      "Epoch : 611  loss = 521.83777  train MSE = 145.84244  test MSE = 268.41946\n",
      "Epoch : 612  loss = 125.61589  train MSE = 34.588505  test MSE = 260.49225\n",
      "Epoch : 613  loss = 23.22961  train MSE = 17.968699  test MSE = 252.29817\n",
      "Epoch : 614  loss = 61.182384  train MSE = 44.919395  test MSE = 245.97894\n",
      "Epoch : 615  loss = 48.90228  train MSE = 35.196495  test MSE = 238.09851\n",
      "Epoch : 616  loss = 69.53166  train MSE = 35.671654  test MSE = 231.17043\n",
      "Epoch : 617  loss = 38.598717  train MSE = 31.470379  test MSE = 224.1219\n",
      "Epoch : 618  loss = 131.59995  train MSE = 127.19678  test MSE = 218.43677\n",
      "Epoch : 619  loss = 26.74494  train MSE = 29.497137  test MSE = 212.41762\n",
      "Epoch : 620  loss = 18.124395  train MSE = 21.248615  test MSE = 207.26675\n",
      "Epoch : 621  loss = 101.84791  train MSE = 112.83585  test MSE = 204.34055\n",
      "Epoch : 622  loss = 14.132914  train MSE = 13.981325  test MSE = 200.94225\n",
      "Epoch : 623  loss = 34.133606  train MSE = 49.184547  test MSE = 197.7039\n",
      "Epoch : 624  loss = 22.764269  train MSE = 18.974146  test MSE = 194.7706\n",
      "Epoch : 625  loss = 28.823236  train MSE = 30.333927  test MSE = 194.16785\n",
      "Epoch : 626  loss = 215.57246  train MSE = 158.20552  test MSE = 194.4279\n",
      "Epoch : 627  loss = 81.34811  train MSE = 87.1327  test MSE = 194.9518\n",
      "Epoch : 628  loss = 53.969704  train MSE = 36.54523  test MSE = 196.39917\n",
      "Epoch : 629  loss = 64.22511  train MSE = 47.773926  test MSE = 198.63109\n",
      "Epoch : 630  loss = 53.147217  train MSE = 37.61806  test MSE = 201.35434\n",
      "Epoch : 631  loss = 32.72417  train MSE = 33.147366  test MSE = 203.3602\n",
      "Epoch : 632  loss = 77.40288  train MSE = 22.855564  test MSE = 205.76974\n",
      "Epoch : 633  loss = 39.14068  train MSE = 46.96887  test MSE = 208.73625\n",
      "Epoch : 634  loss = 119.16714  train MSE = 68.99182  test MSE = 211.70573\n",
      "Epoch : 635  loss = 39.68147  train MSE = 48.314533  test MSE = 215.24962\n",
      "Epoch : 636  loss = 107.39905  train MSE = 75.020424  test MSE = 218.67041\n",
      "Epoch : 637  loss = 31.702564  train MSE = 18.741049  test MSE = 222.524\n",
      "Epoch : 638  loss = 158.05977  train MSE = 115.933395  test MSE = 219.9933\n",
      "Epoch : 639  loss = 340.26675  train MSE = 128.88512  test MSE = 221.00842\n",
      "Epoch : 640  loss = 72.20201  train MSE = 60.852394  test MSE = 221.54106\n",
      "Epoch : 641  loss = 145.50322  train MSE = 63.136375  test MSE = 212.72606\n",
      "Epoch : 642  loss = 44.20976  train MSE = 31.911266  test MSE = 203.78218\n",
      "Epoch : 643  loss = 23.516243  train MSE = 23.876905  test MSE = 195.49808\n",
      "Epoch : 644  loss = 46.452774  train MSE = 52.451653  test MSE = 188.61691\n",
      "Epoch : 645  loss = 88.60326  train MSE = 59.26554  test MSE = 183.00009\n",
      "Epoch : 646  loss = 34.028755  train MSE = 42.029217  test MSE = 177.7105\n",
      "Epoch : 647  loss = 58.327797  train MSE = 56.74774  test MSE = 173.43086\n",
      "Epoch : 648  loss = 54.42566  train MSE = 54.79415  test MSE = 169.80637\n",
      "Epoch : 649  loss = 410.50128  train MSE = 418.47614  test MSE = 168.8787\n",
      "Epoch : 650  loss = 173.31924  train MSE = 145.4851  test MSE = 169.09175\n",
      "Epoch : 651  loss = 29.284866  train MSE = 33.117603  test MSE = 169.44388\n",
      "Epoch : 652  loss = 35.40459  train MSE = 44.5494  test MSE = 169.76122\n",
      "Epoch : 653  loss = 257.51505  train MSE = 176.75575  test MSE = 170.75453\n",
      "Epoch : 654  loss = 57.18537  train MSE = 51.854115  test MSE = 171.46005\n",
      "Epoch : 655  loss = 69.20707  train MSE = 68.193954  test MSE = 172.7006\n",
      "Epoch : 656  loss = 77.843895  train MSE = 80.483284  test MSE = 174.16315\n",
      "Epoch : 657  loss = 280.92163  train MSE = 226.44373  test MSE = 175.7092\n",
      "Epoch : 658  loss = 41.009834  train MSE = 40.492516  test MSE = 177.18512\n",
      "Epoch : 659  loss = 149.31766  train MSE = 139.9656  test MSE = 179.43481\n",
      "Epoch : 660  loss = 28.158962  train MSE = 29.085958  test MSE = 181.57553\n",
      "Epoch : 661  loss = 80.09297  train MSE = 97.777794  test MSE = 185.51945\n",
      "Epoch : 662  loss = 53.73697  train MSE = 58.712036  test MSE = 189.93228\n",
      "Epoch : 663  loss = 44.767994  train MSE = 32.330704  test MSE = 195.63058\n",
      "Epoch : 664  loss = 52.772743  train MSE = 48.50047  test MSE = 200.87614\n",
      "Epoch : 665  loss = 48.844303  train MSE = 53.49346  test MSE = 206.65254\n",
      "Epoch : 666  loss = 262.3665  train MSE = 263.814  test MSE = 218.06616\n",
      "Epoch : 667  loss = 206.15036  train MSE = 133.90422  test MSE = 232.6676\n",
      "Epoch : 668  loss = 32.737335  train MSE = 28.323923  test MSE = 248.5719\n",
      "Epoch : 669  loss = 50.601074  train MSE = 46.306465  test MSE = 263.2617\n",
      "Epoch : 670  loss = 102.88176  train MSE = 41.686745  test MSE = 276.9274\n",
      "Epoch : 671  loss = 31.178905  train MSE = 29.84859  test MSE = 290.0546\n",
      "Epoch : 672  loss = 70.3065  train MSE = 67.46732  test MSE = 300.87912\n",
      "Epoch : 673  loss = 181.98103  train MSE = 109.89888  test MSE = 302.88525\n",
      "Epoch : 674  loss = 94.896835  train MSE = 87.337975  test MSE = 304.58755\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 675  loss = 120.18845  train MSE = 33.517742  test MSE = 301.66498\n",
      "Epoch : 676  loss = 41.41604  train MSE = 35.437824  test MSE = 298.64328\n",
      "Epoch : 677  loss = 39.781075  train MSE = 29.026295  test MSE = 292.7268\n",
      "Epoch : 678  loss = 241.44278  train MSE = 86.2291  test MSE = 291.10672\n",
      "Epoch : 679  loss = 39.078423  train MSE = 18.960678  test MSE = 285.04514\n",
      "Epoch : 680  loss = 72.15534  train MSE = 41.780243  test MSE = 277.46634\n",
      "Epoch : 681  loss = 34.316185  train MSE = 29.884775  test MSE = 267.38986\n",
      "Epoch : 682  loss = 39.293858  train MSE = 39.718994  test MSE = 257.8611\n",
      "Epoch : 683  loss = 31.531204  train MSE = 28.78864  test MSE = 249.94075\n",
      "Epoch : 684  loss = 103.57884  train MSE = 100.4879  test MSE = 245.3889\n",
      "Epoch : 685  loss = 31.405874  train MSE = 16.337246  test MSE = 243.13528\n",
      "Epoch : 686  loss = 148.27687  train MSE = 200.65671  test MSE = 235.89197\n",
      "Epoch : 687  loss = 59.76986  train MSE = 57.00052  test MSE = 230.20212\n",
      "Epoch : 688  loss = 85.17934  train MSE = 62.348766  test MSE = 226.72386\n",
      "Epoch : 689  loss = 60.819664  train MSE = 28.474247  test MSE = 225.18808\n",
      "Epoch : 690  loss = 23.818186  train MSE = 20.734526  test MSE = 224.44934\n",
      "Epoch : 691  loss = 15.776263  train MSE = 15.054731  test MSE = 223.83133\n",
      "Epoch : 692  loss = 65.41765  train MSE = 57.53791  test MSE = 223.17848\n",
      "Epoch : 693  loss = 108.81036  train MSE = 62.271065  test MSE = 222.80545\n",
      "Epoch : 694  loss = 39.704094  train MSE = 18.714111  test MSE = 221.63367\n",
      "Epoch : 695  loss = 50.172478  train MSE = 50.652367  test MSE = 220.85641\n",
      "Epoch : 696  loss = 20.488264  train MSE = 13.496572  test MSE = 220.05841\n",
      "Epoch : 697  loss = 44.49211  train MSE = 32.043816  test MSE = 219.06662\n",
      "Epoch : 698  loss = 24.145254  train MSE = 23.231447  test MSE = 217.517\n",
      "Epoch : 699  loss = 28.209143  train MSE = 22.256382  test MSE = 216.23161\n",
      "Epoch : 700  loss = 40.93444  train MSE = 62.008217  test MSE = 217.38419\n",
      "Epoch : 701  loss = 83.26819  train MSE = 70.78597  test MSE = 214.91037\n",
      "Epoch : 702  loss = 36.675396  train MSE = 38.10432  test MSE = 213.21674\n",
      "Epoch : 703  loss = 35.64673  train MSE = 31.9732  test MSE = 212.15434\n",
      "Epoch : 704  loss = 19.485893  train MSE = 16.762894  test MSE = 211.16087\n",
      "Epoch : 705  loss = 50.45111  train MSE = 22.994259  test MSE = 211.98383\n",
      "Epoch : 706  loss = 42.831055  train MSE = 46.653942  test MSE = 213.63399\n",
      "Epoch : 707  loss = 28.198034  train MSE = 14.90701  test MSE = 215.54733\n",
      "Epoch : 708  loss = 42.667397  train MSE = 20.814447  test MSE = 216.95338\n",
      "Epoch : 709  loss = 63.24068  train MSE = 88.68141  test MSE = 219.80429\n",
      "Epoch : 710  loss = 14.414462  train MSE = 13.32061  test MSE = 222.91124\n",
      "Epoch : 711  loss = 51.715218  train MSE = 86.19122  test MSE = 226.5335\n",
      "Epoch : 712  loss = 39.727394  train MSE = 34.67276  test MSE = 229.55515\n",
      "Epoch : 713  loss = 24.682705  train MSE = 11.900964  test MSE = 232.3147\n",
      "Epoch : 714  loss = 24.58579  train MSE = 20.618092  test MSE = 234.52277\n",
      "Epoch : 715  loss = 58.854362  train MSE = 69.9102  test MSE = 237.99156\n",
      "Epoch : 716  loss = 117.1556  train MSE = 119.82223  test MSE = 241.94948\n",
      "Epoch : 717  loss = 43.116463  train MSE = 22.545307  test MSE = 245.83809\n",
      "Epoch : 718  loss = 50.993652  train MSE = 24.239529  test MSE = 247.82144\n",
      "Epoch : 719  loss = 68.475624  train MSE = 52.390217  test MSE = 249.87946\n",
      "Epoch : 720  loss = 39.379642  train MSE = 25.31591  test MSE = 251.2273\n",
      "Epoch : 721  loss = 156.01364  train MSE = 266.14487  test MSE = 256.39172\n",
      "Epoch : 722  loss = 66.47733  train MSE = 44.949173  test MSE = 260.80685\n",
      "Epoch : 723  loss = 87.83654  train MSE = 54.498024  test MSE = 262.82977\n",
      "Epoch : 724  loss = 20.17178  train MSE = 13.33327  test MSE = 263.20718\n",
      "Epoch : 725  loss = 76.85666  train MSE = 55.883198  test MSE = 262.02304\n",
      "Epoch : 726  loss = 65.84239  train MSE = 52.326378  test MSE = 261.47305\n",
      "Epoch : 727  loss = 21.610512  train MSE = 20.301252  test MSE = 259.73672\n",
      "Epoch : 728  loss = 128.99168  train MSE = 61.89563  test MSE = 257.14838\n",
      "Epoch : 729  loss = 94.0662  train MSE = 96.13283  test MSE = 254.67302\n",
      "Epoch : 730  loss = 38.36917  train MSE = 41.435833  test MSE = 253.03711\n",
      "Epoch : 731  loss = 41.09791  train MSE = 27.680033  test MSE = 252.46014\n",
      "Epoch : 732  loss = 161.52853  train MSE = 119.93718  test MSE = 247.29878\n",
      "Epoch : 733  loss = 26.689625  train MSE = 20.25142  test MSE = 242.97488\n",
      "Epoch : 734  loss = 51.70984  train MSE = 7.1658106  test MSE = 239.25366\n",
      "Epoch : 735  loss = 31.827076  train MSE = 29.050095  test MSE = 235.41364\n",
      "Epoch : 736  loss = 113.33302  train MSE = 81.47529  test MSE = 233.24826\n",
      "Epoch : 737  loss = 42.974873  train MSE = 36.298786  test MSE = 232.07867\n",
      "Epoch : 738  loss = 194.0676  train MSE = 96.0001  test MSE = 232.56923\n",
      "Epoch : 739  loss = 150.36069  train MSE = 99.652405  test MSE = 235.84332\n",
      "Epoch : 740  loss = 183.02042  train MSE = 47.97851  test MSE = 240.20627\n",
      "Epoch : 741  loss = 27.807077  train MSE = 19.598167  test MSE = 242.90182\n",
      "Epoch : 742  loss = 29.27354  train MSE = 16.02649  test MSE = 246.05666\n",
      "Epoch : 743  loss = 38.59675  train MSE = 26.992468  test MSE = 249.70834\n",
      "Epoch : 744  loss = 105.33905  train MSE = 103.62523  test MSE = 252.43909\n",
      "Epoch : 745  loss = 71.92861  train MSE = 67.752716  test MSE = 248.8825\n",
      "Epoch : 746  loss = 117.79453  train MSE = 317.14633  test MSE = 232.72528\n",
      "Epoch : 747  loss = 39.41543  train MSE = 141.65701  test MSE = 221.12105\n",
      "Epoch : 748  loss = 16.0655  train MSE = 17.67915  test MSE = 211.11946\n",
      "Epoch : 749  loss = 180.22295  train MSE = 66.201096  test MSE = 202.50502\n",
      "Epoch : 750  loss = 32.878326  train MSE = 27.573559  test MSE = 194.8495\n",
      "Epoch : 751  loss = 116.280685  train MSE = 97.7083  test MSE = 187.7704\n",
      "Epoch : 752  loss = 70.07562  train MSE = 75.27656  test MSE = 182.02054\n",
      "Epoch : 753  loss = 17.586828  train MSE = 16.08758  test MSE = 176.7168\n",
      "Epoch : 754  loss = 373.10608  train MSE = 372.18622  test MSE = 174.95233\n",
      "Epoch : 755  loss = 72.52916  train MSE = 52.11148  test MSE = 174.69711\n",
      "Epoch : 756  loss = 165.71056  train MSE = 187.40556  test MSE = 175.2733\n",
      "Epoch : 757  loss = 30.418545  train MSE = 26.817379  test MSE = 176.34055\n",
      "Epoch : 758  loss = 56.798363  train MSE = 69.33956  test MSE = 178.80385\n",
      "Epoch : 759  loss = 83.79419  train MSE = 92.71608  test MSE = 181.87059\n",
      "Epoch : 760  loss = 40.865112  train MSE = 60.089012  test MSE = 185.50262\n",
      "Epoch : 761  loss = 41.354603  train MSE = 55.244427  test MSE = 189.58066\n",
      "Epoch : 762  loss = 57.834137  train MSE = 29.394897  test MSE = 194.81953\n",
      "Epoch : 763  loss = 41.000572  train MSE = 18.616089  test MSE = 200.2876\n",
      "Epoch : 764  loss = 63.072807  train MSE = 51.12082  test MSE = 205.8566\n",
      "Epoch : 765  loss = 49.787224  train MSE = 33.72992  test MSE = 211.63364\n",
      "Epoch : 766  loss = 49.84253  train MSE = 59.61139  test MSE = 217.04483\n",
      "Epoch : 767  loss = 54.42912  train MSE = 38.659462  test MSE = 221.81073\n",
      "Epoch : 768  loss = 40.326942  train MSE = 36.036896  test MSE = 231.43312\n",
      "Epoch : 769  loss = 80.82529  train MSE = 76.58291  test MSE = 238.89502\n",
      "Epoch : 770  loss = 173.50003  train MSE = 73.74958  test MSE = 246.34283\n",
      "Epoch : 771  loss = 51.272675  train MSE = 43.096184  test MSE = 253.51904\n",
      "Epoch : 772  loss = 86.909  train MSE = 54.654133  test MSE = 261.57733\n",
      "Epoch : 773  loss = 38.9383  train MSE = 23.207958  test MSE = 269.77158\n",
      "Epoch : 774  loss = 43.700024  train MSE = 36.90982  test MSE = 278.74124\n",
      "Epoch : 775  loss = 45.510548  train MSE = 25.71767  test MSE = 286.47192\n",
      "Epoch : 776  loss = 74.03034  train MSE = 60.82831  test MSE = 292.54752\n",
      "Epoch : 777  loss = 39.296505  train MSE = 25.201067  test MSE = 296.04858\n",
      "Epoch : 778  loss = 130.29405  train MSE = 48.634193  test MSE = 300.41144\n",
      "Epoch : 779  loss = 24.65694  train MSE = 69.38799  test MSE = 303.24033\n",
      "Epoch : 780  loss = 18.60114  train MSE = 14.830096  test MSE = 303.82852\n",
      "Epoch : 781  loss = 96.27128  train MSE = 76.945724  test MSE = 302.10367\n",
      "Epoch : 782  loss = 69.48722  train MSE = 48.97533  test MSE = 293.49045\n",
      "Epoch : 783  loss = 20.966087  train MSE = 15.368591  test MSE = 281.9413\n",
      "Epoch : 784  loss = 19.073515  train MSE = 19.611753  test MSE = 268.67947\n",
      "Epoch : 785  loss = 33.465202  train MSE = 17.198278  test MSE = 256.34692\n",
      "Epoch : 786  loss = 79.97666  train MSE = 66.997055  test MSE = 243.80397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 787  loss = 260.05722  train MSE = 115.299904  test MSE = 233.61707\n",
      "Epoch : 788  loss = 35.920925  train MSE = 30.888622  test MSE = 223.58777\n",
      "Epoch : 789  loss = 10.055255  train MSE = 12.652806  test MSE = 214.20741\n",
      "Epoch : 790  loss = 81.25795  train MSE = 95.60487  test MSE = 208.3074\n",
      "Epoch : 791  loss = 53.035778  train MSE = 46.79064  test MSE = 204.12181\n",
      "Epoch : 792  loss = 23.455122  train MSE = 20.825537  test MSE = 201.62637\n",
      "Epoch : 793  loss = 59.127953  train MSE = 58.292408  test MSE = 198.8966\n",
      "Epoch : 794  loss = 27.684338  train MSE = 38.183826  test MSE = 196.47269\n",
      "Epoch : 795  loss = 30.78387  train MSE = 21.064983  test MSE = 195.15268\n",
      "Epoch : 796  loss = 40.607143  train MSE = 26.67506  test MSE = 193.42912\n",
      "Epoch : 797  loss = 313.21765  train MSE = 96.30556  test MSE = 194.72298\n",
      "Epoch : 798  loss = 84.09983  train MSE = 68.41436  test MSE = 197.00493\n",
      "Epoch : 799  loss = 76.69449  train MSE = 25.77612  test MSE = 199.96364\n",
      "Epoch : 800  loss = 46.665512  train MSE = 24.395113  test MSE = 203.07872\n",
      "Epoch : 801  loss = 41.145977  train MSE = 40.62753  test MSE = 205.58253\n",
      "Epoch : 802  loss = 253.84236  train MSE = 69.03391  test MSE = 207.83078\n",
      "Epoch : 803  loss = 37.26394  train MSE = 25.24697  test MSE = 209.64603\n",
      "Epoch : 804  loss = 42.180817  train MSE = 27.144802  test MSE = 212.32025\n",
      "Epoch : 805  loss = 17.507547  train MSE = 19.614666  test MSE = 214.78499\n",
      "Epoch : 806  loss = 28.91547  train MSE = 22.085772  test MSE = 217.08939\n",
      "Epoch : 807  loss = 14.986351  train MSE = 13.519312  test MSE = 219.04787\n",
      "Epoch : 808  loss = 22.331814  train MSE = 25.32045  test MSE = 220.57979\n",
      "Epoch : 809  loss = 22.79653  train MSE = 20.66923  test MSE = 220.53195\n",
      "Epoch : 810  loss = 362.706  train MSE = 169.83127  test MSE = 216.46173\n",
      "Epoch : 811  loss = 102.68813  train MSE = 68.53086  test MSE = 213.67262\n",
      "Epoch : 812  loss = 40.306366  train MSE = 17.686155  test MSE = 210.67508\n",
      "Epoch : 813  loss = 97.37931  train MSE = 53.06731  test MSE = 207.44417\n",
      "Epoch : 814  loss = 61.7364  train MSE = 64.57525  test MSE = 204.79839\n",
      "Epoch : 815  loss = 114.31435  train MSE = 25.082775  test MSE = 202.68916\n",
      "Epoch : 816  loss = 53.560314  train MSE = 43.327374  test MSE = 200.78984\n",
      "Epoch : 817  loss = 52.655647  train MSE = 73.54881  test MSE = 199.74367\n",
      "Epoch : 818  loss = 43.024956  train MSE = 41.24729  test MSE = 199.4093\n",
      "Epoch : 819  loss = 19.953949  train MSE = 14.425656  test MSE = 198.91673\n",
      "Epoch : 820  loss = 28.994074  train MSE = 28.370426  test MSE = 198.54967\n",
      "Epoch : 821  loss = 21.343548  train MSE = 14.236837  test MSE = 198.77774\n",
      "Epoch : 822  loss = 36.114243  train MSE = 32.474113  test MSE = 198.5818\n",
      "Epoch : 823  loss = 35.73104  train MSE = 28.096918  test MSE = 198.32549\n",
      "Epoch : 824  loss = 73.133484  train MSE = 60.12854  test MSE = 198.42596\n",
      "Epoch : 825  loss = 22.524113  train MSE = 25.060947  test MSE = 198.5027\n",
      "Epoch : 826  loss = 58.79126  train MSE = 73.73192  test MSE = 198.78365\n",
      "Epoch : 827  loss = 59.212536  train MSE = 28.88638  test MSE = 197.91272\n",
      "Epoch : 828  loss = 46.659103  train MSE = 29.304089  test MSE = 198.06725\n",
      "Epoch : 829  loss = 50.274826  train MSE = 38.71858  test MSE = 199.305\n",
      "Epoch : 830  loss = 68.18738  train MSE = 42.313118  test MSE = 201.27669\n",
      "Epoch : 831  loss = 35.519314  train MSE = 21.802708  test MSE = 203.66077\n",
      "Epoch : 832  loss = 24.720133  train MSE = 13.803415  test MSE = 205.6134\n",
      "Epoch : 833  loss = 67.2829  train MSE = 89.88174  test MSE = 208.533\n",
      "Epoch : 834  loss = 68.74891  train MSE = 23.158865  test MSE = 209.90585\n",
      "Epoch : 835  loss = 108.695946  train MSE = 31.731445  test MSE = 210.6819\n",
      "Epoch : 836  loss = 81.684944  train MSE = 76.0453  test MSE = 212.47772\n",
      "Epoch : 837  loss = 21.893694  train MSE = 16.56496  test MSE = 213.87813\n",
      "Epoch : 838  loss = 39.49496  train MSE = 35.390537  test MSE = 214.99525\n",
      "Epoch : 839  loss = 33.66874  train MSE = 8.142814  test MSE = 217.47913\n",
      "Epoch : 840  loss = 36.857117  train MSE = 23.15228  test MSE = 220.92839\n",
      "Epoch : 841  loss = 30.095947  train MSE = 37.788074  test MSE = 225.14935\n",
      "Epoch : 842  loss = 58.832886  train MSE = 16.29413  test MSE = 229.2034\n",
      "Epoch : 843  loss = 32.695305  train MSE = 10.083706  test MSE = 232.5081\n",
      "Epoch : 844  loss = 28.680775  train MSE = 21.981663  test MSE = 233.38596\n",
      "Epoch : 845  loss = 57.847443  train MSE = 36.76818  test MSE = 231.16396\n",
      "Epoch : 846  loss = 36.692486  train MSE = 23.766773  test MSE = 227.42484\n",
      "Epoch : 847  loss = 46.900764  train MSE = 52.492115  test MSE = 223.27252\n",
      "Epoch : 848  loss = 83.97681  train MSE = 74.5576  test MSE = 219.3732\n",
      "Epoch : 849  loss = 39.762  train MSE = 28.946005  test MSE = 216.26926\n",
      "Epoch : 850  loss = 102.934654  train MSE = 33.275814  test MSE = 212.67488\n",
      "Epoch : 851  loss = 27.55106  train MSE = 14.829351  test MSE = 209.60582\n",
      "Epoch : 852  loss = 26.587078  train MSE = 27.151184  test MSE = 206.94402\n",
      "Epoch : 853  loss = 27.40483  train MSE = 26.07426  test MSE = 203.95166\n",
      "Epoch : 854  loss = 106.488266  train MSE = 26.599163  test MSE = 201.4776\n",
      "Epoch : 855  loss = 16.217108  train MSE = 16.270151  test MSE = 199.96005\n",
      "Epoch : 856  loss = 30.316189  train MSE = 26.266336  test MSE = 198.47957\n",
      "Epoch : 857  loss = 20.678144  train MSE = 21.6833  test MSE = 196.43576\n",
      "Epoch : 858  loss = 33.82995  train MSE = 24.224411  test MSE = 195.04816\n",
      "Epoch : 859  loss = 20.607372  train MSE = 15.431735  test MSE = 193.48509\n",
      "Epoch : 860  loss = 53.497997  train MSE = 46.59138  test MSE = 192.57184\n",
      "Epoch : 861  loss = 59.72875  train MSE = 41.716614  test MSE = 192.51248\n",
      "Epoch : 862  loss = 34.05854  train MSE = 23.844105  test MSE = 193.04938\n",
      "Epoch : 863  loss = 151.99686  train MSE = 117.44381  test MSE = 196.12369\n",
      "Epoch : 864  loss = 35.842155  train MSE = 26.312983  test MSE = 199.43704\n",
      "Epoch : 865  loss = 51.32702  train MSE = 28.429377  test MSE = 203.87622\n",
      "Epoch : 866  loss = 65.310356  train MSE = 32.387383  test MSE = 209.86986\n",
      "Epoch : 867  loss = 110.4323  train MSE = 103.9789  test MSE = 215.52737\n",
      "Epoch : 868  loss = 59.054028  train MSE = 52.041912  test MSE = 221.92143\n",
      "Epoch : 869  loss = 22.178669  train MSE = 13.311656  test MSE = 227.99872\n",
      "Epoch : 870  loss = 29.770184  train MSE = 25.69226  test MSE = 234.78458\n",
      "Epoch : 871  loss = 99.58779  train MSE = 93.54998  test MSE = 241.53072\n",
      "Epoch : 872  loss = 20.39437  train MSE = 22.67715  test MSE = 247.21785\n",
      "Epoch : 873  loss = 47.46093  train MSE = 25.68578  test MSE = 251.16747\n",
      "Epoch : 874  loss = 35.89599  train MSE = 27.776958  test MSE = 254.372\n",
      "Epoch : 875  loss = 22.851305  train MSE = 12.145333  test MSE = 254.42406\n",
      "Epoch : 876  loss = 125.21958  train MSE = 57.1385  test MSE = 250.15273\n",
      "Epoch : 877  loss = 14.813527  train MSE = 11.104216  test MSE = 247.84607\n",
      "Epoch : 878  loss = 36.183556  train MSE = 15.163585  test MSE = 246.14934\n",
      "Epoch : 879  loss = 52.017162  train MSE = 21.397144  test MSE = 244.20572\n",
      "Epoch : 880  loss = 35.56613  train MSE = 20.160284  test MSE = 242.91397\n",
      "Epoch : 881  loss = 27.639893  train MSE = 24.30156  test MSE = 240.08986\n",
      "Epoch : 882  loss = 32.998272  train MSE = 22.707203  test MSE = 236.7071\n",
      "Epoch : 883  loss = 23.74522  train MSE = 19.003376  test MSE = 233.09357\n",
      "Epoch : 884  loss = 29.066767  train MSE = 32.335743  test MSE = 228.84837\n",
      "Epoch : 885  loss = 552.44025  train MSE = 132.4294  test MSE = 220.83643\n",
      "Epoch : 886  loss = 47.22484  train MSE = 33.869205  test MSE = 214.6796\n",
      "Epoch : 887  loss = 32.43971  train MSE = 30.876637  test MSE = 208.34996\n",
      "Epoch : 888  loss = 101.06773  train MSE = 78.225876  test MSE = 202.49046\n",
      "Epoch : 889  loss = 170.77994  train MSE = 96.355865  test MSE = 194.83455\n",
      "Epoch : 890  loss = 52.985176  train MSE = 30.036982  test MSE = 188.4977\n",
      "Epoch : 891  loss = 79.743225  train MSE = 63.83123  test MSE = 183.59467\n",
      "Epoch : 892  loss = 31.86822  train MSE = 36.582203  test MSE = 178.73647\n",
      "Epoch : 893  loss = 126.48787  train MSE = 104.640724  test MSE = 175.24866\n",
      "Epoch : 894  loss = 272.71747  train MSE = 184.79318  test MSE = 173.0504\n",
      "Epoch : 895  loss = 29.862408  train MSE = 24.457645  test MSE = 171.9092\n",
      "Epoch : 896  loss = 36.84539  train MSE = 36.876827  test MSE = 171.55972\n",
      "Epoch : 897  loss = 30.65072  train MSE = 40.931946  test MSE = 171.16902\n",
      "Epoch : 898  loss = 369.3227  train MSE = 333.64395  test MSE = 176.7188\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 899  loss = 21.3251  train MSE = 17.178448  test MSE = 182.09503\n",
      "Epoch : 900  loss = 13.783087  train MSE = 10.552902  test MSE = 187.40747\n",
      "Epoch : 901  loss = 46.687  train MSE = 34.60565  test MSE = 193.16695\n",
      "Epoch : 902  loss = 24.027136  train MSE = 16.097153  test MSE = 200.86192\n",
      "Epoch : 903  loss = 19.19459  train MSE = 14.937213  test MSE = 208.59575\n",
      "Epoch : 904  loss = 58.23628  train MSE = 30.440954  test MSE = 217.03256\n",
      "Epoch : 905  loss = 20.43307  train MSE = 23.312881  test MSE = 224.74484\n",
      "Epoch : 906  loss = 24.343948  train MSE = 16.84837  test MSE = 232.552\n",
      "Epoch : 907  loss = 23.873362  train MSE = 14.375658  test MSE = 239.23935\n",
      "Epoch : 908  loss = 23.566418  train MSE = 24.444569  test MSE = 244.09329\n",
      "Epoch : 909  loss = 17.413433  train MSE = 16.884968  test MSE = 244.5197\n",
      "Epoch : 910  loss = 51.568092  train MSE = 44.524773  test MSE = 244.01381\n",
      "Epoch : 911  loss = 76.873726  train MSE = 83.302124  test MSE = 242.32784\n",
      "Epoch : 912  loss = 44.966934  train MSE = 31.301922  test MSE = 240.97661\n",
      "Epoch : 913  loss = 23.173248  train MSE = 9.764088  test MSE = 239.9043\n",
      "Epoch : 914  loss = 26.585064  train MSE = 23.603437  test MSE = 239.73204\n",
      "Epoch : 915  loss = 88.35956  train MSE = 79.85007  test MSE = 238.99205\n",
      "Epoch : 916  loss = 14.043462  train MSE = 12.466377  test MSE = 238.54143\n",
      "Epoch : 917  loss = 41.370808  train MSE = 12.155661  test MSE = 236.51779\n",
      "Epoch : 918  loss = 46.534378  train MSE = 45.31293  test MSE = 233.17874\n",
      "Epoch : 919  loss = 77.724236  train MSE = 37.494373  test MSE = 228.56544\n",
      "Epoch : 920  loss = 61.855263  train MSE = 29.528296  test MSE = 224.14821\n",
      "Epoch : 921  loss = 104.30733  train MSE = 19.979235  test MSE = 219.95132\n",
      "Epoch : 922  loss = 19.231174  train MSE = 25.509716  test MSE = 213.01086\n",
      "Epoch : 923  loss = 62.52239  train MSE = 39.828083  test MSE = 208.23839\n",
      "Epoch : 924  loss = 103.139786  train MSE = 84.24257  test MSE = 204.64864\n",
      "Epoch : 925  loss = 53.92121  train MSE = 50.480335  test MSE = 202.38904\n",
      "Epoch : 926  loss = 27.948435  train MSE = 28.940516  test MSE = 201.47755\n",
      "Epoch : 927  loss = 92.47234  train MSE = 23.738417  test MSE = 200.00143\n",
      "Epoch : 928  loss = 25.037428  train MSE = 20.229696  test MSE = 198.2961\n",
      "Epoch : 929  loss = 23.622486  train MSE = 48.104218  test MSE = 197.74455\n",
      "Epoch : 930  loss = 39.386307  train MSE = 29.584759  test MSE = 197.8118\n",
      "Epoch : 931  loss = 22.124046  train MSE = 12.725016  test MSE = 197.98248\n",
      "Epoch : 932  loss = 144.8912  train MSE = 59.777477  test MSE = 200.50554\n",
      "Epoch : 933  loss = 25.85373  train MSE = 10.314341  test MSE = 203.32942\n",
      "Epoch : 934  loss = 47.407127  train MSE = 36.042213  test MSE = 206.1016\n",
      "Epoch : 935  loss = 25.52614  train MSE = 21.52502  test MSE = 209.68951\n",
      "Epoch : 936  loss = 26.903711  train MSE = 25.219349  test MSE = 214.50983\n",
      "Epoch : 937  loss = 63.951393  train MSE = 72.187546  test MSE = 220.32387\n",
      "Epoch : 938  loss = 32.132828  train MSE = 25.897497  test MSE = 225.48602\n",
      "Epoch : 939  loss = 96.71877  train MSE = 102.42522  test MSE = 231.00023\n",
      "Epoch : 940  loss = 127.89717  train MSE = 61.477264  test MSE = 235.87029\n",
      "Epoch : 941  loss = 48.387123  train MSE = 24.946394  test MSE = 241.63821\n",
      "Epoch : 942  loss = 72.158134  train MSE = 29.92658  test MSE = 246.92825\n",
      "Epoch : 943  loss = 21.047781  train MSE = 9.3597765  test MSE = 252.02594\n",
      "Epoch : 944  loss = 30.733952  train MSE = 15.920256  test MSE = 257.2175\n",
      "Epoch : 945  loss = 18.600534  train MSE = 8.862719  test MSE = 260.1144\n",
      "Epoch : 946  loss = 79.59062  train MSE = 43.638557  test MSE = 262.58395\n",
      "Epoch : 947  loss = 23.009592  train MSE = 16.43181  test MSE = 263.6542\n",
      "Epoch : 948  loss = 61.50641  train MSE = 25.699514  test MSE = 263.99704\n",
      "Epoch : 949  loss = 46.98733  train MSE = 28.851675  test MSE = 263.93973\n",
      "Epoch : 950  loss = 59.74012  train MSE = 33.820858  test MSE = 262.38846\n",
      "Epoch : 951  loss = 64.80503  train MSE = 58.470695  test MSE = 252.61656\n",
      "Epoch : 952  loss = 22.387243  train MSE = 23.471233  test MSE = 244.29074\n",
      "Epoch : 953  loss = 27.659693  train MSE = 16.45795  test MSE = 237.01277\n",
      "Epoch : 954  loss = 12.484374  train MSE = 12.483208  test MSE = 230.65813\n",
      "Epoch : 955  loss = 20.811342  train MSE = 16.90701  test MSE = 226.12238\n",
      "Epoch : 956  loss = 346.58258  train MSE = 113.822266  test MSE = 222.90259\n",
      "Epoch : 957  loss = 86.77577  train MSE = 115.3655  test MSE = 218.20494\n",
      "Epoch : 958  loss = 53.646446  train MSE = 43.639168  test MSE = 213.62762\n",
      "Epoch : 959  loss = 32.844223  train MSE = 28.229883  test MSE = 209.31488\n",
      "Epoch : 960  loss = 34.44055  train MSE = 41.283924  test MSE = 204.00294\n",
      "Epoch : 961  loss = 13.869171  train MSE = 12.766014  test MSE = 199.36478\n",
      "Epoch : 962  loss = 155.54451  train MSE = 155.58398  test MSE = 197.33017\n",
      "Epoch : 963  loss = 82.016556  train MSE = 50.060493  test MSE = 195.788\n",
      "Epoch : 964  loss = 31.688038  train MSE = 37.986702  test MSE = 194.72762\n",
      "Epoch : 965  loss = 44.050835  train MSE = 28.264542  test MSE = 194.8215\n",
      "Epoch : 966  loss = 169.81058  train MSE = 106.42143  test MSE = 196.59763\n",
      "Epoch : 967  loss = 102.57085  train MSE = 35.547733  test MSE = 199.60504\n",
      "Epoch : 968  loss = 34.902626  train MSE = 18.425444  test MSE = 202.83676\n",
      "Epoch : 969  loss = 68.66361  train MSE = 59.636024  test MSE = 205.94606\n",
      "Epoch : 970  loss = 38.423203  train MSE = 16.443653  test MSE = 208.7998\n",
      "Epoch : 971  loss = 30.907063  train MSE = 29.663288  test MSE = 212.07991\n",
      "Epoch : 972  loss = 148.53114  train MSE = 118.72721  test MSE = 214.10994\n",
      "Epoch : 973  loss = 17.126797  train MSE = 13.352697  test MSE = 216.02742\n",
      "Epoch : 974  loss = 71.06296  train MSE = 15.7621975  test MSE = 217.35172\n",
      "Epoch : 975  loss = 15.963101  train MSE = 20.169428  test MSE = 218.4035\n",
      "Epoch : 976  loss = 197.88771  train MSE = 24.464817  test MSE = 219.62466\n",
      "Epoch : 977  loss = 13.7293625  train MSE = 5.652776  test MSE = 220.40459\n",
      "Epoch : 978  loss = 26.106945  train MSE = 20.025023  test MSE = 220.90663\n",
      "Epoch : 979  loss = 22.483288  train MSE = 29.339138  test MSE = 221.07402\n",
      "Epoch : 980  loss = 204.05972  train MSE = 21.886072  test MSE = 221.97563\n",
      "Epoch : 981  loss = 14.372345  train MSE = 24.423418  test MSE = 220.00623\n",
      "Epoch : 982  loss = 42.320538  train MSE = 47.482403  test MSE = 218.5328\n",
      "Epoch : 983  loss = 54.536915  train MSE = 54.81944  test MSE = 217.00534\n",
      "Epoch : 984  loss = 337.38486  train MSE = 119.74459  test MSE = 219.45001\n",
      "Epoch : 985  loss = 22.02407  train MSE = 10.300655  test MSE = 220.94917\n",
      "Epoch : 986  loss = 10.12226  train MSE = 7.3385353  test MSE = 221.42285\n",
      "Epoch : 987  loss = 63.06968  train MSE = 37.251698  test MSE = 220.63698\n",
      "Epoch : 988  loss = 61.951244  train MSE = 23.478376  test MSE = 219.77768\n",
      "Epoch : 989  loss = 31.87182  train MSE = 15.751566  test MSE = 219.3743\n",
      "Epoch : 990  loss = 15.53816  train MSE = 25.640331  test MSE = 219.21167\n",
      "Epoch : 991  loss = 17.138859  train MSE = 9.790476  test MSE = 219.69266\n",
      "Epoch : 992  loss = 22.501055  train MSE = 18.028152  test MSE = 220.07639\n",
      "Epoch : 993  loss = 12.812181  train MSE = 18.590912  test MSE = 220.91049\n",
      "Epoch : 994  loss = 147.11414  train MSE = 118.5944  test MSE = 221.22661\n",
      "Epoch : 995  loss = 53.326923  train MSE = 67.64172  test MSE = 221.85799\n",
      "Epoch : 996  loss = 52.44704  train MSE = 43.362133  test MSE = 223.10835\n",
      "Epoch : 997  loss = 30.577658  train MSE = 14.397342  test MSE = 223.23277\n",
      "Epoch : 998  loss = 49.745693  train MSE = 37.511086  test MSE = 222.27869\n",
      "Epoch : 999  loss = 18.519953  train MSE = 15.466388  test MSE = 221.50465\n",
      "Epoch : 1000  loss = 22.751514  train MSE = 18.165356  test MSE = 221.11377\n"
     ]
    }
   ],
   "source": [
    "predict_CNN = training(model_CNN, X_train_4D,y_train, X_test_4D, y_test, batch_size, n_epochs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=5831092, shape=(918, 1), dtype=float32, numpy=\n",
       "array([[-1.18187208e+01],\n",
       "       [-1.13601608e+01],\n",
       "       [-1.06049986e+01],\n",
       "       [-1.09132900e+01],\n",
       "       [-7.22602224e+00],\n",
       "       [-6.04375839e+00],\n",
       "       [-6.45275593e+00],\n",
       "       [-4.50961256e+00],\n",
       "       [-1.34321058e+00],\n",
       "       [-1.72845185e+00],\n",
       "       [ 2.08533955e+00],\n",
       "       [ 3.18366241e+00],\n",
       "       [-7.05968499e-01],\n",
       "       [-1.29705274e+00],\n",
       "       [-3.78390670e+00],\n",
       "       [-4.92135096e+00],\n",
       "       [-3.50647163e+00],\n",
       "       [-2.88608694e+00],\n",
       "       [-2.35974336e+00],\n",
       "       [-8.49016756e-02],\n",
       "       [ 1.71715438e+00],\n",
       "       [-2.44019374e-01],\n",
       "       [-2.29334936e-01],\n",
       "       [-1.66422307e+00],\n",
       "       [-1.82307565e+00],\n",
       "       [ 2.11926341e+00],\n",
       "       [ 4.31452990e+00],\n",
       "       [ 7.59863615e+00],\n",
       "       [ 1.40228119e+01],\n",
       "       [ 1.70889225e+01],\n",
       "       [ 1.92745533e+01],\n",
       "       [ 2.06567173e+01],\n",
       "       [ 1.39160252e+01],\n",
       "       [ 1.37912207e+01],\n",
       "       [ 1.60854836e+01],\n",
       "       [ 1.62526703e+01],\n",
       "       [ 1.90781593e+01],\n",
       "       [ 2.23248692e+01],\n",
       "       [ 1.76916142e+01],\n",
       "       [ 1.71055889e+01],\n",
       "       [ 1.33725271e+01],\n",
       "       [ 4.67615175e+00],\n",
       "       [ 4.03193712e+00],\n",
       "       [ 3.73053575e+00],\n",
       "       [ 5.17608833e+00],\n",
       "       [ 8.08657932e+00],\n",
       "       [ 1.42048845e+01],\n",
       "       [ 1.49381781e+01],\n",
       "       [ 1.69055042e+01],\n",
       "       [ 1.69654903e+01],\n",
       "       [ 1.29124422e+01],\n",
       "       [ 1.15316448e+01],\n",
       "       [ 1.15044069e+01],\n",
       "       [ 1.05258970e+01],\n",
       "       [ 1.42217903e+01],\n",
       "       [ 1.72539043e+01],\n",
       "       [ 1.34915199e+01],\n",
       "       [ 1.48425970e+01],\n",
       "       [ 1.31987514e+01],\n",
       "       [ 8.06625938e+00],\n",
       "       [ 8.60351944e+00],\n",
       "       [ 8.13498116e+00],\n",
       "       [ 7.07692575e+00],\n",
       "       [ 1.00743828e+01],\n",
       "       [ 1.51076078e+01],\n",
       "       [ 1.32430010e+01],\n",
       "       [ 1.61366577e+01],\n",
       "       [ 2.01959362e+01],\n",
       "       [ 1.51010227e+01],\n",
       "       [ 1.02879601e+01],\n",
       "       [ 1.23913469e+01],\n",
       "       [ 7.91682005e+00],\n",
       "       [ 7.64331579e+00],\n",
       "       [ 1.69556465e+01],\n",
       "       [ 1.34989834e+01],\n",
       "       [ 1.75514927e+01],\n",
       "       [ 2.52837715e+01],\n",
       "       [ 1.95073795e+01],\n",
       "       [ 1.63979015e+01],\n",
       "       [ 1.45613232e+01],\n",
       "       [ 1.32556229e+01],\n",
       "       [ 1.36718273e+01],\n",
       "       [ 2.00523148e+01],\n",
       "       [ 1.76233730e+01],\n",
       "       [ 2.54954338e+01],\n",
       "       [ 2.65859318e+01],\n",
       "       [ 1.61300259e+01],\n",
       "       [ 1.61599751e+01],\n",
       "       [ 1.47968073e+01],\n",
       "       [ 7.86812353e+00],\n",
       "       [ 9.35919285e+00],\n",
       "       [ 1.61450806e+01],\n",
       "       [ 1.05610447e+01],\n",
       "       [ 1.37618542e+01],\n",
       "       [ 1.86434860e+01],\n",
       "       [ 1.68280602e+01],\n",
       "       [ 1.71660786e+01],\n",
       "       [ 1.65175362e+01],\n",
       "       [ 1.58401403e+01],\n",
       "       [ 1.73237591e+01],\n",
       "       [ 2.35486641e+01],\n",
       "       [ 2.04306793e+01],\n",
       "       [ 1.91553097e+01],\n",
       "       [ 1.98947353e+01],\n",
       "       [ 1.68103542e+01],\n",
       "       [ 1.32387066e+01],\n",
       "       [ 1.24251709e+01],\n",
       "       [ 1.06651192e+01],\n",
       "       [ 9.63676453e+00],\n",
       "       [ 1.09953785e+01],\n",
       "       [ 8.81844616e+00],\n",
       "       [ 8.85646439e+00],\n",
       "       [ 1.15114431e+01],\n",
       "       [ 1.14127588e+01],\n",
       "       [ 1.09595490e+01],\n",
       "       [ 1.27736721e+01],\n",
       "       [ 1.18656301e+01],\n",
       "       [ 1.02597179e+01],\n",
       "       [ 9.59506702e+00],\n",
       "       [ 9.32611370e+00],\n",
       "       [ 8.52947330e+00],\n",
       "       [ 6.35101652e+00],\n",
       "       [ 4.98797369e+00],\n",
       "       [ 4.21922398e+00],\n",
       "       [ 3.10021782e+00],\n",
       "       [ 2.55440688e+00],\n",
       "       [ 9.03479695e-01],\n",
       "       [-3.92576098e-01],\n",
       "       [ 2.63165975e+00],\n",
       "       [ 2.48099715e-02],\n",
       "       [ 3.52895904e+00],\n",
       "       [ 3.59566188e+00],\n",
       "       [ 2.02469158e+00],\n",
       "       [ 1.23486650e+00],\n",
       "       [ 1.26533091e+00],\n",
       "       [-1.11474618e-01],\n",
       "       [-3.20761943e+00],\n",
       "       [-9.74079967e-01],\n",
       "       [-1.99094212e+00],\n",
       "       [-3.63084102e+00],\n",
       "       [-3.21604443e+00],\n",
       "       [-5.23786449e+00],\n",
       "       [-5.26815367e+00],\n",
       "       [-3.47669888e+00],\n",
       "       [-3.40791011e+00],\n",
       "       [-1.68601140e-01],\n",
       "       [ 3.24695206e+00],\n",
       "       [ 5.95414042e-01],\n",
       "       [ 2.50662398e+00],\n",
       "       [ 3.15060544e+00],\n",
       "       [ 8.49408060e-02],\n",
       "       [-2.25085950e+00],\n",
       "       [-1.60112274e+00],\n",
       "       [-2.47977281e+00],\n",
       "       [-5.99030256e+00],\n",
       "       [-5.94120884e+00],\n",
       "       [-9.52095413e+00],\n",
       "       [-1.34794607e+01],\n",
       "       [-1.50572510e+01],\n",
       "       [-1.59087286e+01],\n",
       "       [-1.95763512e+01],\n",
       "       [-1.73347054e+01],\n",
       "       [-1.73182125e+01],\n",
       "       [-1.98355904e+01],\n",
       "       [-1.65646973e+01],\n",
       "       [-1.47627516e+01],\n",
       "       [-1.65291748e+01],\n",
       "       [-1.60869923e+01],\n",
       "       [-1.49515133e+01],\n",
       "       [-1.81985283e+01],\n",
       "       [-2.02035923e+01],\n",
       "       [-1.83163853e+01],\n",
       "       [-1.76963863e+01],\n",
       "       [-2.01038799e+01],\n",
       "       [-1.73859959e+01],\n",
       "       [-1.84557400e+01],\n",
       "       [-2.00796375e+01],\n",
       "       [-1.82904816e+01],\n",
       "       [-1.93636665e+01],\n",
       "       [-2.05415230e+01],\n",
       "       [-1.81104126e+01],\n",
       "       [-1.75883617e+01],\n",
       "       [-1.97987576e+01],\n",
       "       [-1.93505306e+01],\n",
       "       [-1.93105927e+01],\n",
       "       [-2.00966702e+01],\n",
       "       [-2.02846851e+01],\n",
       "       [-2.23324528e+01],\n",
       "       [-2.52766705e+01],\n",
       "       [-2.57520008e+01],\n",
       "       [-2.39121265e+01],\n",
       "       [-2.64310017e+01],\n",
       "       [-2.52150364e+01],\n",
       "       [-2.52653046e+01],\n",
       "       [-2.64298611e+01],\n",
       "       [-2.36030064e+01],\n",
       "       [-2.27934628e+01],\n",
       "       [-2.24502602e+01],\n",
       "       [-2.46138458e+01],\n",
       "       [-2.18911724e+01],\n",
       "       [-2.18301601e+01],\n",
       "       [-1.69932556e+01],\n",
       "       [-1.60636883e+01],\n",
       "       [-1.69293423e+01],\n",
       "       [-1.83613091e+01],\n",
       "       [-1.94598637e+01],\n",
       "       [-2.03302536e+01],\n",
       "       [-2.08363991e+01],\n",
       "       [-1.96648026e+01],\n",
       "       [-2.12949314e+01],\n",
       "       [-1.88096275e+01],\n",
       "       [-1.70457306e+01],\n",
       "       [-1.78649616e+01],\n",
       "       [-1.63677540e+01],\n",
       "       [-1.47004442e+01],\n",
       "       [-1.28836679e+01],\n",
       "       [-1.43131828e+01],\n",
       "       [-1.29548683e+01],\n",
       "       [-1.26505222e+01],\n",
       "       [-1.25537148e+01],\n",
       "       [-1.26009207e+01],\n",
       "       [-1.26426392e+01],\n",
       "       [-1.16605339e+01],\n",
       "       [-1.47247458e+01],\n",
       "       [-1.30662699e+01],\n",
       "       [-1.42585526e+01],\n",
       "       [-1.26268692e+01],\n",
       "       [-1.26078300e+01],\n",
       "       [-1.27443228e+01],\n",
       "       [-8.97250748e+00],\n",
       "       [-8.39686871e+00],\n",
       "       [-8.70965862e+00],\n",
       "       [-1.06188707e+01],\n",
       "       [-6.82828569e+00],\n",
       "       [-6.06624126e+00],\n",
       "       [-5.07812214e+00],\n",
       "       [-1.85281789e+00],\n",
       "       [-1.31738290e-01],\n",
       "       [-1.30487120e+00],\n",
       "       [-2.36555290e+00],\n",
       "       [-2.94796491e+00],\n",
       "       [-4.93237448e+00],\n",
       "       [-5.34367418e+00],\n",
       "       [-7.04602671e+00],\n",
       "       [-9.34214497e+00],\n",
       "       [-7.99059248e+00],\n",
       "       [-1.12283707e+01],\n",
       "       [-1.28444872e+01],\n",
       "       [-1.08546371e+01],\n",
       "       [-1.28390093e+01],\n",
       "       [-1.52787857e+01],\n",
       "       [-1.23722124e+01],\n",
       "       [-1.54506197e+01],\n",
       "       [-1.61773491e+01],\n",
       "       [-1.51596022e+01],\n",
       "       [-1.69705029e+01],\n",
       "       [-1.71171246e+01],\n",
       "       [-1.50976133e+01],\n",
       "       [-1.54986267e+01],\n",
       "       [-1.66105461e+01],\n",
       "       [-1.37968979e+01],\n",
       "       [-1.59175844e+01],\n",
       "       [-1.37187719e+01],\n",
       "       [-1.44768028e+01],\n",
       "       [-1.47554264e+01],\n",
       "       [-1.26321220e+01],\n",
       "       [-7.95572042e+00],\n",
       "       [-8.83320332e+00],\n",
       "       [-7.83230591e+00],\n",
       "       [-6.92298937e+00],\n",
       "       [-1.02127838e+01],\n",
       "       [-9.82630444e+00],\n",
       "       [-8.68705368e+00],\n",
       "       [-9.97103310e+00],\n",
       "       [-8.68961620e+00],\n",
       "       [-6.57650948e+00],\n",
       "       [-6.87045527e+00],\n",
       "       [-7.03143549e+00],\n",
       "       [-7.45786047e+00],\n",
       "       [-8.38018227e+00],\n",
       "       [-7.88251543e+00],\n",
       "       [-9.84454441e+00],\n",
       "       [-9.88798428e+00],\n",
       "       [-9.85751534e+00],\n",
       "       [-9.18939877e+00],\n",
       "       [-8.92561817e+00],\n",
       "       [-1.14593506e+01],\n",
       "       [-1.07455082e+01],\n",
       "       [-1.08557634e+01],\n",
       "       [-1.23538151e+01],\n",
       "       [-1.37396536e+01],\n",
       "       [-1.13103027e+01],\n",
       "       [-1.21481276e+01],\n",
       "       [-1.27538929e+01],\n",
       "       [-1.10373611e+01],\n",
       "       [-1.18003426e+01],\n",
       "       [-1.26982813e+01],\n",
       "       [-1.32702579e+01],\n",
       "       [-1.47438707e+01],\n",
       "       [-1.57776022e+01],\n",
       "       [-1.44041739e+01],\n",
       "       [-1.47723751e+01],\n",
       "       [-1.46799498e+01],\n",
       "       [-1.23509092e+01],\n",
       "       [-1.30216990e+01],\n",
       "       [-1.23581724e+01],\n",
       "       [-1.20645723e+01],\n",
       "       [-1.26170626e+01],\n",
       "       [-1.23135862e+01],\n",
       "       [-9.83769035e+00],\n",
       "       [-9.97515774e+00],\n",
       "       [-1.20809374e+01],\n",
       "       [-8.94184399e+00],\n",
       "       [-8.06352901e+00],\n",
       "       [-9.92294502e+00],\n",
       "       [-9.68570995e+00],\n",
       "       [-1.03237057e+01],\n",
       "       [-1.04847860e+01],\n",
       "       [-8.48757172e+00],\n",
       "       [-8.02703953e+00],\n",
       "       [-9.53115273e+00],\n",
       "       [-8.81878662e+00],\n",
       "       [-7.58707762e+00],\n",
       "       [-8.96409798e+00],\n",
       "       [-8.36918354e+00],\n",
       "       [-8.37214088e+00],\n",
       "       [-8.96606827e+00],\n",
       "       [-7.04235888e+00],\n",
       "       [-5.92584467e+00],\n",
       "       [-7.33011580e+00],\n",
       "       [-5.78520346e+00],\n",
       "       [-4.91666794e+00],\n",
       "       [-5.92411566e+00],\n",
       "       [-5.06972933e+00],\n",
       "       [-6.19289494e+00],\n",
       "       [-5.75866890e+00],\n",
       "       [-2.96247387e+00],\n",
       "       [-2.86957693e+00],\n",
       "       [-5.83184862e+00],\n",
       "       [-3.21886182e+00],\n",
       "       [-4.25815964e+00],\n",
       "       [-6.95315170e+00],\n",
       "       [-4.40831470e+00],\n",
       "       [-4.70686197e+00],\n",
       "       [-5.30973148e+00],\n",
       "       [-1.75842559e+00],\n",
       "       [-1.97779572e+00],\n",
       "       [-5.88297653e+00],\n",
       "       [-2.24684858e+00],\n",
       "       [-1.17137337e+00],\n",
       "       [-3.52935243e+00],\n",
       "       [-2.92478919e-01],\n",
       "       [-1.61055291e+00],\n",
       "       [-2.15437555e+00],\n",
       "       [ 1.30585647e+00],\n",
       "       [-8.08365703e-01],\n",
       "       [-2.18149829e+00],\n",
       "       [-3.01761478e-02],\n",
       "       [-1.09574401e+00],\n",
       "       [-1.93511486e+00],\n",
       "       [-6.91985965e-01],\n",
       "       [-1.04566753e+00],\n",
       "       [-6.62009716e-01],\n",
       "       [ 1.83096182e+00],\n",
       "       [ 2.65256882e+00],\n",
       "       [ 7.09269881e-01],\n",
       "       [ 4.18161106e+00],\n",
       "       [ 3.21880245e+00],\n",
       "       [ 1.07026434e+00],\n",
       "       [ 2.59287786e+00],\n",
       "       [ 4.88041937e-01],\n",
       "       [-8.23835135e-01],\n",
       "       [ 4.18687016e-02],\n",
       "       [-6.47950709e-01],\n",
       "       [-1.75005233e+00],\n",
       "       [ 2.59073782e+00],\n",
       "       [ 9.22725797e-01],\n",
       "       [-2.34191942e+00],\n",
       "       [-1.37984765e+00],\n",
       "       [-2.11488318e+00],\n",
       "       [-1.82497740e+00],\n",
       "       [-1.42623913e+00],\n",
       "       [-2.50013781e+00],\n",
       "       [-1.25444114e+00],\n",
       "       [ 5.53594947e-01],\n",
       "       [-8.70634198e-01],\n",
       "       [-1.12446547e+00],\n",
       "       [-1.22773063e+00],\n",
       "       [-2.88606977e+00],\n",
       "       [-1.37230921e+00],\n",
       "       [-9.18972135e-01],\n",
       "       [-1.95421970e+00],\n",
       "       [ 2.07828060e-01],\n",
       "       [ 2.10405087e+00],\n",
       "       [-1.53345621e+00],\n",
       "       [-2.64065266e+00],\n",
       "       [-3.25146484e+00],\n",
       "       [-3.39824033e+00],\n",
       "       [-3.93656802e+00],\n",
       "       [-5.49754286e+00],\n",
       "       [-5.25555611e+00],\n",
       "       [-6.07903671e+00],\n",
       "       [-7.14136600e+00],\n",
       "       [-7.63221216e+00],\n",
       "       [-8.91677856e+00],\n",
       "       [-1.09904518e+01],\n",
       "       [-1.37430782e+01],\n",
       "       [-1.42806005e+01],\n",
       "       [-1.34888382e+01],\n",
       "       [-1.43901854e+01],\n",
       "       [-1.20910912e+01],\n",
       "       [-1.11320257e+01],\n",
       "       [-9.76518059e+00],\n",
       "       [-1.08721933e+01],\n",
       "       [-1.18637781e+01],\n",
       "       [-1.46443930e+01],\n",
       "       [-1.49372358e+01],\n",
       "       [-1.11634302e+01],\n",
       "       [-1.26168165e+01],\n",
       "       [-9.69285870e+00],\n",
       "       [-7.19088125e+00],\n",
       "       [-9.52242947e+00],\n",
       "       [-7.99943304e+00],\n",
       "       [-7.95528364e+00],\n",
       "       [-9.03580379e+00],\n",
       "       [-9.66016197e+00],\n",
       "       [-6.73842001e+00],\n",
       "       [-8.90810299e+00],\n",
       "       [-8.35018921e+00],\n",
       "       [-7.75522804e+00],\n",
       "       [-9.55641079e+00],\n",
       "       [-1.07830105e+01],\n",
       "       [-1.02908850e+01],\n",
       "       [-1.17061968e+01],\n",
       "       [-9.52800751e+00],\n",
       "       [-4.88823080e+00],\n",
       "       [-8.89600086e+00],\n",
       "       [-7.78010273e+00],\n",
       "       [-8.35145473e+00],\n",
       "       [-1.06011400e+01],\n",
       "       [-1.16072531e+01],\n",
       "       [-1.02201967e+01],\n",
       "       [-1.18476009e+01],\n",
       "       [-1.02714453e+01],\n",
       "       [-2.09475803e+00],\n",
       "       [-4.52798891e+00],\n",
       "       [-4.82570553e+00],\n",
       "       [-3.78163266e+00],\n",
       "       [-5.25286388e+00],\n",
       "       [-6.05604458e+00],\n",
       "       [-1.73454237e+00],\n",
       "       [-4.08791685e+00],\n",
       "       [-5.28140068e+00],\n",
       "       [ 2.78017759e+00],\n",
       "       [-7.08072543e-01],\n",
       "       [-9.83418107e-01],\n",
       "       [-9.74530935e-01],\n",
       "       [-1.71877742e+00],\n",
       "       [-2.48560762e+00],\n",
       "       [ 2.75156069e+00],\n",
       "       [ 1.01942527e+00],\n",
       "       [ 1.20317709e+00],\n",
       "       [ 9.15181351e+00],\n",
       "       [ 7.05164862e+00],\n",
       "       [ 7.13903475e+00],\n",
       "       [ 6.24671364e+00],\n",
       "       [ 6.34494400e+00],\n",
       "       [ 4.23000097e+00],\n",
       "       [ 9.57856369e+00],\n",
       "       [ 7.83880806e+00],\n",
       "       [ 5.33521032e+00],\n",
       "       [ 9.26910114e+00],\n",
       "       [ 8.56448174e+00],\n",
       "       [ 5.59599972e+00],\n",
       "       [ 2.95920014e+00],\n",
       "       [ 1.62464380e+00],\n",
       "       [ 6.83343172e-01],\n",
       "       [ 2.53411865e+00],\n",
       "       [ 5.03460765e-01],\n",
       "       [ 5.21373630e-01],\n",
       "       [ 4.74959517e+00],\n",
       "       [ 1.89715886e+00],\n",
       "       [ 2.40768686e-01],\n",
       "       [ 2.44670677e+00],\n",
       "       [ 1.96968317e+00],\n",
       "       [-1.18041706e+00],\n",
       "       [ 2.88391972e+00],\n",
       "       [ 1.20781255e+00],\n",
       "       [-1.31446397e+00],\n",
       "       [ 3.88773847e+00],\n",
       "       [ 3.31917357e+00],\n",
       "       [ 1.68120015e+00],\n",
       "       [ 1.21810353e+00],\n",
       "       [-3.39822412e-01],\n",
       "       [-2.52932143e+00],\n",
       "       [-2.27758026e+00],\n",
       "       [-3.35003042e+00],\n",
       "       [-4.55635786e+00],\n",
       "       [-4.33881378e+00],\n",
       "       [-4.55157614e+00],\n",
       "       [-4.71803808e+00],\n",
       "       [-4.09854174e+00],\n",
       "       [-2.64540696e+00],\n",
       "       [-2.42357254e+00],\n",
       "       [-1.83652544e+00],\n",
       "       [-1.17877197e+00],\n",
       "       [-2.30538225e+00],\n",
       "       [ 3.20685059e-02],\n",
       "       [ 3.12489390e-01],\n",
       "       [ 7.43548870e-01],\n",
       "       [ 2.63552237e+00],\n",
       "       [ 3.18390369e+00],\n",
       "       [ 2.16029954e+00],\n",
       "       [ 2.04032755e+00],\n",
       "       [ 2.06641865e+00],\n",
       "       [ 9.32463408e-01],\n",
       "       [ 2.31295601e-01],\n",
       "       [ 4.70498204e-01],\n",
       "       [ 2.20012522e+00],\n",
       "       [ 3.50631285e+00],\n",
       "       [ 3.95036435e+00],\n",
       "       [ 4.28850985e+00],\n",
       "       [ 5.54670858e+00],\n",
       "       [ 5.24648285e+00],\n",
       "       [ 3.21713448e+00],\n",
       "       [ 3.97136474e+00],\n",
       "       [ 5.19736624e+00],\n",
       "       [ 7.28174925e+00],\n",
       "       [ 5.90937233e+00],\n",
       "       [ 6.14349270e+00],\n",
       "       [ 6.54900408e+00],\n",
       "       [ 6.21118021e+00],\n",
       "       [ 6.83409643e+00],\n",
       "       [ 4.98826265e+00],\n",
       "       [ 4.40732241e+00],\n",
       "       [ 6.39648438e+00],\n",
       "       [ 1.04396830e+01],\n",
       "       [ 1.30865908e+01],\n",
       "       [ 1.24345522e+01],\n",
       "       [ 1.08928804e+01],\n",
       "       [ 1.00540304e+01],\n",
       "       [ 1.06674671e+01],\n",
       "       [ 5.71541023e+00],\n",
       "       [ 5.37338448e+00],\n",
       "       [ 6.54994059e+00],\n",
       "       [ 4.77431536e+00],\n",
       "       [ 3.57210255e+00],\n",
       "       [ 3.74503994e+00],\n",
       "       [ 9.79920268e-01],\n",
       "       [ 1.67428389e-01],\n",
       "       [-2.62396455e-01],\n",
       "       [-1.13527000e+00],\n",
       "       [-2.12323532e-01],\n",
       "       [-5.15514970e-01],\n",
       "       [-1.12500250e+00],\n",
       "       [-1.07448423e+00],\n",
       "       [-7.64108777e-01],\n",
       "       [-3.66793656e+00],\n",
       "       [-3.17611575e+00],\n",
       "       [-4.27465057e+00],\n",
       "       [-5.70399189e+00],\n",
       "       [-4.23537445e+00],\n",
       "       [-2.94102001e+00],\n",
       "       [-5.69919157e+00],\n",
       "       [-3.32066870e+00],\n",
       "       [-3.15996313e+00],\n",
       "       [-5.03098726e+00],\n",
       "       [-1.24096453e+00],\n",
       "       [-2.83240938e+00],\n",
       "       [-4.79863787e+00],\n",
       "       [-1.64524400e+00],\n",
       "       [-2.83764291e+00],\n",
       "       [-6.34212351e+00],\n",
       "       [ 1.11421227e+00],\n",
       "       [ 8.66948515e-02],\n",
       "       [-3.18139219e+00],\n",
       "       [-2.39189252e-01],\n",
       "       [ 1.26799369e+00],\n",
       "       [-4.67221498e-01],\n",
       "       [ 2.84912205e+00],\n",
       "       [ 4.27992010e+00],\n",
       "       [ 3.10297322e+00],\n",
       "       [ 6.50566912e+00],\n",
       "       [ 5.95579100e+00],\n",
       "       [ 1.26806772e+00],\n",
       "       [ 9.09764886e-01],\n",
       "       [-5.42642236e-01],\n",
       "       [-4.33010721e+00],\n",
       "       [-1.95573342e+00],\n",
       "       [-2.45273066e+00],\n",
       "       [-3.10647655e+00],\n",
       "       [ 2.10288420e-01],\n",
       "       [-1.87787473e+00],\n",
       "       [-1.31847656e+00],\n",
       "       [ 2.30553246e+00],\n",
       "       [ 4.01641703e+00],\n",
       "       [ 6.56558228e+00],\n",
       "       [ 1.00536757e+01],\n",
       "       [ 1.16378756e+01],\n",
       "       [ 7.84694433e+00],\n",
       "       [ 1.25127945e+01],\n",
       "       [ 1.16923923e+01],\n",
       "       [ 7.46079588e+00],\n",
       "       [ 9.89029217e+00],\n",
       "       [ 1.04210281e+01],\n",
       "       [ 5.90065289e+00],\n",
       "       [ 1.17955666e+01],\n",
       "       [ 1.27789803e+01],\n",
       "       [ 1.05504456e+01],\n",
       "       [ 1.51341467e+01],\n",
       "       [ 9.92175388e+00],\n",
       "       [ 7.29427862e+00],\n",
       "       [ 7.51098824e+00],\n",
       "       [ 1.07173910e+01],\n",
       "       [ 6.92842913e+00],\n",
       "       [ 9.70349026e+00],\n",
       "       [ 9.15439510e+00],\n",
       "       [ 4.27746105e+00],\n",
       "       [ 6.54074717e+00],\n",
       "       [ 2.54224038e+00],\n",
       "       [ 1.99087584e+00],\n",
       "       [ 1.44358313e+00],\n",
       "       [-3.33276439e+00],\n",
       "       [-3.81417704e+00],\n",
       "       [-1.41372478e+00],\n",
       "       [-3.11686325e+00],\n",
       "       [-4.59346724e+00],\n",
       "       [-5.03742099e-01],\n",
       "       [-3.71552515e+00],\n",
       "       [-2.44899511e+00],\n",
       "       [-3.87639761e+00],\n",
       "       [-6.45714521e+00],\n",
       "       [-6.44610548e+00],\n",
       "       [-4.57775259e+00],\n",
       "       [-4.56363773e+00],\n",
       "       [-4.94398832e+00],\n",
       "       [-5.00280023e-01],\n",
       "       [-2.22356105e+00],\n",
       "       [-3.24677515e+00],\n",
       "       [-4.01444864e+00],\n",
       "       [-6.58648920e+00],\n",
       "       [-9.17065620e+00],\n",
       "       [-7.96433973e+00],\n",
       "       [-1.04396276e+01],\n",
       "       [-1.24712400e+01],\n",
       "       [-9.51042843e+00],\n",
       "       [-1.19225531e+01],\n",
       "       [-1.23988400e+01],\n",
       "       [-1.26712341e+01],\n",
       "       [-1.34996405e+01],\n",
       "       [-1.47956800e+01],\n",
       "       [-1.09281445e+01],\n",
       "       [-1.62722244e+01],\n",
       "       [-1.35870104e+01],\n",
       "       [-1.09114590e+01],\n",
       "       [-1.64601021e+01],\n",
       "       [-1.73750095e+01],\n",
       "       [-1.63596210e+01],\n",
       "       [-2.04653835e+01],\n",
       "       [-2.15844116e+01],\n",
       "       [-1.96412601e+01],\n",
       "       [-2.49496441e+01],\n",
       "       [-2.37564182e+01],\n",
       "       [-2.19576931e+01],\n",
       "       [-2.39387112e+01],\n",
       "       [-2.44041252e+01],\n",
       "       [-2.47958832e+01],\n",
       "       [-2.35303726e+01],\n",
       "       [-2.54404736e+01],\n",
       "       [-2.28820896e+01],\n",
       "       [-2.50186749e+01],\n",
       "       [-2.48126335e+01],\n",
       "       [-2.36012173e+01],\n",
       "       [-2.53890095e+01],\n",
       "       [-2.48855648e+01],\n",
       "       [-2.50099049e+01],\n",
       "       [-2.30810890e+01],\n",
       "       [-2.44730511e+01],\n",
       "       [-2.53221798e+01],\n",
       "       [-2.73294582e+01],\n",
       "       [-2.55672054e+01],\n",
       "       [-2.58335514e+01],\n",
       "       [-2.49387150e+01],\n",
       "       [-2.54811916e+01],\n",
       "       [-2.70179806e+01],\n",
       "       [-2.19815674e+01],\n",
       "       [-2.38742104e+01],\n",
       "       [-2.29780617e+01],\n",
       "       [-1.92812557e+01],\n",
       "       [-2.03958740e+01],\n",
       "       [-2.21295147e+01],\n",
       "       [-2.27472591e+01],\n",
       "       [-2.21477661e+01],\n",
       "       [-2.24334621e+01],\n",
       "       [-1.71704807e+01],\n",
       "       [-1.53273726e+01],\n",
       "       [-1.52935629e+01],\n",
       "       [-1.44877062e+01],\n",
       "       [-1.61746540e+01],\n",
       "       [-1.90049896e+01],\n",
       "       [-1.98443375e+01],\n",
       "       [-1.94035740e+01],\n",
       "       [-2.10120850e+01],\n",
       "       [-1.66867046e+01],\n",
       "       [-1.42307501e+01],\n",
       "       [-1.32281857e+01],\n",
       "       [-8.72476864e+00],\n",
       "       [-1.24247618e+01],\n",
       "       [-1.43496618e+01],\n",
       "       [-1.56507273e+01],\n",
       "       [-1.68542671e+01],\n",
       "       [-1.78522568e+01],\n",
       "       [-1.37097692e+01],\n",
       "       [-1.13786192e+01],\n",
       "       [-1.11034155e+01],\n",
       "       [-8.63893414e+00],\n",
       "       [-1.15697203e+01],\n",
       "       [-1.43964653e+01],\n",
       "       [-1.44546900e+01],\n",
       "       [-1.50493431e+01],\n",
       "       [-1.61075134e+01],\n",
       "       [-1.14129343e+01],\n",
       "       [-9.60272121e+00],\n",
       "       [-9.45025635e+00],\n",
       "       [-1.70857513e+00],\n",
       "       [-5.53549767e+00],\n",
       "       [-5.91136885e+00],\n",
       "       [-7.66711950e+00],\n",
       "       [-6.52648544e+00],\n",
       "       [-6.99373055e+00],\n",
       "       [-6.44644117e+00],\n",
       "       [-4.32550859e+00],\n",
       "       [-6.38293743e+00],\n",
       "       [-6.69213676e+00],\n",
       "       [-8.61519337e+00],\n",
       "       [-1.12971287e+01],\n",
       "       [-1.32521229e+01],\n",
       "       [-9.35645103e+00],\n",
       "       [-1.13048468e+01],\n",
       "       [-1.13674097e+01],\n",
       "       [-5.36208725e+00],\n",
       "       [-5.30787802e+00],\n",
       "       [-3.94629741e+00],\n",
       "       [-4.72803164e+00],\n",
       "       [-5.76178408e+00],\n",
       "       [-6.60163164e+00],\n",
       "       [-3.57792759e+00],\n",
       "       [-6.59606218e+00],\n",
       "       [-7.54472971e+00],\n",
       "       [-5.32332087e+00],\n",
       "       [-7.99192286e+00],\n",
       "       [-9.70827770e+00],\n",
       "       [-8.06894493e+00],\n",
       "       [-1.06040106e+01],\n",
       "       [-1.52211342e+01],\n",
       "       [-1.43780909e+01],\n",
       "       [-1.75950375e+01],\n",
       "       [-2.29219017e+01],\n",
       "       [-2.04718132e+01],\n",
       "       [-2.15704136e+01],\n",
       "       [-2.22921829e+01],\n",
       "       [-1.98043022e+01],\n",
       "       [-2.40403309e+01],\n",
       "       [-2.66807041e+01],\n",
       "       [-2.33026352e+01],\n",
       "       [-2.23213482e+01],\n",
       "       [-2.11232967e+01],\n",
       "       [-1.58752947e+01],\n",
       "       [-1.59478426e+01],\n",
       "       [-1.59934187e+01],\n",
       "       [-1.34334068e+01],\n",
       "       [-1.21485186e+01],\n",
       "       [-1.41510849e+01],\n",
       "       [-1.15415249e+01],\n",
       "       [-1.29981308e+01],\n",
       "       [-1.57998314e+01],\n",
       "       [-1.37673798e+01],\n",
       "       [-1.54370813e+01],\n",
       "       [-1.51535511e+01],\n",
       "       [-1.27571592e+01],\n",
       "       [-1.44846210e+01],\n",
       "       [-1.41604595e+01],\n",
       "       [-1.13067083e+01],\n",
       "       [-1.23860092e+01],\n",
       "       [-1.58965893e+01],\n",
       "       [-1.21558466e+01],\n",
       "       [-1.36533871e+01],\n",
       "       [-1.00067587e+01],\n",
       "       [-8.49895382e+00],\n",
       "       [-6.76642036e+00],\n",
       "       [-4.91019249e+00],\n",
       "       [-5.54245949e+00],\n",
       "       [-4.89443874e+00],\n",
       "       [-6.84832144e+00],\n",
       "       [-7.27803087e+00],\n",
       "       [-7.71732664e+00],\n",
       "       [-7.13533306e+00],\n",
       "       [-7.40253830e+00],\n",
       "       [-6.74328184e+00],\n",
       "       [-3.85454607e+00],\n",
       "       [-4.44304895e+00],\n",
       "       [-5.46417856e+00],\n",
       "       [-6.15040541e+00],\n",
       "       [-8.90690708e+00],\n",
       "       [-1.07148638e+01],\n",
       "       [-6.87907457e+00],\n",
       "       [-8.19587612e+00],\n",
       "       [-8.16415977e+00],\n",
       "       [-6.00580311e+00],\n",
       "       [-5.30677509e+00],\n",
       "       [-5.61360359e+00],\n",
       "       [-8.20941257e+00],\n",
       "       [-7.06971121e+00],\n",
       "       [-7.34825182e+00],\n",
       "       [-8.07944775e+00],\n",
       "       [-8.38814163e+00],\n",
       "       [-8.02146149e+00],\n",
       "       [-6.50260687e+00],\n",
       "       [-6.45272112e+00],\n",
       "       [-7.08505821e+00],\n",
       "       [-7.62333632e+00],\n",
       "       [-7.97061872e+00],\n",
       "       [-7.61036539e+00],\n",
       "       [-4.88324451e+00],\n",
       "       [-6.76759720e+00],\n",
       "       [-7.74274635e+00],\n",
       "       [-3.28853774e+00],\n",
       "       [-6.05610800e+00],\n",
       "       [-6.36107588e+00],\n",
       "       [-4.12661076e+00],\n",
       "       [-5.12684488e+00],\n",
       "       [-5.32269478e+00],\n",
       "       [-5.35175371e+00],\n",
       "       [-9.09907150e+00],\n",
       "       [-9.07218075e+00],\n",
       "       [-6.36588717e+00],\n",
       "       [-6.87213421e+00],\n",
       "       [-8.15195751e+00],\n",
       "       [-8.33437538e+00],\n",
       "       [-7.20692968e+00],\n",
       "       [-6.24156046e+00],\n",
       "       [-8.08864307e+00],\n",
       "       [-8.53050804e+00],\n",
       "       [-7.08496189e+00],\n",
       "       [-5.19325352e+00],\n",
       "       [-6.30394602e+00],\n",
       "       [-5.81026506e+00],\n",
       "       [-4.40922165e+00],\n",
       "       [-2.57470918e+00],\n",
       "       [-1.68178439e+00],\n",
       "       [-2.30810118e+00],\n",
       "       [-4.24735022e+00],\n",
       "       [-4.00793219e+00],\n",
       "       [-2.24239635e+00],\n",
       "       [-3.36668110e+00],\n",
       "       [-4.34046698e+00],\n",
       "       [-3.69186568e+00],\n",
       "       [-2.68597078e+00],\n",
       "       [-3.18812060e+00],\n",
       "       [-6.08096313e+00],\n",
       "       [-7.87653589e+00],\n",
       "       [-6.55057955e+00],\n",
       "       [-5.58523226e+00],\n",
       "       [-4.58381319e+00],\n",
       "       [-3.08128357e+00],\n",
       "       [-2.27423906e+00],\n",
       "       [-1.17211878e+00],\n",
       "       [ 8.54380727e-01],\n",
       "       [-4.10946512e+00],\n",
       "       [-3.27057362e+00],\n",
       "       [-1.37564540e+00],\n",
       "       [-5.08844137e-01],\n",
       "       [-1.04417169e+00],\n",
       "       [ 9.75708246e-01],\n",
       "       [ 3.01412129e+00],\n",
       "       [ 3.18367434e+00],\n",
       "       [ 3.25739312e+00],\n",
       "       [ 3.85879427e-02],\n",
       "       [ 8.74016166e-01],\n",
       "       [ 1.73483264e+00],\n",
       "       [ 6.53525740e-02],\n",
       "       [ 1.00553358e+00],\n",
       "       [ 1.39792097e+00],\n",
       "       [ 2.21237674e-01],\n",
       "       [ 1.96006119e+00],\n",
       "       [ 2.80427933e+00],\n",
       "       [ 9.54881907e-01],\n",
       "       [ 3.04785299e+00],\n",
       "       [ 3.20752096e+00],\n",
       "       [ 2.89661384e+00],\n",
       "       [ 3.81863976e+00],\n",
       "       [ 3.48687744e+00],\n",
       "       [ 5.66172934e+00],\n",
       "       [ 5.58860731e+00],\n",
       "       [ 6.75326920e+00],\n",
       "       [ 5.49534750e+00],\n",
       "       [ 6.03583574e+00],\n",
       "       [ 5.31465387e+00],\n",
       "       [ 5.23454475e+00],\n",
       "       [ 5.05436659e+00],\n",
       "       [ 2.40073466e+00],\n",
       "       [ 4.40194941e+00],\n",
       "       [ 6.92194414e+00],\n",
       "       [ 4.02216673e+00],\n",
       "       [ 3.04881549e+00],\n",
       "       [ 7.17473364e+00],\n",
       "       [ 6.32967997e+00],\n",
       "       [ 6.35536337e+00],\n",
       "       [ 9.92676353e+00],\n",
       "       [ 7.42961550e+00],\n",
       "       [ 7.55017662e+00],\n",
       "       [ 1.03340778e+01],\n",
       "       [ 9.17514992e+00],\n",
       "       [ 6.99899769e+00],\n",
       "       [ 1.05032473e+01],\n",
       "       [ 8.96814251e+00],\n",
       "       [ 8.19751453e+00],\n",
       "       [ 1.02723398e+01]], dtype=float32)>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.array(y_test))\n",
    "plt.plot(np.array(predict_CNN))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-20.88353414],\n",
       "       [-20.41237113],\n",
       "       [-18.04979253],\n",
       "       [-18.65828092],\n",
       "       [-13.91304348],\n",
       "       [-12.77056277],\n",
       "       [-11.06290672],\n",
       "       [ -7.92951542],\n",
       "       [ -5.6768559 ],\n",
       "       [ -4.62555066],\n",
       "       [ -9.17030568],\n",
       "       [ -9.26724138],\n",
       "       [ -9.26724138],\n",
       "       [ -9.85010707],\n",
       "       [ -9.0712743 ],\n",
       "       [ -8.47826087],\n",
       "       [-11.85344828],\n",
       "       [ -9.44206009],\n",
       "       [ -6.88172043],\n",
       "       [ -7.08154506],\n",
       "       [ -6.91144708],\n",
       "       [ -6.17021277],\n",
       "       [ -5.2742616 ],\n",
       "       [ -3.41880342],\n",
       "       [ -2.35042735],\n",
       "       [ -2.14132762],\n",
       "       [ -2.80777538],\n",
       "       [ -2.15053763],\n",
       "       [ -0.21367521],\n",
       "       [  0.21459227],\n",
       "       [ -2.62008734],\n",
       "       [ -3.50877193],\n",
       "       [ -7.47863248],\n",
       "       [ -8.62068966],\n",
       "       [ -7.01754386],\n",
       "       [ -6.12691466],\n",
       "       [ -7.74193548],\n",
       "       [ -4.79302832],\n",
       "       [ -5.90809628],\n",
       "       [ -5.48245614],\n",
       "       [ -6.23655914],\n",
       "       [ -6.63811563],\n",
       "       [ -8.13704497],\n",
       "       [ -7.17391304],\n",
       "       [ -1.86046512],\n",
       "       [ -2.55220418],\n",
       "       [ -2.55220418],\n",
       "       [ -3.71229698],\n",
       "       [  0.        ],\n",
       "       [  0.71428571],\n",
       "       [  0.72115385],\n",
       "       [  2.20588235],\n",
       "       [  0.96153846],\n",
       "       [  1.47783251],\n",
       "       [  0.24213075],\n",
       "       [ -2.15827338],\n",
       "       [ -2.39808153],\n",
       "       [ -1.45985401],\n",
       "       [ -0.73891626],\n",
       "       [  0.49382716],\n",
       "       [  3.29949239],\n",
       "       [  9.32642487],\n",
       "       [  6.58227848],\n",
       "       [  7.73195876],\n",
       "       [  4.29292929],\n",
       "       [  2.48138958],\n",
       "       [ -0.24390244],\n",
       "       [ -3.58851675],\n",
       "       [ -6.25      ],\n",
       "       [ -4.61893764],\n",
       "       [ -2.16346154],\n",
       "       [  0.        ],\n",
       "       [ -2.13776722],\n",
       "       [ -0.23752969],\n",
       "       [  1.42517815],\n",
       "       [  0.71258907],\n",
       "       [  3.66748166],\n",
       "       [  0.47393365],\n",
       "       [ -3.00230947],\n",
       "       [ -2.5404157 ],\n",
       "       [ -1.39211137],\n",
       "       [ -3.17460317],\n",
       "       [ -5.12249443],\n",
       "       [ -6.19469027],\n",
       "       [ -8.09628009],\n",
       "       [ -7.65864333],\n",
       "       [ -6.44444444],\n",
       "       [ -7.47252747],\n",
       "       [-10.49250535],\n",
       "       [ -7.92291221],\n",
       "       [ -3.3632287 ],\n",
       "       [ -3.18181818],\n",
       "       [  0.        ],\n",
       "       [  3.77358491],\n",
       "       [  4.71698113],\n",
       "       [  1.86480186],\n",
       "       [  0.6993007 ],\n",
       "       [  0.68649886],\n",
       "       [  2.3255814 ],\n",
       "       [  2.78422274],\n",
       "       [  0.4587156 ],\n",
       "       [ -0.4587156 ],\n",
       "       [ -1.16550117],\n",
       "       [ -1.17096019],\n",
       "       [  2.36966825],\n",
       "       [  2.85714286],\n",
       "       [  0.23809524],\n",
       "       [  1.20481928],\n",
       "       [ -0.47505938],\n",
       "       [ -0.94562648],\n",
       "       [  0.23866348],\n",
       "       [  0.71942446],\n",
       "       [ -0.71428571],\n",
       "       [  2.66990291],\n",
       "       [  2.65700483],\n",
       "       [  6.37254902],\n",
       "       [  5.8968059 ],\n",
       "       [  7.90123457],\n",
       "       [  8.68486352],\n",
       "       [  6.38820639],\n",
       "       [  5.65110565],\n",
       "       [  1.65876777],\n",
       "       [  2.6128266 ],\n",
       "       [  3.8277512 ],\n",
       "       [  5.08474576],\n",
       "       [  6.53753027],\n",
       "       [  8.06845966],\n",
       "       [  9.4292804 ],\n",
       "       [ 10.12345679],\n",
       "       [  7.02179177],\n",
       "       [  7.12530713],\n",
       "       [  5.70071259],\n",
       "       [  8.49514563],\n",
       "       [  9.52380952],\n",
       "       [  8.66510539],\n",
       "       [  8.96226415],\n",
       "       [  8.25471698],\n",
       "       [  8.49056604],\n",
       "       [  7.85714286],\n",
       "       [  5.92417062],\n",
       "       [  4.        ],\n",
       "       [  3.27868852],\n",
       "       [  3.75586854],\n",
       "       [  4.71698113],\n",
       "       [  6.9047619 ],\n",
       "       [  6.16113744],\n",
       "       [  7.83847981],\n",
       "       [  7.60095012],\n",
       "       [  8.37320574],\n",
       "       [  5.58139535],\n",
       "       [  5.80046404],\n",
       "       [  6.33802817],\n",
       "       [  3.46420323],\n",
       "       [  0.45454545],\n",
       "       [ -0.9009009 ],\n",
       "       [  0.91533181],\n",
       "       [  2.08333333],\n",
       "       [  1.13636364],\n",
       "       [  0.45454545],\n",
       "       [ -0.45146727],\n",
       "       [ -0.2283105 ],\n",
       "       [ -0.23041475],\n",
       "       [  1.88679245],\n",
       "       [  4.50236967],\n",
       "       [  1.62037037],\n",
       "       [  2.31481481],\n",
       "       [  6.17577197],\n",
       "       [  5.71428571],\n",
       "       [  4.53460621],\n",
       "       [  3.81861575],\n",
       "       [  3.57142857],\n",
       "       [  3.57142857],\n",
       "       [  4.31654676],\n",
       "       [  2.60047281],\n",
       "       [  2.11764706],\n",
       "       [ -0.46082949],\n",
       "       [  1.16009281],\n",
       "       [  0.91533181],\n",
       "       [  0.91324201],\n",
       "       [  2.5404157 ],\n",
       "       [  3.48837209],\n",
       "       [  5.36130536],\n",
       "       [  4.62962963],\n",
       "       [  4.14746544],\n",
       "       [  3.4562212 ],\n",
       "       [  1.13636364],\n",
       "       [ -0.90497738],\n",
       "       [ -2.04081633],\n",
       "       [ -1.56950673],\n",
       "       [ -0.67873303],\n",
       "       [  0.4587156 ],\n",
       "       [ -1.34831461],\n",
       "       [ -1.11856823],\n",
       "       [ -1.95652174],\n",
       "       [ -1.93965517],\n",
       "       [ -0.86580087],\n",
       "       [ -2.17864924],\n",
       "       [ -2.60869565],\n",
       "       [  0.4415011 ],\n",
       "       [  3.57941834],\n",
       "       [  4.07239819],\n",
       "       [  5.44217687],\n",
       "       [  1.58371041],\n",
       "       [  0.67567568],\n",
       "       [  0.66815145],\n",
       "       [ -0.22321429],\n",
       "       [ -1.54185022],\n",
       "       [ -0.22075055],\n",
       "       [ -1.32450331],\n",
       "       [ -2.64317181],\n",
       "       [ -4.16666667],\n",
       "       [ -2.86975717],\n",
       "       [ -2.45535714],\n",
       "       [ -1.80995475],\n",
       "       [ -1.13636364],\n",
       "       [ -1.58730159],\n",
       "       [ -1.58730159],\n",
       "       [ -1.34831461],\n",
       "       [ -0.45248869],\n",
       "       [ -0.22675737],\n",
       "       [  3.43249428],\n",
       "       [  4.61893764],\n",
       "       [  4.62962963],\n",
       "       [  0.90702948],\n",
       "       [  1.82232346],\n",
       "       [  2.26244344],\n",
       "       [  0.44742729],\n",
       "       [  2.7027027 ],\n",
       "       [  4.10958904],\n",
       "       [  5.28735632],\n",
       "       [  6.43678161],\n",
       "       [  6.43678161],\n",
       "       [  6.20689655],\n",
       "       [  6.22119816],\n",
       "       [  7.14285714],\n",
       "       [  6.25      ],\n",
       "       [  5.50458716],\n",
       "       [  5.2154195 ],\n",
       "       [  3.6199095 ],\n",
       "       [  4.05405405],\n",
       "       [  3.37078652],\n",
       "       [  1.99115044],\n",
       "       [  1.99115044],\n",
       "       [  2.21238938],\n",
       "       [  2.89532294],\n",
       "       [  0.8988764 ],\n",
       "       [  2.28310502],\n",
       "       [  3.7037037 ],\n",
       "       [  2.73348519],\n",
       "       [  1.59453303],\n",
       "       [  1.82648402],\n",
       "       [  3.87243736],\n",
       "       [  2.48868778],\n",
       "       [  0.22172949],\n",
       "       [ -1.0989011 ],\n",
       "       [ -0.65502183],\n",
       "       [  2.00445434],\n",
       "       [  2.00892857],\n",
       "       [ -1.31868132],\n",
       "       [ -2.37580994],\n",
       "       [ -2.39130435],\n",
       "       [ -3.65591398],\n",
       "       [ -0.66815145],\n",
       "       [ -1.34228188],\n",
       "       [ -1.99115044],\n",
       "       [ -2.01342282],\n",
       "       [ -0.44742729],\n",
       "       [ -1.54867257],\n",
       "       [ -0.44742729],\n",
       "       [  0.90497738],\n",
       "       [  0.68649886],\n",
       "       [ -0.90909091],\n",
       "       [  0.4576659 ],\n",
       "       [  2.30414747],\n",
       "       [  4.13793103],\n",
       "       [  3.4562212 ],\n",
       "       [  3.22580645],\n",
       "       [  3.64464692],\n",
       "       [  4.31818182],\n",
       "       [  4.09090909],\n",
       "       [ -0.44247788],\n",
       "       [ -0.4415011 ],\n",
       "       [  0.22123894],\n",
       "       [  1.79775281],\n",
       "       [  1.78970917],\n",
       "       [  2.21238938],\n",
       "       [  2.89532294],\n",
       "       [  1.53508772],\n",
       "       [  1.75438596],\n",
       "       [  1.09170306],\n",
       "       [  0.        ],\n",
       "       [ -0.86393089],\n",
       "       [  0.86580087],\n",
       "       [  0.65075922],\n",
       "       [ -0.64516129],\n",
       "       [  5.01089325],\n",
       "       [  7.82608696],\n",
       "       [  6.46551724],\n",
       "       [  6.98689956],\n",
       "       [  6.27705628],\n",
       "       [ 11.95652174],\n",
       "       [ 11.06290672],\n",
       "       [  8.45986985],\n",
       "       [  8.65800866],\n",
       "       [  9.52380952],\n",
       "       [ 15.59020045],\n",
       "       [ 17.1875    ],\n",
       "       [ 16.07142857],\n",
       "       [ 15.07760532],\n",
       "       [ 16.8161435 ],\n",
       "       [ 13.22869955],\n",
       "       [ 12.06140351],\n",
       "       [ 11.25827815],\n",
       "       [ 12.38938053],\n",
       "       [ 14.22222222],\n",
       "       [ 11.20879121],\n",
       "       [ 11.79039301],\n",
       "       [ 11.59737418],\n",
       "       [ 12.47216036],\n",
       "       [  8.40707965],\n",
       "       [  8.24053452],\n",
       "       [  9.59821429],\n",
       "       [  9.19282511],\n",
       "       [  9.97732426],\n",
       "       [ 11.51241535],\n",
       "       [ 13.47031963],\n",
       "       [ 11.91011236],\n",
       "       [ 13.03370787],\n",
       "       [ 13.03370787],\n",
       "       [ 11.88340807],\n",
       "       [ 10.90909091],\n",
       "       [ 10.09174312],\n",
       "       [  9.11161731],\n",
       "       [  7.88288288],\n",
       "       [  5.9602649 ],\n",
       "       [  6.90423163],\n",
       "       [  5.13392857],\n",
       "       [  3.51648352],\n",
       "       [  2.83224401],\n",
       "       [  3.05676856],\n",
       "       [  6.        ],\n",
       "       [  4.4345898 ],\n",
       "       [  4.19426049],\n",
       "       [  4.19426049],\n",
       "       [  5.27472527],\n",
       "       [  3.67965368],\n",
       "       [  4.32900433],\n",
       "       [  5.61555076],\n",
       "       [  6.25      ],\n",
       "       [  6.26349892],\n",
       "       [  8.85529158],\n",
       "       [  8.71459695],\n",
       "       [  9.22746781],\n",
       "       [  7.75862069],\n",
       "       [  7.79220779],\n",
       "       [  5.18672199],\n",
       "       [  0.60483871],\n",
       "       [  1.41700405],\n",
       "       [  0.40816327],\n",
       "       [  0.20366599],\n",
       "       [ -4.66019417],\n",
       "       [ -5.078125  ],\n",
       "       [ -1.6       ],\n",
       "       [ -1.79282869],\n",
       "       [ -1.97628458],\n",
       "       [ -6.16570328],\n",
       "       [ -7.80952381],\n",
       "       [ -6.92307692],\n",
       "       [ -4.62427746],\n",
       "       [ -4.22264875],\n",
       "       [  0.79207921],\n",
       "       [  2.93542074],\n",
       "       [  2.38095238],\n",
       "       [  1.57480315],\n",
       "       [  2.72373541],\n",
       "       [  5.13833992],\n",
       "       [  2.1484375 ],\n",
       "       [  4.31372549],\n",
       "       [  8.71287129],\n",
       "       [  7.75510204],\n",
       "       [ 10.08230453],\n",
       "       [  4.88798371],\n",
       "       [  5.33880903],\n",
       "       [  5.36082474],\n",
       "       [  5.26315789],\n",
       "       [  3.42052314],\n",
       "       [  3.41365462],\n",
       "       [  4.57256461],\n",
       "       [  4.1749503 ],\n",
       "       [  4.60921844],\n",
       "       [  7.37704918],\n",
       "       [  9.58333333],\n",
       "       [ 12.94363257],\n",
       "       [ 16.28392484],\n",
       "       [ 15.625     ],\n",
       "       [ 12.5       ],\n",
       "       [ 18.47133758],\n",
       "       [ 17.62208068],\n",
       "       [ 17.79661017],\n",
       "       [ 17.58474576],\n",
       "       [ 18.86792453],\n",
       "       [ 20.16985138],\n",
       "       [ 17.16101695],\n",
       "       [ 18.43220339],\n",
       "       [ 14.82254697],\n",
       "       [ 14.61377871],\n",
       "       [ 11.61825726],\n",
       "       [  8.38445808],\n",
       "       [  9.3306288 ],\n",
       "       [  9.55284553],\n",
       "       [  6.54761905],\n",
       "       [  5.81162325],\n",
       "       [  2.1611002 ],\n",
       "       [  4.        ],\n",
       "       [  5.42168675],\n",
       "       [  2.95857988],\n",
       "       [  5.61122244],\n",
       "       [  4.19161677],\n",
       "       [  6.50406504],\n",
       "       [  5.89430894],\n",
       "       [  4.68431772],\n",
       "       [  4.32098765],\n",
       "       [  2.43902439],\n",
       "       [  0.60851927],\n",
       "       [  0.        ],\n",
       "       [  0.20533881],\n",
       "       [  3.92561983],\n",
       "       [  3.09917355],\n",
       "       [ -0.4040404 ],\n",
       "       [ -1.20240481],\n",
       "       [ -2.1611002 ],\n",
       "       [ -7.98479087],\n",
       "       [ -1.9379845 ],\n",
       "       [ -3.10077519],\n",
       "       [ -6.81818182],\n",
       "       [ -7.70676692],\n",
       "       [ -4.78011472],\n",
       "       [ -5.26315789],\n",
       "       [ -9.10746812],\n",
       "       [ -3.59848485],\n",
       "       [ -3.92523364],\n",
       "       [  0.        ],\n",
       "       [ -0.58479532],\n",
       "       [ -0.19569472],\n",
       "       [ -1.92307692],\n",
       "       [ -0.77821012],\n",
       "       [ -0.97087379],\n",
       "       [ -3.0418251 ],\n",
       "       [ -2.67175573],\n",
       "       [  2.49042146],\n",
       "       [  1.71755725],\n",
       "       [  1.90114068],\n",
       "       [ -1.1090573 ],\n",
       "       [ -2.51346499],\n",
       "       [ -3.24324324],\n",
       "       [ -0.92592593],\n",
       "       [ -5.19713262],\n",
       "       [ -4.33212996],\n",
       "       [ -5.75539568],\n",
       "       [ -5.22522523],\n",
       "       [ -5.82010582],\n",
       "       [ -6.89045936],\n",
       "       [ -5.60578662],\n",
       "       [ -7.87119857],\n",
       "       [ -4.        ],\n",
       "       [ -4.00728597],\n",
       "       [ -1.48698885],\n",
       "       [  0.37735849],\n",
       "       [ -3.52504638],\n",
       "       [ -5.00927644],\n",
       "       [ -5.40037244],\n",
       "       [ -3.97727273],\n",
       "       [ -1.53846154],\n",
       "       [ -2.88461538],\n",
       "       [ -4.38095238],\n",
       "       [ -3.44827586],\n",
       "       [ -3.03605313],\n",
       "       [ -2.49042146],\n",
       "       [ -4.38931298],\n",
       "       [ -3.83877159],\n",
       "       [  1.3618677 ],\n",
       "       [  2.56410256],\n",
       "       [  3.76984127],\n",
       "       [  4.83870968],\n",
       "       [  4.43548387],\n",
       "       [  4.91803279],\n",
       "       [  4.97017893],\n",
       "       [  6.21242485],\n",
       "       [  9.3306288 ],\n",
       "       [ 11.76470588],\n",
       "       [ 10.64257028],\n",
       "       [ 14.66942149],\n",
       "       [  7.31225296],\n",
       "       [  9.8       ],\n",
       "       [  9.75609756],\n",
       "       [ 12.62729124],\n",
       "       [ 10.24096386],\n",
       "       [  9.92063492],\n",
       "       [  9.21843687],\n",
       "       [  7.26915521],\n",
       "       [  5.83657588],\n",
       "       [  5.63106796],\n",
       "       [  7.45098039],\n",
       "       [  7.45098039],\n",
       "       [  9.01960784],\n",
       "       [  4.90196078],\n",
       "       [  4.70588235],\n",
       "       [  4.70588235],\n",
       "       [  4.70588235],\n",
       "       [ -0.74766355],\n",
       "       [ -1.68855535],\n",
       "       [ -3.17164179],\n",
       "       [  0.        ],\n",
       "       [ -2.02578269],\n",
       "       [  2.79329609],\n",
       "       [  0.37383178],\n",
       "       [  1.51228733],\n",
       "       [  0.75471698],\n",
       "       [  1.52671756],\n",
       "       [  1.14068441],\n",
       "       [ -0.18726592],\n",
       "       [  5.5028463 ],\n",
       "       [  7.27969349],\n",
       "       [  6.40776699],\n",
       "       [  3.78787879],\n",
       "       [  3.98481973],\n",
       "       [  0.56603774],\n",
       "       [  0.56390977],\n",
       "       [  2.11538462],\n",
       "       [  4.296875  ],\n",
       "       [  4.52755906],\n",
       "       [  3.15581854],\n",
       "       [  2.34375   ],\n",
       "       [  2.97029703],\n",
       "       [  1.79282869],\n",
       "       [ -1.58730159],\n",
       "       [ -4.50097847],\n",
       "       [ -5.69744597],\n",
       "       [ -4.19161677],\n",
       "       [ -3.99201597],\n",
       "       [ -6.33397313],\n",
       "       [ -6.15384615],\n",
       "       [ -6.69216061],\n",
       "       [ -6.34615385],\n",
       "       [ -5.40540541],\n",
       "       [ -3.90625   ],\n",
       "       [ -9.09090909],\n",
       "       [ -8.86792453],\n",
       "       [-11.13172542],\n",
       "       [-12.52268603],\n",
       "       [-12.88566243],\n",
       "       [-13.51351351],\n",
       "       [-13.62799263],\n",
       "       [-14.75409836],\n",
       "       [-14.07407407],\n",
       "       [-16.09403255],\n",
       "       [-16.39344262],\n",
       "       [-15.70397112],\n",
       "       [-13.39449541],\n",
       "       [-13.91941392],\n",
       "       [-14.52205882],\n",
       "       [-13.05147059],\n",
       "       [-12.59124088],\n",
       "       [-12.77372263],\n",
       "       [-16.36690647],\n",
       "       [-14.01869159],\n",
       "       [-13.29588015],\n",
       "       [-14.41947566],\n",
       "       [-15.16853933],\n",
       "       [-14.12429379],\n",
       "       [-13.54961832],\n",
       "       [-11.36801541],\n",
       "       [-12.71028037],\n",
       "       [-11.65413534],\n",
       "       [-14.13043478],\n",
       "       [-12.66294227],\n",
       "       [-13.22160149],\n",
       "       [-12.92134831],\n",
       "       [-11.65413534],\n",
       "       [-12.59398496],\n",
       "       [-13.32082552],\n",
       "       [-17.62589928],\n",
       "       [-20.        ],\n",
       "       [-17.70072993],\n",
       "       [-18.43065693],\n",
       "       [-18.97810219],\n",
       "       [-16.69793621],\n",
       "       [-15.88785047],\n",
       "       [-14.87758945],\n",
       "       [-16.29213483],\n",
       "       [-14.68926554],\n",
       "       [-14.91395793],\n",
       "       [-13.54961832],\n",
       "       [-12.11538462],\n",
       "       [-10.56751468],\n",
       "       [ -7.86290323],\n",
       "       [ -5.12295082],\n",
       "       [ -3.95833333],\n",
       "       [ -3.95833333],\n",
       "       [ -5.1975052 ],\n",
       "       [ -7.37704918],\n",
       "       [ -8.81147541],\n",
       "       [ -7.99180328],\n",
       "       [ -9.24024641],\n",
       "       [-10.        ],\n",
       "       [-10.16260163],\n",
       "       [ -7.5       ],\n",
       "       [ -7.66045549],\n",
       "       [ -7.09812109],\n",
       "       [ -7.67634855],\n",
       "       [ -6.45833333],\n",
       "       [ -6.66666667],\n",
       "       [ -2.98507463],\n",
       "       [ -2.77777778],\n",
       "       [ -1.93965517],\n",
       "       [ -3.87931034],\n",
       "       [ -3.26797386],\n",
       "       [ -4.92505353],\n",
       "       [ -5.50847458],\n",
       "       [ -5.31914894],\n",
       "       [ -4.30107527],\n",
       "       [ -4.22832981],\n",
       "       [ -3.96659708],\n",
       "       [  0.83682008],\n",
       "       [  6.02150538],\n",
       "       [  7.17391304],\n",
       "       [  4.31965443],\n",
       "       [  2.62582057],\n",
       "       [  4.85651214],\n",
       "       [  3.28947368],\n",
       "       [  6.62251656],\n",
       "       [  5.65217391],\n",
       "       [  3.64025696],\n",
       "       [  0.85106383],\n",
       "       [  0.        ],\n",
       "       [  0.21321962],\n",
       "       [ -0.64377682],\n",
       "       [  0.21505376],\n",
       "       [ -1.91489362],\n",
       "       [ -1.50537634],\n",
       "       [ -1.08225108],\n",
       "       [ -0.43668122],\n",
       "       [  1.5625    ],\n",
       "       [  1.33037694],\n",
       "       [  4.25055928],\n",
       "       [  6.98198198],\n",
       "       [  6.98198198],\n",
       "       [  6.88888889],\n",
       "       [  4.86725664],\n",
       "       [  6.48769575],\n",
       "       [  4.85651214],\n",
       "       [  6.29213483],\n",
       "       [  5.73951435],\n",
       "       [  5.25164114],\n",
       "       [  5.47045952],\n",
       "       [  5.90809628],\n",
       "       [  3.23974082],\n",
       "       [  4.98915401],\n",
       "       [  6.29067245],\n",
       "       [  9.86842105],\n",
       "       [  9.2920354 ],\n",
       "       [ 13.70786517],\n",
       "       [ 14.4766147 ],\n",
       "       [ 17.42081448],\n",
       "       [ 13.60544218],\n",
       "       [ 14.02714932],\n",
       "       [ 15.76576577],\n",
       "       [ 15.02242152],\n",
       "       [ 15.50561798],\n",
       "       [ 16.62921348],\n",
       "       [ 15.59020045],\n",
       "       [ 18.08035714],\n",
       "       [ 14.94505495],\n",
       "       [ 15.16483516],\n",
       "       [ 12.96703297],\n",
       "       [ 13.90134529],\n",
       "       [ 16.44144144],\n",
       "       [ 18.69369369],\n",
       "       [ 17.04035874],\n",
       "       [ 17.75280899],\n",
       "       [ 18.42696629],\n",
       "       [ 15.23178808],\n",
       "       [ 14.13043478],\n",
       "       [  6.43153527],\n",
       "       [  6.28803245],\n",
       "       [  6.49087221],\n",
       "       [  8.2815735 ],\n",
       "       [ 12.36673774],\n",
       "       [ 11.78947368],\n",
       "       [ 15.2866242 ],\n",
       "       [ 13.45755694],\n",
       "       [ 13.58024691],\n",
       "       [ 14.46280992],\n",
       "       [ 17.51054852],\n",
       "       [ 16.87763713],\n",
       "       [ 14.68085106],\n",
       "       [ 17.27861771],\n",
       "       [ 15.66523605],\n",
       "       [ 16.9197397 ],\n",
       "       [ 17.68558952],\n",
       "       [ 17.94310722],\n",
       "       [ 20.61403509],\n",
       "       [ 23.95604396],\n",
       "       [ 20.56892779],\n",
       "       [ 16.73819742],\n",
       "       [ 14.52631579],\n",
       "       [ 17.68421053],\n",
       "       [ 16.83991684],\n",
       "       [ 17.51054852],\n",
       "       [ 17.01680672],\n",
       "       [ 20.        ],\n",
       "       [ 14.16490486],\n",
       "       [ 12.7348643 ],\n",
       "       [ 11.64241164],\n",
       "       [ 11.2033195 ],\n",
       "       [ 13.2231405 ],\n",
       "       [ 16.94560669],\n",
       "       [ 16.73553719],\n",
       "       [ 16.73469388],\n",
       "       [ 12.9740519 ],\n",
       "       [ 12.75303644],\n",
       "       [ 12.05533597],\n",
       "       [  7.00389105],\n",
       "       [  6.3583815 ],\n",
       "       [  7.38522954],\n",
       "       [  6.34920635],\n",
       "       [  1.75097276],\n",
       "       [  0.38986355],\n",
       "       [  2.52918288],\n",
       "       [  1.15606936],\n",
       "       [  3.46820809],\n",
       "       [  1.1342155 ],\n",
       "       [  1.7208413 ],\n",
       "       [  1.90839695],\n",
       "       [  3.69649805],\n",
       "       [  6.2992126 ],\n",
       "       [  3.28820116],\n",
       "       [  5.88235294],\n",
       "       [  5.74712644],\n",
       "       [  7.82442748],\n",
       "       [  6.26185958],\n",
       "       [  9.77011494],\n",
       "       [  8.19047619],\n",
       "       [  8.9668616 ],\n",
       "       [  6.29770992],\n",
       "       [  8.19047619],\n",
       "       [  8.6042065 ],\n",
       "       [  9.86717268],\n",
       "       [  6.21468927],\n",
       "       [  2.39410681],\n",
       "       [  2.91970803],\n",
       "       [  4.16666667],\n",
       "       [  2.16606498],\n",
       "       [  1.79533214],\n",
       "       [  3.79061372],\n",
       "       [  5.00927644],\n",
       "       [  3.13075506],\n",
       "       [  3.8961039 ],\n",
       "       [  2.78293135],\n",
       "       [  0.18552876],\n",
       "       [  1.2987013 ],\n",
       "       [ -0.72727273],\n",
       "       [ -2.12765957],\n",
       "       [  0.1814882 ],\n",
       "       [  1.47058824],\n",
       "       [ -1.10294118],\n",
       "       [ -4.83005367],\n",
       "       [ -4.09252669],\n",
       "       [ -3.23159785],\n",
       "       [ -3.23159785],\n",
       "       [ -4.38596491],\n",
       "       [  2.03703704],\n",
       "       [  3.51851852],\n",
       "       [  2.97951583],\n",
       "       [  2.98507463],\n",
       "       [ -1.64233577],\n",
       "       [ -3.0411449 ],\n",
       "       [ -6.72566372],\n",
       "       [ -6.64335664],\n",
       "       [ -5.12367491],\n",
       "       [ -3.9497307 ],\n",
       "       [ -5.29100529],\n",
       "       [ -2.        ],\n",
       "       [ -4.52898551],\n",
       "       [ -1.85873606],\n",
       "       [  0.37313433],\n",
       "       [  3.63288719],\n",
       "       [  4.46601942],\n",
       "       [  3.41555977],\n",
       "       [  1.9047619 ],\n",
       "       [ -3.72439479],\n",
       "       [ -3.55140187],\n",
       "       [ -2.2556391 ],\n",
       "       [ -2.62172285],\n",
       "       [ -2.43902439],\n",
       "       [ -3.7037037 ],\n",
       "       [ -2.80898876],\n",
       "       [ -6.4516129 ],\n",
       "       [ -5.43478261],\n",
       "       [ -8.14159292],\n",
       "       [ -7.32142857],\n",
       "       [ -9.2495637 ],\n",
       "       [-10.21126761],\n",
       "       [ -7.15563506],\n",
       "       [ -6.6427289 ],\n",
       "       [ -8.45070423],\n",
       "       [ -8.8028169 ],\n",
       "       [ -9.84455959],\n",
       "       [ -7.80141844],\n",
       "       [ -6.65467626],\n",
       "       [ -6.73758865],\n",
       "       [ -9.39130435],\n",
       "       [ -7.95053004],\n",
       "       [ -7.93650794],\n",
       "       [ -9.2173913 ],\n",
       "       [ -8.83392226],\n",
       "       [ -9.28571429],\n",
       "       [-10.        ],\n",
       "       [-10.10830325],\n",
       "       [ -8.14814815],\n",
       "       [-10.07326007],\n",
       "       [ -9.70695971],\n",
       "       [ -8.15217391],\n",
       "       [ -8.69565217],\n",
       "       [ -8.33333333],\n",
       "       [ -6.69144981],\n",
       "       [ -6.20300752],\n",
       "       [ -6.49350649],\n",
       "       [ -6.12244898],\n",
       "       [ -7.05009276],\n",
       "       [ -7.1559633 ],\n",
       "       [ -8.71143376],\n",
       "       [-10.01788909],\n",
       "       [ -7.95660036],\n",
       "       [ -9.05797101],\n",
       "       [ -9.83302412],\n",
       "       [ -9.4095941 ],\n",
       "       [ -6.4516129 ],\n",
       "       [ -8.05243446],\n",
       "       [ -9.12476723],\n",
       "       [ -7.85046729],\n",
       "       [ -8.75232775],\n",
       "       [ -9.09090909],\n",
       "       [ -6.64136622],\n",
       "       [ -6.25      ],\n",
       "       [ -7.99256506],\n",
       "       [ -9.77859779],\n",
       "       [ -9.4795539 ],\n",
       "       [-10.27522936],\n",
       "       [ -8.97196262],\n",
       "       [ -4.83558994],\n",
       "       [ -3.87596899],\n",
       "       [ -5.19230769],\n",
       "       [ -4.80769231],\n",
       "       [ -4.80769231],\n",
       "       [ -4.80769231],\n",
       "       [ -3.08285164],\n",
       "       [ -1.91570881],\n",
       "       [ -1.91570881],\n",
       "       [ -1.15606936],\n",
       "       [  1.15606936],\n",
       "       [  1.34615385],\n",
       "       [  2.94117647],\n",
       "       [  0.        ],\n",
       "       [ -0.19230769],\n",
       "       [ -0.76923077],\n",
       "       [ -0.38610039],\n",
       "       [ -1.34099617],\n",
       "       [  0.19230769],\n",
       "       [ -0.57803468],\n",
       "       [ -3.80228137],\n",
       "       [ -3.64683301],\n",
       "       [ -2.87907869],\n",
       "       [ -1.72413793],\n",
       "       [ -2.87356322],\n",
       "       [ -2.3255814 ],\n",
       "       [ -1.57480315],\n",
       "       [ -0.5952381 ],\n",
       "       [  0.40160643],\n",
       "       [  2.41935484],\n",
       "       [  4.88798371],\n",
       "       [  4.46247465],\n",
       "       [  0.59171598],\n",
       "       [  0.5952381 ],\n",
       "       [  0.19762846],\n",
       "       [  2.39043825],\n",
       "       [  4.00801603],\n",
       "       [  2.77777778],\n",
       "       [  3.35968379],\n",
       "       [  5.38922156],\n",
       "       [  3.95256917],\n",
       "       [  3.57852883],\n",
       "       [  4.1749503 ],\n",
       "       [  2.1611002 ],\n",
       "       [  3.187251  ],\n",
       "       [  6.58436214],\n",
       "       [  4.88798371],\n",
       "       [  5.07099391],\n",
       "       [  4.68431772],\n",
       "       [  6.76229508],\n",
       "       [  4.6653144 ],\n",
       "       [  6.93877551],\n",
       "       [  7.14285714],\n",
       "       [  5.89430894],\n",
       "       [  3.03030303],\n",
       "       [  2.82828283],\n",
       "       [  4.08997955],\n",
       "       [  6.7761807 ],\n",
       "       [  6.54396728],\n",
       "       [  6.57084189],\n",
       "       [  5.28455285],\n",
       "       [  4.63709677],\n",
       "       [  4.86815416],\n",
       "       [  3.83838384],\n",
       "       [  4.04040404],\n",
       "       [  3.03030303],\n",
       "       [ -1.19284294],\n",
       "       [ -4.1015625 ]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(y_test, axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-20.88353414, -20.41237113, -18.04979253, -18.65828092,\n",
       "       -13.91304348, -12.77056277, -11.06290672,  -7.92951542,\n",
       "        -5.6768559 ,  -4.62555066,  -9.17030568,  -9.26724138,\n",
       "        -9.26724138,  -9.85010707,  -9.0712743 ,  -8.47826087,\n",
       "       -11.85344828,  -9.44206009,  -6.88172043,  -7.08154506,\n",
       "        -6.91144708,  -6.17021277,  -5.2742616 ,  -3.41880342,\n",
       "        -2.35042735,  -2.14132762,  -2.80777538,  -2.15053763,\n",
       "        -0.21367521,   0.21459227,  -2.62008734,  -3.50877193,\n",
       "        -7.47863248,  -8.62068966,  -7.01754386,  -6.12691466,\n",
       "        -7.74193548,  -4.79302832,  -5.90809628,  -5.48245614,\n",
       "        -6.23655914,  -6.63811563,  -8.13704497,  -7.17391304,\n",
       "        -1.86046512,  -2.55220418,  -2.55220418,  -3.71229698,\n",
       "         0.        ,   0.71428571,   0.72115385,   2.20588235,\n",
       "         0.96153846,   1.47783251,   0.24213075,  -2.15827338,\n",
       "        -2.39808153,  -1.45985401,  -0.73891626,   0.49382716,\n",
       "         3.29949239,   9.32642487,   6.58227848,   7.73195876,\n",
       "         4.29292929,   2.48138958,  -0.24390244,  -3.58851675,\n",
       "        -6.25      ,  -4.61893764,  -2.16346154,   0.        ,\n",
       "        -2.13776722,  -0.23752969,   1.42517815,   0.71258907,\n",
       "         3.66748166,   0.47393365,  -3.00230947,  -2.5404157 ,\n",
       "        -1.39211137,  -3.17460317,  -5.12249443,  -6.19469027,\n",
       "        -8.09628009,  -7.65864333,  -6.44444444,  -7.47252747,\n",
       "       -10.49250535,  -7.92291221,  -3.3632287 ,  -3.18181818,\n",
       "         0.        ,   3.77358491,   4.71698113,   1.86480186,\n",
       "         0.6993007 ,   0.68649886,   2.3255814 ,   2.78422274,\n",
       "         0.4587156 ,  -0.4587156 ,  -1.16550117,  -1.17096019,\n",
       "         2.36966825,   2.85714286,   0.23809524,   1.20481928,\n",
       "        -0.47505938,  -0.94562648,   0.23866348,   0.71942446,\n",
       "        -0.71428571,   2.66990291,   2.65700483,   6.37254902,\n",
       "         5.8968059 ,   7.90123457,   8.68486352,   6.38820639,\n",
       "         5.65110565,   1.65876777,   2.6128266 ,   3.8277512 ,\n",
       "         5.08474576,   6.53753027,   8.06845966,   9.4292804 ,\n",
       "        10.12345679,   7.02179177,   7.12530713,   5.70071259,\n",
       "         8.49514563,   9.52380952,   8.66510539,   8.96226415,\n",
       "         8.25471698,   8.49056604,   7.85714286,   5.92417062,\n",
       "         4.        ,   3.27868852,   3.75586854,   4.71698113,\n",
       "         6.9047619 ,   6.16113744,   7.83847981,   7.60095012,\n",
       "         8.37320574,   5.58139535,   5.80046404,   6.33802817,\n",
       "         3.46420323,   0.45454545,  -0.9009009 ,   0.91533181,\n",
       "         2.08333333,   1.13636364,   0.45454545,  -0.45146727,\n",
       "        -0.2283105 ,  -0.23041475,   1.88679245,   4.50236967,\n",
       "         1.62037037,   2.31481481,   6.17577197,   5.71428571,\n",
       "         4.53460621,   3.81861575,   3.57142857,   3.57142857,\n",
       "         4.31654676,   2.60047281,   2.11764706,  -0.46082949,\n",
       "         1.16009281,   0.91533181,   0.91324201,   2.5404157 ,\n",
       "         3.48837209,   5.36130536,   4.62962963,   4.14746544,\n",
       "         3.4562212 ,   1.13636364,  -0.90497738,  -2.04081633,\n",
       "        -1.56950673,  -0.67873303,   0.4587156 ,  -1.34831461,\n",
       "        -1.11856823,  -1.95652174,  -1.93965517,  -0.86580087,\n",
       "        -2.17864924,  -2.60869565,   0.4415011 ,   3.57941834,\n",
       "         4.07239819,   5.44217687,   1.58371041,   0.67567568,\n",
       "         0.66815145,  -0.22321429,  -1.54185022,  -0.22075055,\n",
       "        -1.32450331,  -2.64317181,  -4.16666667,  -2.86975717,\n",
       "        -2.45535714,  -1.80995475,  -1.13636364,  -1.58730159,\n",
       "        -1.58730159,  -1.34831461,  -0.45248869,  -0.22675737,\n",
       "         3.43249428,   4.61893764,   4.62962963,   0.90702948,\n",
       "         1.82232346,   2.26244344,   0.44742729,   2.7027027 ,\n",
       "         4.10958904,   5.28735632,   6.43678161,   6.43678161,\n",
       "         6.20689655,   6.22119816,   7.14285714,   6.25      ,\n",
       "         5.50458716,   5.2154195 ,   3.6199095 ,   4.05405405,\n",
       "         3.37078652,   1.99115044,   1.99115044,   2.21238938,\n",
       "         2.89532294,   0.8988764 ,   2.28310502,   3.7037037 ,\n",
       "         2.73348519,   1.59453303,   1.82648402,   3.87243736,\n",
       "         2.48868778,   0.22172949,  -1.0989011 ,  -0.65502183,\n",
       "         2.00445434,   2.00892857,  -1.31868132,  -2.37580994,\n",
       "        -2.39130435,  -3.65591398,  -0.66815145,  -1.34228188,\n",
       "        -1.99115044,  -2.01342282,  -0.44742729,  -1.54867257,\n",
       "        -0.44742729,   0.90497738,   0.68649886,  -0.90909091,\n",
       "         0.4576659 ,   2.30414747,   4.13793103,   3.4562212 ,\n",
       "         3.22580645,   3.64464692,   4.31818182,   4.09090909,\n",
       "        -0.44247788,  -0.4415011 ,   0.22123894,   1.79775281,\n",
       "         1.78970917,   2.21238938,   2.89532294,   1.53508772,\n",
       "         1.75438596,   1.09170306,   0.        ,  -0.86393089,\n",
       "         0.86580087,   0.65075922,  -0.64516129,   5.01089325,\n",
       "         7.82608696,   6.46551724,   6.98689956,   6.27705628,\n",
       "        11.95652174,  11.06290672,   8.45986985,   8.65800866,\n",
       "         9.52380952,  15.59020045,  17.1875    ,  16.07142857,\n",
       "        15.07760532,  16.8161435 ,  13.22869955,  12.06140351,\n",
       "        11.25827815,  12.38938053,  14.22222222,  11.20879121,\n",
       "        11.79039301,  11.59737418,  12.47216036,   8.40707965,\n",
       "         8.24053452,   9.59821429,   9.19282511,   9.97732426,\n",
       "        11.51241535,  13.47031963,  11.91011236,  13.03370787,\n",
       "        13.03370787,  11.88340807,  10.90909091,  10.09174312,\n",
       "         9.11161731,   7.88288288,   5.9602649 ,   6.90423163,\n",
       "         5.13392857,   3.51648352,   2.83224401,   3.05676856,\n",
       "         6.        ,   4.4345898 ,   4.19426049,   4.19426049,\n",
       "         5.27472527,   3.67965368,   4.32900433,   5.61555076,\n",
       "         6.25      ,   6.26349892,   8.85529158,   8.71459695,\n",
       "         9.22746781,   7.75862069,   7.79220779,   5.18672199,\n",
       "         0.60483871,   1.41700405,   0.40816327,   0.20366599,\n",
       "        -4.66019417,  -5.078125  ,  -1.6       ,  -1.79282869,\n",
       "        -1.97628458,  -6.16570328,  -7.80952381,  -6.92307692,\n",
       "        -4.62427746,  -4.22264875,   0.79207921,   2.93542074,\n",
       "         2.38095238,   1.57480315,   2.72373541,   5.13833992,\n",
       "         2.1484375 ,   4.31372549,   8.71287129,   7.75510204,\n",
       "        10.08230453,   4.88798371,   5.33880903,   5.36082474,\n",
       "         5.26315789,   3.42052314,   3.41365462,   4.57256461,\n",
       "         4.1749503 ,   4.60921844,   7.37704918,   9.58333333,\n",
       "        12.94363257,  16.28392484,  15.625     ,  12.5       ,\n",
       "        18.47133758,  17.62208068,  17.79661017,  17.58474576,\n",
       "        18.86792453,  20.16985138,  17.16101695,  18.43220339,\n",
       "        14.82254697,  14.61377871,  11.61825726,   8.38445808,\n",
       "         9.3306288 ,   9.55284553,   6.54761905,   5.81162325,\n",
       "         2.1611002 ,   4.        ,   5.42168675,   2.95857988,\n",
       "         5.61122244,   4.19161677,   6.50406504,   5.89430894,\n",
       "         4.68431772,   4.32098765,   2.43902439,   0.60851927,\n",
       "         0.        ,   0.20533881,   3.92561983,   3.09917355,\n",
       "        -0.4040404 ,  -1.20240481,  -2.1611002 ,  -7.98479087,\n",
       "        -1.9379845 ,  -3.10077519,  -6.81818182,  -7.70676692,\n",
       "        -4.78011472,  -5.26315789,  -9.10746812,  -3.59848485,\n",
       "        -3.92523364,   0.        ,  -0.58479532,  -0.19569472,\n",
       "        -1.92307692,  -0.77821012,  -0.97087379,  -3.0418251 ,\n",
       "        -2.67175573,   2.49042146,   1.71755725,   1.90114068,\n",
       "        -1.1090573 ,  -2.51346499,  -3.24324324,  -0.92592593,\n",
       "        -5.19713262,  -4.33212996,  -5.75539568,  -5.22522523,\n",
       "        -5.82010582,  -6.89045936,  -5.60578662,  -7.87119857,\n",
       "        -4.        ,  -4.00728597,  -1.48698885,   0.37735849,\n",
       "        -3.52504638,  -5.00927644,  -5.40037244,  -3.97727273,\n",
       "        -1.53846154,  -2.88461538,  -4.38095238,  -3.44827586,\n",
       "        -3.03605313,  -2.49042146,  -4.38931298,  -3.83877159,\n",
       "         1.3618677 ,   2.56410256,   3.76984127,   4.83870968,\n",
       "         4.43548387,   4.91803279,   4.97017893,   6.21242485,\n",
       "         9.3306288 ,  11.76470588,  10.64257028,  14.66942149,\n",
       "         7.31225296,   9.8       ,   9.75609756,  12.62729124,\n",
       "        10.24096386,   9.92063492,   9.21843687,   7.26915521,\n",
       "         5.83657588,   5.63106796,   7.45098039,   7.45098039,\n",
       "         9.01960784,   4.90196078,   4.70588235,   4.70588235,\n",
       "         4.70588235,  -0.74766355,  -1.68855535,  -3.17164179,\n",
       "         0.        ,  -2.02578269,   2.79329609,   0.37383178,\n",
       "         1.51228733,   0.75471698,   1.52671756,   1.14068441,\n",
       "        -0.18726592,   5.5028463 ,   7.27969349,   6.40776699,\n",
       "         3.78787879,   3.98481973,   0.56603774,   0.56390977,\n",
       "         2.11538462,   4.296875  ,   4.52755906,   3.15581854,\n",
       "         2.34375   ,   2.97029703,   1.79282869,  -1.58730159,\n",
       "        -4.50097847,  -5.69744597,  -4.19161677,  -3.99201597,\n",
       "        -6.33397313,  -6.15384615,  -6.69216061,  -6.34615385,\n",
       "        -5.40540541,  -3.90625   ,  -9.09090909,  -8.86792453,\n",
       "       -11.13172542, -12.52268603, -12.88566243, -13.51351351,\n",
       "       -13.62799263, -14.75409836, -14.07407407, -16.09403255,\n",
       "       -16.39344262, -15.70397112, -13.39449541, -13.91941392,\n",
       "       -14.52205882, -13.05147059, -12.59124088, -12.77372263,\n",
       "       -16.36690647, -14.01869159, -13.29588015, -14.41947566,\n",
       "       -15.16853933, -14.12429379, -13.54961832, -11.36801541,\n",
       "       -12.71028037, -11.65413534, -14.13043478, -12.66294227,\n",
       "       -13.22160149, -12.92134831, -11.65413534, -12.59398496,\n",
       "       -13.32082552, -17.62589928, -20.        , -17.70072993,\n",
       "       -18.43065693, -18.97810219, -16.69793621, -15.88785047,\n",
       "       -14.87758945, -16.29213483, -14.68926554, -14.91395793,\n",
       "       -13.54961832, -12.11538462, -10.56751468,  -7.86290323,\n",
       "        -5.12295082,  -3.95833333,  -3.95833333,  -5.1975052 ,\n",
       "        -7.37704918,  -8.81147541,  -7.99180328,  -9.24024641,\n",
       "       -10.        , -10.16260163,  -7.5       ,  -7.66045549,\n",
       "        -7.09812109,  -7.67634855,  -6.45833333,  -6.66666667,\n",
       "        -2.98507463,  -2.77777778,  -1.93965517,  -3.87931034,\n",
       "        -3.26797386,  -4.92505353,  -5.50847458,  -5.31914894,\n",
       "        -4.30107527,  -4.22832981,  -3.96659708,   0.83682008,\n",
       "         6.02150538,   7.17391304,   4.31965443,   2.62582057,\n",
       "         4.85651214,   3.28947368,   6.62251656,   5.65217391,\n",
       "         3.64025696,   0.85106383,   0.        ,   0.21321962,\n",
       "        -0.64377682,   0.21505376,  -1.91489362,  -1.50537634,\n",
       "        -1.08225108,  -0.43668122,   1.5625    ,   1.33037694,\n",
       "         4.25055928,   6.98198198,   6.98198198,   6.88888889,\n",
       "         4.86725664,   6.48769575,   4.85651214,   6.29213483,\n",
       "         5.73951435,   5.25164114,   5.47045952,   5.90809628,\n",
       "         3.23974082,   4.98915401,   6.29067245,   9.86842105,\n",
       "         9.2920354 ,  13.70786517,  14.4766147 ,  17.42081448,\n",
       "        13.60544218,  14.02714932,  15.76576577,  15.02242152,\n",
       "        15.50561798,  16.62921348,  15.59020045,  18.08035714,\n",
       "        14.94505495,  15.16483516,  12.96703297,  13.90134529,\n",
       "        16.44144144,  18.69369369,  17.04035874,  17.75280899,\n",
       "        18.42696629,  15.23178808,  14.13043478,   6.43153527,\n",
       "         6.28803245,   6.49087221,   8.2815735 ,  12.36673774,\n",
       "        11.78947368,  15.2866242 ,  13.45755694,  13.58024691,\n",
       "        14.46280992,  17.51054852,  16.87763713,  14.68085106,\n",
       "        17.27861771,  15.66523605,  16.9197397 ,  17.68558952,\n",
       "        17.94310722,  20.61403509,  23.95604396,  20.56892779,\n",
       "        16.73819742,  14.52631579,  17.68421053,  16.83991684,\n",
       "        17.51054852,  17.01680672,  20.        ,  14.16490486,\n",
       "        12.7348643 ,  11.64241164,  11.2033195 ,  13.2231405 ,\n",
       "        16.94560669,  16.73553719,  16.73469388,  12.9740519 ,\n",
       "        12.75303644,  12.05533597,   7.00389105,   6.3583815 ,\n",
       "         7.38522954,   6.34920635,   1.75097276,   0.38986355,\n",
       "         2.52918288,   1.15606936,   3.46820809,   1.1342155 ,\n",
       "         1.7208413 ,   1.90839695,   3.69649805,   6.2992126 ,\n",
       "         3.28820116,   5.88235294,   5.74712644,   7.82442748,\n",
       "         6.26185958,   9.77011494,   8.19047619,   8.9668616 ,\n",
       "         6.29770992,   8.19047619,   8.6042065 ,   9.86717268,\n",
       "         6.21468927,   2.39410681,   2.91970803,   4.16666667,\n",
       "         2.16606498,   1.79533214,   3.79061372,   5.00927644,\n",
       "         3.13075506,   3.8961039 ,   2.78293135,   0.18552876,\n",
       "         1.2987013 ,  -0.72727273,  -2.12765957,   0.1814882 ,\n",
       "         1.47058824,  -1.10294118,  -4.83005367,  -4.09252669,\n",
       "        -3.23159785,  -3.23159785,  -4.38596491,   2.03703704,\n",
       "         3.51851852,   2.97951583,   2.98507463,  -1.64233577,\n",
       "        -3.0411449 ,  -6.72566372,  -6.64335664,  -5.12367491,\n",
       "        -3.9497307 ,  -5.29100529,  -2.        ,  -4.52898551,\n",
       "        -1.85873606,   0.37313433,   3.63288719,   4.46601942,\n",
       "         3.41555977,   1.9047619 ,  -3.72439479,  -3.55140187,\n",
       "        -2.2556391 ,  -2.62172285,  -2.43902439,  -3.7037037 ,\n",
       "        -2.80898876,  -6.4516129 ,  -5.43478261,  -8.14159292,\n",
       "        -7.32142857,  -9.2495637 , -10.21126761,  -7.15563506,\n",
       "        -6.6427289 ,  -8.45070423,  -8.8028169 ,  -9.84455959,\n",
       "        -7.80141844,  -6.65467626,  -6.73758865,  -9.39130435,\n",
       "        -7.95053004,  -7.93650794,  -9.2173913 ,  -8.83392226,\n",
       "        -9.28571429, -10.        , -10.10830325,  -8.14814815,\n",
       "       -10.07326007,  -9.70695971,  -8.15217391,  -8.69565217,\n",
       "        -8.33333333,  -6.69144981,  -6.20300752,  -6.49350649,\n",
       "        -6.12244898,  -7.05009276,  -7.1559633 ,  -8.71143376,\n",
       "       -10.01788909,  -7.95660036,  -9.05797101,  -9.83302412,\n",
       "        -9.4095941 ,  -6.4516129 ,  -8.05243446,  -9.12476723,\n",
       "        -7.85046729,  -8.75232775,  -9.09090909,  -6.64136622,\n",
       "        -6.25      ,  -7.99256506,  -9.77859779,  -9.4795539 ,\n",
       "       -10.27522936,  -8.97196262,  -4.83558994,  -3.87596899,\n",
       "        -5.19230769,  -4.80769231,  -4.80769231,  -4.80769231,\n",
       "        -3.08285164,  -1.91570881,  -1.91570881,  -1.15606936,\n",
       "         1.15606936,   1.34615385,   2.94117647,   0.        ,\n",
       "        -0.19230769,  -0.76923077,  -0.38610039,  -1.34099617,\n",
       "         0.19230769,  -0.57803468,  -3.80228137,  -3.64683301,\n",
       "        -2.87907869,  -1.72413793,  -2.87356322,  -2.3255814 ,\n",
       "        -1.57480315,  -0.5952381 ,   0.40160643,   2.41935484,\n",
       "         4.88798371,   4.46247465,   0.59171598,   0.5952381 ,\n",
       "         0.19762846,   2.39043825,   4.00801603,   2.77777778,\n",
       "         3.35968379,   5.38922156,   3.95256917,   3.57852883,\n",
       "         4.1749503 ,   2.1611002 ,   3.187251  ,   6.58436214,\n",
       "         4.88798371,   5.07099391,   4.68431772,   6.76229508,\n",
       "         4.6653144 ,   6.93877551,   7.14285714,   5.89430894,\n",
       "         3.03030303,   2.82828283,   4.08997955,   6.7761807 ,\n",
       "         6.54396728,   6.57084189,   5.28455285,   4.63709677,\n",
       "         4.86815416,   3.83838384,   4.04040404,   3.03030303,\n",
       "        -1.19284294,  -4.1015625 ])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1  loss = 478.65594  train MSE = 478.65594  test MSE = 58649733000000.0\n",
      "Epoch : 2  loss = 639.9042  train MSE = 639.9042  test MSE = 54951766000000.0\n",
      "Epoch : 3  loss = 333.96024  train MSE = 333.96024  test MSE = 49223880000000.0\n",
      "Epoch : 4  loss = 851.3628  train MSE = 851.3628  test MSE = 44616087000000.0\n",
      "Epoch : 5  loss = 93.473976  train MSE = 93.473976  test MSE = 40522740000000.0\n",
      "Epoch : 6  loss = 1314.4886  train MSE = 1314.4886  test MSE = 37648210000000.0\n",
      "Epoch : 7  loss = 260.53476  train MSE = 260.53476  test MSE = 34977849000000.0\n",
      "Epoch : 8  loss = 973.346  train MSE = 973.346  test MSE = 33226850000000.0\n",
      "Epoch : 9  loss = 206.98358  train MSE = 206.98358  test MSE = 30848630000000.0\n",
      "Epoch : 10  loss = 445.55548  train MSE = 445.55548  test MSE = 28513682000000.0\n",
      "Epoch : 11  loss = 202.9843  train MSE = 202.9843  test MSE = 26248516000000.0\n",
      "Epoch : 12  loss = 1348.9167  train MSE = 1348.9167  test MSE = 25434220000000.0\n",
      "Epoch : 13  loss = 253.38437  train MSE = 253.38437  test MSE = 24339896000000.0\n",
      "Epoch : 14  loss = 321.6694  train MSE = 321.6694  test MSE = 23412462000000.0\n",
      "Epoch : 15  loss = 154.78156  train MSE = 154.78156  test MSE = 22494480000000.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_FallbackException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;34m\"explicit_paddings\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data_format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m         \"dilations\", dilations)\n\u001b[0m\u001b[1;32m   1025\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31m_FallbackException\u001b[0m: Expecting int64_t value for attr strides, got numpy.int32",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-e4c43c59139f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mtrain_MSE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_CNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mtest_MSE\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_CNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_4D\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch :'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' loss ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m' train MSE ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_MSE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m' test MSE ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_MSE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-69e42ec110f9>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(model, X_data, labels)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mMES\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_mean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msquare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mMES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    677\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    678\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[0;32m--> 679\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    749\u001b[0m                                 ' implement a `call` method.')\n\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_internal_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcompute_output_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_run_internal_graph\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m           \u001b[0;31m# Compute outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m           \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m           \u001b[0;31m# Update tensor_dict.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    677\u001b[0m           with base_layer_utils.autocast_context_manager(\n\u001b[1;32m    678\u001b[0m               input_list, self._mixed_precision_policy.should_cast_variables):\n\u001b[0;32m--> 679\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    680\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle_activity_regularization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_mask_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convolution_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inp, filter)\u001b[0m\n\u001b[1;32m   1077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inp, filter)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=redefined-builtin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inp, filter)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m         \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         name=self.name)\n\u001b[0m\u001b[1;32m    235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, data_format, dilations, name, filters)\u001b[0m\n\u001b[1;32m   1951\u001b[0m                            \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1952\u001b[0m                            \u001b[0mdilations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdilations\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1953\u001b[0;31m                            name=name)\n\u001b[0m\u001b[1;32m   1954\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[1;32m   1029\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m             data_format=data_format, dilations=dilations, name=name, ctx=_ctx)\n\u001b[0m\u001b[1;32m   1032\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_SymbolicException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m         \u001b[0;32mpass\u001b[0m  \u001b[0;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_eager_fallback\u001b[0;34m(input, filter, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name, ctx)\u001b[0m\n\u001b[1;32m   1128\u001b[0m   explicit_paddings, \"data_format\", data_format, \"dilations\", dilations)\n\u001b[1;32m   1129\u001b[0m   _result = _execute.execute(b\"Conv2D\", 1, inputs=_inputs_flat, attrs=_attrs,\n\u001b[0;32m-> 1130\u001b[0;31m                              ctx=_ctx, name=name)\n\u001b[0m\u001b[1;32m   1131\u001b[0m   _execute.record_gradient(\n\u001b[1;32m   1132\u001b[0m       \"Conv2D\", _inputs_flat, _attrs, _result, name)\n",
      "\u001b[0;32m~/anaconda3/envs/tf1/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "        X, labels = next_batch(X_train_4D, y_train, batch_size)\n",
    "\n",
    "        grads = grad(model_CNN, X, labels)\n",
    "        optimizer.apply_gradients(zip(grads, model_CNN.variables))\n",
    "        loss = loss_fn(model_CNN, X, labels)\n",
    "        train_MSE = evaluate(model_CNN, X, labels)\n",
    "\n",
    "        test_MSE =  evaluate(model_CNN, X_test_4D, y_test)\n",
    "\n",
    "        print('Epoch :', epoch+1, ' loss =', loss.numpy(), ' train MSE =', train_MSE.numpy(),' test MSE =', test_MSE.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = model_CNN(X, training = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1192368, shape=(20, 1), dtype=float32, numpy=\n",
       "array([[3.8521044],\n",
       "       [3.02193  ],\n",
       "       [2.018183 ],\n",
       "       [3.5595186],\n",
       "       [3.4086463],\n",
       "       [4.00454  ],\n",
       "       [2.495425 ],\n",
       "       [3.4965823],\n",
       "       [3.3698356],\n",
       "       [3.314314 ],\n",
       "       [2.8059113],\n",
       "       [3.7677057],\n",
       "       [2.7676346],\n",
       "       [3.6273441],\n",
       "       [3.833732 ],\n",
       "       [3.7255607],\n",
       "       [2.5182974],\n",
       "       [3.924812 ],\n",
       "       [4.1107583],\n",
       "       [4.660215 ]], dtype=float32)>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(time_step):\n",
    "    with tf.device('/gpu:0') :\n",
    "        inputs1 = keras.Input(shape = (time_step,10,1))\n",
    "        conv1 = keras.layers.Conv2D(filters = 1, kernel_size = (1,10), padding = 'VALID', activation = tf.nn.elu,kernel_regularizer=keras.regularizers.l2(0.001))(inputs1)\n",
    "    \n",
    "    with tf.device('/gpu:1') :\n",
    "        inputs2 = keras.Input(shape = (time_step,10,1))\n",
    "        conv2 = keras.layers.Conv2D(filters = 1, kernel_size = (1,10), padding = 'VALID', activation = tf.nn.elu,kernel_regularizer=keras.regularizers.l2(0.001))(inputs2)\n",
    "    \n",
    "    with tf.device('/gpu:2') :\n",
    "        inputs3 = keras.Input(shape = (time_step,10,1))\n",
    "        conv3 = keras.layers.Conv2D(filters = 1, kernel_size = (1,10), padding = 'VALID', activation = tf.nn.elu,kernel_regularizer=keras.regularizers.l2(0.001))(inputs3)\n",
    "    \n",
    "    with tf.device('/gpu:3') :\n",
    "        inputs = (inputs1,inputs2,inputs3)\n",
    "        concat_layer = tf.concat([conv1, conv2,conv3], axis =2)\n",
    "        CNN_output = tf.reshape (concat_layer,(-1,concat_layer.shape[1],concat_layer.shape[2]))\n",
    "\n",
    "        #pool3_flat = keras.layers.Flatten()(pool3)\n",
    "        #dense4 = keras.layers.Dense(units= 50,kernel_regularizer=keras.regularizers.l2(0.001), activation = tf.nn.elu)(pool2)\n",
    "        #drop4 = keras.layers.Dropout(rate = 0.9)(dense4)\n",
    "        #CNN_output = keras.layers.Dense(units = 50)(drop4)\n",
    "        \n",
    "        \n",
    "        lstm5 = keras.layers.LSTM(units = 50, activation = tf.nn.relu6,return_sequences=True)(CNN_output)\n",
    "        drop5 = keras.layers.TimeDistributed(keras.layers.Dropout(rate = .5))(lstm5)\n",
    "        lstm6 = keras.layers.LSTM(units = 50, activation = tf.nn.relu6)(drop5)\n",
    "        drop6 = keras.layers.Dropout(rate = .9)(lstm6)\n",
    "\n",
    "        dense8 = keras.layers.Dense(units = 64)(drop6)\n",
    "        drop8 = keras.layers.Dropout(rate = .9)(dense8)\n",
    "        logits = keras.layers.Dense(units = 1)(drop8)\n",
    "\n",
    "    return keras.Model(inputs = inputs, outputs = logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Database connection successful!\n",
      "[success] enterprise_absolute_value_indicator\n",
      "[success] enterprise_consensus_estimate\n",
      "[success] enterprise_dividend\n",
      "[success] enterprise_financial_ratio\n",
      "[success] enterprise_financial_statements\n",
      "[success] enterprise_fixed_result_news\n",
      "[success] enterprise_relative_value_indicator\n",
      "[success] enterprise_stock\n",
      "[success] enterprise_tentative_result\n",
      "[success] market_financial_institution_reception_trend\n",
      "[success] market_fund_investors\n",
      "[success] market_money_around_market\n",
      "[success] market_retirement_pension\n",
      "[success] market_stock_cam\n",
      "[success] market_trends_elw\n",
      "[success] market_trends_investor_bond\n",
      "[success] market_trends_investor_dollar\n",
      "[success] market_trends_investor_eur\n",
      "[success] market_trends_investor_gold\n",
      "[success] market_trends_investor_index\n",
      "[success] market_trends_investor_jpy\n",
      "[success] market_trends_investor_pork\n",
      "[success] market_trends_investor_rates\n",
      "[success] market_trends_investor_stock\n",
      "[success] market_trends_program\n",
      "[success] economy_employ\n",
      "[success] economy_finance\n",
      "[success] economy_income\n",
      "[success] economy_industry\n",
      "[success] economy_oversea_statistics\n",
      "[success] economy_prices\n",
      "[success] economy_trade\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_part = corss_train_test_split(data)\n",
    "part = 'part_1'\n",
    "target_name = '수정주가(원)'\n",
    "timestep = 100\n",
    "future_day = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_labels, test_X, test_labels = create_CNN_input_data(training_part, 'part_1',target_name,timestep,future_day)\n",
    "learning_rate = 0.0001\n",
    "training_epochs = 200\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_18\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_49 (InputLayer)           [(None, 100, 10, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_50 (InputLayer)           [(None, 100, 10, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_51 (InputLayer)           [(None, 100, 10, 1)] 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_48 (Conv2D)              (None, 100, 1, 1)    11          input_49[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_49 (Conv2D)              (None, 100, 1, 1)    11          input_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 100, 1, 1)    11          input_51[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_concat_11 (TensorFl [(None, 100, 3, 1)]  0           conv2d_48[0][0]                  \n",
      "                                                                 conv2d_49[0][0]                  \n",
      "                                                                 conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_Reshape_16 (TensorF [(None, 100, 3)]     0           tf_op_layer_concat_11[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lstm_31 (LSTM)                  (None, 100, 50)      10800       tf_op_layer_Reshape_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_14 (TimeDistri (None, 100, 50)      0           lstm_31[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_32 (LSTM)                  (None, 50)           20200       time_distributed_14[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dropout_52 (Dropout)            (None, 50)           0           lstm_32[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_46 (Dense)                (None, 64)           3264        dropout_52[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_53 (Dropout)            (None, 64)           0           dense_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_47 (Dense)                (None, 1)            65          dropout_53[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 34,362\n",
      "Trainable params: 34,362\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(timestep)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "keras.utils.plot_model(model, 'mini_resnet.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-18-1fa9a84d3d12>, line 1)",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-18-1fa9a84d3d12>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    def training(model, train_X,train_y, test_X, test_y, batch_size, n_epochs )\u001b[0m\n\u001b[0m                                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "def training(model, train_X,train_y, test_X, test_y, batch_size, n_epochs )\n",
    "    for epoch in range(n_epochs):\n",
    "        X, labels = next_batch(train_X, train_y, batch_size)\n",
    "\n",
    "        grads = grad(model, X, labels)\n",
    "        optimizer.apply_gradients(zip(grads, model.variables))\n",
    "        loss = loss_fn(model, X, labels)\n",
    "        train_MSE = evaluate(model, X, labels)\n",
    "\n",
    "        test_MSE =  evaluate(model, test_X, test_y)\n",
    "\n",
    "        print('Epoch :', epoch+1, ' loss =', loss.numpy(), ' train MSE =', train_MSE.numpy(),' test MSE =', test_MSE.numpy())\n",
    "    return  model(test_X, training = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'conv2d_64/kernel:0' shape=(5, 10, 1, 3) dtype=float32, numpy=\n",
       " array([[[[ 0.10874216,  0.16319056, -0.01177126]],\n",
       " \n",
       "         [[ 0.10038777, -0.08204729, -0.07228488]],\n",
       " \n",
       "         [[ 0.00611727,  0.02527888, -0.145368  ]],\n",
       " \n",
       "         [[-0.09344019, -0.07780596, -0.0688545 ]],\n",
       " \n",
       "         [[-0.13971315,  0.09860794,  0.04781915]],\n",
       " \n",
       "         [[ 0.09211581,  0.1034332 , -0.04474393]],\n",
       " \n",
       "         [[ 0.14119068, -0.06710611,  0.02539247]],\n",
       " \n",
       "         [[ 0.12237725,  0.03345631,  0.1156529 ]],\n",
       " \n",
       "         [[-0.11780613,  0.03479483,  0.15549444]],\n",
       " \n",
       "         [[ 0.01143897,  0.10663389, -0.051325  ]]],\n",
       " \n",
       " \n",
       "        [[[ 0.06886984,  0.12440365, -0.10400017]],\n",
       " \n",
       "         [[-0.09398296,  0.09851002, -0.09748083]],\n",
       " \n",
       "         [[-0.03611998, -0.00692982,  0.02656997]],\n",
       " \n",
       "         [[-0.14225107,  0.08287907,  0.05146901]],\n",
       " \n",
       "         [[-0.01668895, -0.13511951,  0.1285592 ]],\n",
       " \n",
       "         [[-0.10419893,  0.1508516 ,  0.04065238]],\n",
       " \n",
       "         [[ 0.16048689,  0.13095966,  0.04164167]],\n",
       " \n",
       "         [[-0.06695955, -0.02376617, -0.04349763]],\n",
       " \n",
       "         [[-0.06680647, -0.15222766,  0.08996227]],\n",
       " \n",
       "         [[ 0.12376539,  0.05606809,  0.13235176]]],\n",
       " \n",
       " \n",
       "        [[[-0.04274213, -0.02397207,  0.12411336]],\n",
       " \n",
       "         [[ 0.03652233,  0.11207911, -0.1725996 ]],\n",
       " \n",
       "         [[ 0.04574724, -0.0496168 ,  0.01132826]],\n",
       " \n",
       "         [[ 0.17121631,  0.10399397,  0.07126421]],\n",
       " \n",
       "         [[ 0.14271282,  0.08945276,  0.08080242]],\n",
       " \n",
       "         [[-0.16390926, -0.05068421,  0.10635661]],\n",
       " \n",
       "         [[ 0.06700937, -0.01569025, -0.06216469]],\n",
       " \n",
       "         [[-0.09237237, -0.16283837,  0.14937876]],\n",
       " \n",
       "         [[-0.16265127,  0.00270757,  0.09609506]],\n",
       " \n",
       "         [[-0.16957246,  0.070838  , -0.07623113]]],\n",
       " \n",
       " \n",
       "        [[[-0.00065074,  0.07792282, -0.06880859]],\n",
       " \n",
       "         [[-0.12769441, -0.0387353 ,  0.09623347]],\n",
       " \n",
       "         [[-0.11539871, -0.05633415,  0.08809287]],\n",
       " \n",
       "         [[ 0.11984451, -0.07147987,  0.12758575]],\n",
       " \n",
       "         [[-0.1368    , -0.16039503, -0.01420131]],\n",
       " \n",
       "         [[-0.16387844, -0.06824669,  0.14628547]],\n",
       " \n",
       "         [[ 0.09532339,  0.1109428 , -0.01360903]],\n",
       " \n",
       "         [[-0.04398882, -0.01414305,  0.16573885]],\n",
       " \n",
       "         [[ 0.06202084,  0.05632072, -0.11421297]],\n",
       " \n",
       "         [[ 0.04658989,  0.09658445, -0.05686631]]],\n",
       " \n",
       " \n",
       "        [[[ 0.01434358, -0.03408669, -0.02167153]],\n",
       " \n",
       "         [[-0.0230163 , -0.03614642, -0.03926059]],\n",
       " \n",
       "         [[-0.15670457, -0.14826602, -0.1543444 ]],\n",
       " \n",
       "         [[ 0.11316186, -0.02814396,  0.0487778 ]],\n",
       " \n",
       "         [[-0.10047692, -0.15123883, -0.14786379]],\n",
       " \n",
       "         [[ 0.13582318,  0.00920385,  0.03638024]],\n",
       " \n",
       "         [[ 0.1379252 , -0.04466353,  0.13582367]],\n",
       " \n",
       "         [[ 0.0923947 ,  0.01658836, -0.06316561]],\n",
       " \n",
       "         [[ 0.13979366,  0.13435863,  0.13923874]],\n",
       " \n",
       "         [[-0.03605029,  0.00736625, -0.0195824 ]]]], dtype=float32)>,\n",
       " <tf.Variable 'conv2d_64/bias:0' shape=(3,) dtype=float32, numpy=array([ 0.0069573 ,  0.00733208, -0.00705102], dtype=float32)>,\n",
       " <tf.Variable 'conv2d_65/kernel:0' shape=(5, 10, 3, 1) dtype=float32, numpy=\n",
       " array([[[[ 0.15602201],\n",
       "          [-0.06360421],\n",
       "          [ 0.11455547]],\n",
       " \n",
       "         [[ 0.08479992],\n",
       "          [ 0.09451722],\n",
       "          [-0.03020433]],\n",
       " \n",
       "         [[ 0.1001123 ],\n",
       "          [-0.15121251],\n",
       "          [ 0.01725198]],\n",
       " \n",
       "         [[ 0.12190773],\n",
       "          [-0.08486114],\n",
       "          [-0.14007635]],\n",
       " \n",
       "         [[ 0.0228416 ],\n",
       "          [ 0.11991309],\n",
       "          [ 0.0192157 ]],\n",
       " \n",
       "         [[-0.05584527],\n",
       "          [-0.09961215],\n",
       "          [ 0.0405466 ]],\n",
       " \n",
       "         [[ 0.0297002 ],\n",
       "          [ 0.06352188],\n",
       "          [-0.07708354]],\n",
       " \n",
       "         [[ 0.01577733],\n",
       "          [-0.14216936],\n",
       "          [-0.12524217]],\n",
       " \n",
       "         [[ 0.13277744],\n",
       "          [-0.16686763],\n",
       "          [-0.03522088]],\n",
       " \n",
       "         [[-0.07521476],\n",
       "          [ 0.14498922],\n",
       "          [ 0.10230032]]],\n",
       " \n",
       " \n",
       "        [[[-0.09157879],\n",
       "          [ 0.1152155 ],\n",
       "          [ 0.04700402]],\n",
       " \n",
       "         [[-0.15033382],\n",
       "          [-0.00055945],\n",
       "          [ 0.14339693]],\n",
       " \n",
       "         [[-0.18050553],\n",
       "          [-0.05692152],\n",
       "          [ 0.12797008]],\n",
       " \n",
       "         [[-0.16075352],\n",
       "          [-0.02635707],\n",
       "          [-0.05244999]],\n",
       " \n",
       "         [[ 0.00613477],\n",
       "          [-0.04380288],\n",
       "          [ 0.05322642]],\n",
       " \n",
       "         [[-0.17925024],\n",
       "          [ 0.15296645],\n",
       "          [ 0.16622543]],\n",
       " \n",
       "         [[ 0.05717981],\n",
       "          [-0.08580057],\n",
       "          [-0.05533504]],\n",
       " \n",
       "         [[-0.15349829],\n",
       "          [ 0.12033261],\n",
       "          [ 0.12388325]],\n",
       " \n",
       "         [[-0.03847955],\n",
       "          [-0.128993  ],\n",
       "          [ 0.01869361]],\n",
       " \n",
       "         [[ 0.04985201],\n",
       "          [-0.13831775],\n",
       "          [-0.06721728]]],\n",
       " \n",
       " \n",
       "        [[[ 0.09080709],\n",
       "          [ 0.13262756],\n",
       "          [-0.01127736]],\n",
       " \n",
       "         [[ 0.15502466],\n",
       "          [ 0.05571036],\n",
       "          [ 0.00364001]],\n",
       " \n",
       "         [[-0.14127083],\n",
       "          [ 0.00816438],\n",
       "          [ 0.13195899]],\n",
       " \n",
       "         [[-0.14555034],\n",
       "          [ 0.0307896 ],\n",
       "          [-0.0159709 ]],\n",
       " \n",
       "         [[-0.11627305],\n",
       "          [-0.14580931],\n",
       "          [-0.07363259]],\n",
       " \n",
       "         [[ 0.12662904],\n",
       "          [-0.0773095 ],\n",
       "          [-0.01352891]],\n",
       " \n",
       "         [[ 0.13230377],\n",
       "          [ 0.00077891],\n",
       "          [-0.11851109]],\n",
       " \n",
       "         [[ 0.0254211 ],\n",
       "          [-0.08603684],\n",
       "          [-0.03847761]],\n",
       " \n",
       "         [[-0.17004813],\n",
       "          [ 0.03923698],\n",
       "          [ 0.0345767 ]],\n",
       " \n",
       "         [[-0.12760812],\n",
       "          [-0.11694658],\n",
       "          [ 0.06580488]]],\n",
       " \n",
       " \n",
       "        [[[ 0.05922476],\n",
       "          [ 0.09627395],\n",
       "          [ 0.1291324 ]],\n",
       " \n",
       "         [[-0.03337899],\n",
       "          [-0.10898556],\n",
       "          [-0.00698451]],\n",
       " \n",
       "         [[ 0.01635771],\n",
       "          [ 0.08209203],\n",
       "          [ 0.01775431]],\n",
       " \n",
       "         [[-0.11815342],\n",
       "          [-0.1431925 ],\n",
       "          [ 0.16075444]],\n",
       " \n",
       "         [[-0.02426168],\n",
       "          [ 0.06503326],\n",
       "          [ 0.01986273]],\n",
       " \n",
       "         [[ 0.04831084],\n",
       "          [ 0.1514551 ],\n",
       "          [ 0.07409211]],\n",
       " \n",
       "         [[-0.1374679 ],\n",
       "          [ 0.02384542],\n",
       "          [-0.0448637 ]],\n",
       " \n",
       "         [[-0.0093683 ],\n",
       "          [ 0.10586937],\n",
       "          [-0.03560923]],\n",
       " \n",
       "         [[-0.04338528],\n",
       "          [-0.07827465],\n",
       "          [ 0.11132081]],\n",
       " \n",
       "         [[ 0.05640529],\n",
       "          [-0.00534123],\n",
       "          [-0.06569748]]],\n",
       " \n",
       " \n",
       "        [[[ 0.04106909],\n",
       "          [ 0.10258007],\n",
       "          [-0.16682075]],\n",
       " \n",
       "         [[-0.02650359],\n",
       "          [-0.13197982],\n",
       "          [ 0.04163155]],\n",
       " \n",
       "         [[ 0.05562862],\n",
       "          [-0.17723624],\n",
       "          [-0.10485486]],\n",
       " \n",
       "         [[-0.09817263],\n",
       "          [-0.15077192],\n",
       "          [-0.04085331]],\n",
       " \n",
       "         [[ 0.15033557],\n",
       "          [-0.06122199],\n",
       "          [-0.10608625]],\n",
       " \n",
       "         [[-0.15649903],\n",
       "          [ 0.16692865],\n",
       "          [-0.12582582]],\n",
       " \n",
       "         [[-0.00757875],\n",
       "          [ 0.15427928],\n",
       "          [ 0.08113265]],\n",
       " \n",
       "         [[-0.06070601],\n",
       "          [ 0.0560356 ],\n",
       "          [-0.00740177]],\n",
       " \n",
       "         [[-0.03235761],\n",
       "          [-0.13291594],\n",
       "          [-0.02601154]],\n",
       " \n",
       "         [[-0.15589233],\n",
       "          [-0.08166669],\n",
       "          [-0.13178901]]]], dtype=float32)>,\n",
       " <tf.Variable 'conv2d_65/bias:0' shape=(1,) dtype=float32, numpy=array([-0.00676264], dtype=float32)>,\n",
       " <tf.Variable 'dense_78/kernel:0' shape=(109, 300) dtype=float32, numpy=\n",
       " array([[-0.11369029,  0.09668558,  0.04885765, ..., -0.05455732,\n",
       "         -0.10661334, -0.06676888],\n",
       "        [ 0.06082867, -0.00618205, -0.11090195, ..., -0.02201607,\n",
       "          0.09704869,  0.09336951],\n",
       "        [ 0.0722431 ,  0.03720487, -0.06253459, ...,  0.08767622,\n",
       "         -0.04464656, -0.05976663],\n",
       "        ...,\n",
       "        [-0.01639947, -0.08625333, -0.03594817, ...,  0.09943564,\n",
       "          0.02224942, -0.11880801],\n",
       "        [-0.01948486, -0.03582389, -0.05436717, ..., -0.05758623,\n",
       "          0.02404716,  0.07597461],\n",
       "        [ 0.01950409, -0.07014617,  0.04108804, ..., -0.05060842,\n",
       "         -0.10137688,  0.10924961]], dtype=float32)>,\n",
       " <tf.Variable 'dense_78/bias:0' shape=(300,) dtype=float32, numpy=\n",
       " array([-5.3403340e-03,  2.5158403e-03, -2.9875612e-04,  3.4148053e-03,\n",
       "         4.6306364e-03, -6.8881419e-03, -5.8416761e-03,  9.4355773e-03,\n",
       "         5.3129136e-03,  3.4168372e-03,  4.6445923e-03,  4.7216630e-03,\n",
       "         2.0609691e-03, -1.1652966e-03,  3.9758682e-03,  8.2330720e-05,\n",
       "        -3.7941376e-03,  5.7823199e-04, -4.3869807e-04, -3.5307456e-03,\n",
       "         1.4922910e-03, -2.6248067e-03,  3.2006444e-03,  5.7627978e-03,\n",
       "        -1.3168628e-03,  4.2305081e-03,  3.0957505e-03,  2.5203885e-03,\n",
       "         3.3030068e-03, -3.5817842e-03,  4.1054669e-03, -9.3592971e-04,\n",
       "        -4.7925562e-03,  5.1744394e-03, -4.8946063e-03,  1.5678655e-03,\n",
       "         4.6802880e-03,  4.5820242e-03, -6.4288783e-03, -3.7228672e-03,\n",
       "         1.1728378e-04, -1.6918641e-03,  4.2801150e-03, -1.8138280e-03,\n",
       "         3.9874883e-03,  2.7160202e-03, -7.4760793e-03, -3.1143201e-03,\n",
       "         2.9507347e-03, -5.6944969e-03, -4.5823222e-03,  4.9034529e-03,\n",
       "         3.0990643e-03,  8.5022184e-04,  9.2282949e-05,  6.5935339e-04,\n",
       "         1.0528782e-03,  7.3891628e-04, -5.8231712e-03, -2.3271372e-03,\n",
       "         1.6526458e-03,  5.3650928e-03,  7.9669862e-04,  3.0007407e-03,\n",
       "         1.9342245e-03,  6.5477437e-04, -5.3144898e-03,  2.7367461e-03,\n",
       "         2.0577738e-03,  1.1566123e-03, -2.6052853e-03,  4.1206959e-03,\n",
       "        -5.9581669e-03, -3.3225634e-03, -7.0674340e-03, -3.4967787e-03,\n",
       "         6.1778086e-03, -3.6254008e-03,  3.5780515e-03,  4.6961112e-03,\n",
       "        -2.1237480e-03, -2.5794609e-04, -2.7952022e-03, -4.4956068e-03,\n",
       "        -1.3129469e-03, -2.1178480e-03,  7.3108138e-03, -3.5978148e-03,\n",
       "        -5.8701029e-05, -2.5638004e-03,  2.5763465e-03,  5.6954017e-03,\n",
       "        -2.5591371e-03, -6.5266001e-03,  1.1352745e-04, -1.8354513e-03,\n",
       "         3.0102578e-04,  3.7261157e-03, -2.2800006e-03, -2.2379311e-03,\n",
       "         2.8072479e-03, -2.7428481e-03, -1.5480947e-03, -4.1993205e-03,\n",
       "        -6.5475603e-04, -2.9964936e-03, -5.8560306e-04, -8.9801976e-04,\n",
       "         6.5991045e-03, -2.7802121e-03, -4.8207794e-04, -1.5996074e-03,\n",
       "         6.5799523e-03,  6.8394179e-03, -6.2650470e-03,  2.8615610e-03,\n",
       "         3.9060989e-03, -1.0718648e-04, -3.9212136e-03, -5.8116443e-03,\n",
       "         3.2362319e-03, -6.0203103e-03, -1.3667014e-03,  2.5730906e-03,\n",
       "         7.7459257e-04, -4.4746920e-03,  2.9681155e-03, -2.9152380e-03,\n",
       "         1.4649931e-04, -5.0774601e-04, -8.7233886e-05,  5.1317649e-04,\n",
       "        -3.1427678e-04,  8.9615968e-04, -3.7267811e-03, -9.4181014e-05,\n",
       "         3.5713650e-03, -3.6272360e-03, -4.1108713e-03, -9.5680275e-04,\n",
       "        -5.9119998e-03, -5.7449867e-04,  4.1520922e-05,  3.2911957e-03,\n",
       "        -1.2753166e-03, -1.5132180e-03, -1.2405240e-03, -1.7215814e-03,\n",
       "        -4.6553193e-03,  6.3019590e-03, -2.7798777e-03,  4.3558590e-03,\n",
       "        -4.4516828e-03, -4.8859981e-03,  5.0397050e-03,  2.1256797e-04,\n",
       "         3.3560782e-03, -3.5809532e-03,  2.2478602e-03, -4.1913092e-03,\n",
       "        -2.9374617e-03,  1.8193586e-03,  4.6781208e-03, -7.3564341e-03,\n",
       "         4.5622485e-03, -4.8855436e-04,  2.9881536e-03,  2.7504403e-03,\n",
       "         1.3318164e-03, -2.8906269e-03,  4.3116757e-03, -1.0058527e-02,\n",
       "        -2.3192260e-03, -1.7841809e-03, -6.9921422e-03,  8.0473360e-04,\n",
       "         1.3523140e-03, -9.8071527e-04, -2.1556111e-03,  3.1431103e-03,\n",
       "        -4.1735135e-03, -5.7336752e-04,  3.2321713e-04, -2.3605453e-03,\n",
       "        -8.7621333e-03, -5.7842471e-03, -1.8663622e-03, -2.1485812e-03,\n",
       "        -1.8478413e-03, -5.0117439e-03,  4.6327766e-03,  6.3148845e-04,\n",
       "         8.4632012e-04, -2.3470114e-03, -2.1510914e-03,  3.3287096e-03,\n",
       "         4.7679502e-03, -6.9531025e-03,  3.7992401e-03, -2.3657039e-03,\n",
       "        -2.4064046e-03, -4.2538405e-03, -1.9743445e-03, -1.6926442e-03,\n",
       "        -2.8386924e-03,  6.7005144e-04, -6.0322722e-03,  5.3359284e-03,\n",
       "        -4.1323751e-03,  5.6865197e-03,  1.9326938e-03,  1.7475614e-03,\n",
       "        -3.0555984e-03,  5.5575995e-03, -3.2085027e-03,  2.3375927e-03,\n",
       "        -2.6445764e-03,  1.9070376e-03,  5.2138004e-03,  3.3368405e-03,\n",
       "        -5.6514144e-04,  1.6878052e-03, -8.5820984e-03, -1.0167281e-03,\n",
       "        -4.6387548e-03, -2.5565363e-03,  2.5174392e-03,  3.0797970e-04,\n",
       "         3.5938961e-03, -6.0755839e-03, -1.3698317e-04, -1.6863224e-04,\n",
       "        -4.2061749e-04, -5.1154802e-03,  1.5780258e-03, -3.8407380e-03,\n",
       "        -2.2387644e-03, -8.8692602e-04,  2.4148685e-04, -3.3875280e-03,\n",
       "        -4.2677773e-03,  3.2527067e-03, -4.3059392e-03, -1.6858042e-03,\n",
       "         1.4919759e-03,  5.5979530e-04, -5.9251180e-03, -2.0792896e-03,\n",
       "        -3.5487560e-03, -3.3193277e-03,  5.3290986e-03,  6.3709808e-03,\n",
       "        -4.5650662e-03, -2.7529888e-03, -1.2161508e-03, -1.3345920e-03,\n",
       "         3.7906021e-03, -3.3094313e-03,  4.8750220e-04, -2.5946905e-03,\n",
       "        -1.6980857e-04, -4.5283779e-04,  2.3463955e-03,  3.0398337e-04,\n",
       "         6.2489521e-04, -1.7415447e-04,  7.0971046e-03,  4.3132063e-03,\n",
       "        -4.3668859e-03,  2.5762874e-03,  1.1075404e-03,  4.8624323e-04,\n",
       "        -1.5840278e-03, -5.9270333e-03, -4.4358782e-03,  1.0673869e-03,\n",
       "        -2.4344551e-03, -2.2014885e-04, -7.1620340e-03, -5.3007579e-03,\n",
       "         3.4581549e-03, -1.4269372e-03, -4.3740873e-03, -1.6053733e-03,\n",
       "         4.1871374e-03, -1.6102507e-03, -2.8238943e-04, -1.7028545e-03,\n",
       "         1.5120556e-03, -3.4118202e-03, -3.8612406e-03,  1.1846684e-03,\n",
       "         1.7180394e-03, -2.6716779e-03, -7.4717174e-03, -2.1656076e-03,\n",
       "        -1.9522210e-03, -2.7261185e-03,  6.0456097e-03, -2.0420253e-03],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_79/kernel:0' shape=(300, 100) dtype=float32, numpy=\n",
       " array([[-0.08332   , -0.00274203, -0.04583507, ...,  0.01956658,\n",
       "          0.03045847,  0.01156801],\n",
       "        [-0.03081661,  0.0774357 ,  0.01248332, ..., -0.05960605,\n",
       "         -0.1131083 ,  0.03529031],\n",
       "        [ 0.06777002,  0.02696139, -0.10551237, ...,  0.0305225 ,\n",
       "          0.04975959, -0.08647445],\n",
       "        ...,\n",
       "        [-0.05024402,  0.02490494,  0.08399634, ..., -0.11754778,\n",
       "          0.01702539, -0.023688  ],\n",
       "        [ 0.08926531,  0.11437576, -0.02738929, ...,  0.088601  ,\n",
       "          0.04241757, -0.0451283 ],\n",
       "        [ 0.10046854,  0.09916691,  0.0219265 , ...,  0.00335981,\n",
       "          0.08909546, -0.07381534]], dtype=float32)>,\n",
       " <tf.Variable 'dense_79/bias:0' shape=(100,) dtype=float32, numpy=\n",
       " array([-5.4176920e-04,  7.5677596e-03,  4.4373181e-03,  3.2711942e-03,\n",
       "         3.6147931e-03,  5.4815416e-03, -6.9929902e-03, -5.7661678e-03,\n",
       "        -2.5676205e-03, -3.5973231e-03,  6.9495435e-03,  9.8828750e-04,\n",
       "         1.2979854e-03,  3.2848048e-03, -4.1010599e-03, -7.1718046e-03,\n",
       "         6.6618007e-03, -2.2692529e-03,  5.2108578e-03,  2.1445958e-03,\n",
       "        -5.0781565e-03, -2.7007672e-03, -5.7558161e-03, -4.8877450e-04,\n",
       "        -2.3209918e-04, -2.6965216e-03,  2.1378109e-03, -1.4595068e-03,\n",
       "         2.5164001e-03,  6.7979544e-03,  8.6050102e-05, -3.8883595e-03,\n",
       "        -8.0803540e-03, -7.8048155e-04, -4.1043275e-04, -5.9515936e-04,\n",
       "         3.1700383e-03,  5.9604449e-03,  4.1355649e-03,  6.4218836e-03,\n",
       "        -5.6644791e-04,  4.4887532e-03,  1.0345830e-05, -2.5334770e-03,\n",
       "        -1.2046888e-03, -1.0564860e-03, -1.6954289e-03,  4.8684664e-03,\n",
       "         4.9849541e-04,  1.9755347e-03,  5.8739440e-04, -2.2277934e-05,\n",
       "         3.9847777e-03,  5.4514310e-03, -2.4824601e-03, -4.4199242e-03,\n",
       "         1.6881262e-03,  3.5202829e-03,  5.5412552e-04,  3.0498072e-03,\n",
       "         2.1701635e-04, -2.0161208e-03,  8.0036803e-04,  5.4399930e-03,\n",
       "         4.7879550e-03,  1.5058575e-04, -5.0521153e-04, -3.6492848e-03,\n",
       "        -3.0289721e-03,  3.6869065e-03, -2.9710522e-03, -2.9086325e-04,\n",
       "        -1.6627166e-03,  3.7408706e-03,  3.0452954e-03,  2.3850326e-03,\n",
       "         2.3658066e-03, -3.6703171e-03,  4.6114726e-03, -5.2749878e-03,\n",
       "         2.2879490e-03,  1.3904222e-03,  2.5769295e-03,  7.0438115e-03,\n",
       "         1.6100014e-03, -6.1040604e-03,  1.6357754e-03, -4.9938046e-04,\n",
       "        -1.4095915e-03,  2.8955057e-04, -3.1430711e-04, -2.4956032e-03,\n",
       "        -7.6454872e-04, -3.1217425e-03,  2.1776869e-03, -2.2351011e-03,\n",
       "        -9.2310971e-04,  3.6276865e-03, -4.3940917e-03,  3.0521669e-03],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'lstm_15/kernel:0' shape=(100, 1200) dtype=float32, numpy=\n",
       " array([[ 0.06131396, -0.04892484, -0.05072425, ...,  0.03729662,\n",
       "         -0.03486688, -0.03590881],\n",
       "        [-0.02361156, -0.02342401, -0.00813904, ..., -0.0508491 ,\n",
       "          0.03887358,  0.0307934 ],\n",
       "        [ 0.02002394, -0.02606707,  0.03964768, ..., -0.02320867,\n",
       "         -0.01811546, -0.05200269],\n",
       "        ...,\n",
       "        [ 0.0217879 , -0.06366696, -0.06752647, ...,  0.01497604,\n",
       "         -0.02716341,  0.04217262],\n",
       "        [ 0.00130289,  0.02988674, -0.03891368, ..., -0.04192091,\n",
       "          0.05777594, -0.00385692],\n",
       "        [ 0.06439349, -0.06985899,  0.05950429, ..., -0.03898653,\n",
       "         -0.06041918,  0.05587468]], dtype=float32)>,\n",
       " <tf.Variable 'lstm_15/recurrent_kernel:0' shape=(300, 1200) dtype=float32, numpy=\n",
       " array([[-0.01582667, -0.0349675 , -0.01832605, ...,  0.01172424,\n",
       "         -0.01999635,  0.00557328],\n",
       "        [-0.02153436, -0.05735456, -0.00060621, ...,  0.0015179 ,\n",
       "         -0.01753079,  0.03792957],\n",
       "        [-0.0314512 ,  0.05462005, -0.03012289, ..., -0.0494507 ,\n",
       "          0.01709838, -0.00779165],\n",
       "        ...,\n",
       "        [-0.03759253, -0.02349712,  0.02204348, ..., -0.03337234,\n",
       "         -0.00656725, -0.00750769],\n",
       "        [ 0.0166539 ,  0.015637  , -0.00053042, ...,  0.00680382,\n",
       "          0.00615939, -0.07266152],\n",
       "        [-0.00521798,  0.00313694,  0.02368053, ..., -0.01986413,\n",
       "         -0.00211134, -0.04410617]], dtype=float32)>,\n",
       " <tf.Variable 'lstm_15/bias:0' shape=(1200,) dtype=float32, numpy=\n",
       " array([ 0.00044451, -0.00275216,  0.0019194 , ..., -0.000723  ,\n",
       "        -0.00471994,  0.00062239], dtype=float32)>,\n",
       " <tf.Variable 'lstm_16/kernel:0' shape=(300, 1200) dtype=float32, numpy=\n",
       " array([[ 0.0350739 ,  0.05695539, -0.02868562, ...,  0.01023138,\n",
       "          0.02221695,  0.01234981],\n",
       "        [ 0.01588608, -0.04068678,  0.05108044, ...,  0.04723339,\n",
       "          0.02881216, -0.0101069 ],\n",
       "        [-0.04062671,  0.00912556, -0.00070743, ...,  0.00266335,\n",
       "         -0.02235084, -0.05115794],\n",
       "        ...,\n",
       "        [-0.01210178,  0.02792141,  0.03480038, ..., -0.03987828,\n",
       "          0.00657939, -0.06059711],\n",
       "        [-0.05764311,  0.03242131,  0.02496341, ...,  0.04905942,\n",
       "         -0.02510453, -0.05915439],\n",
       "        [-0.00499927, -0.02115143, -0.06376377, ...,  0.03575211,\n",
       "          0.02157556,  0.02785532]], dtype=float32)>,\n",
       " <tf.Variable 'lstm_16/recurrent_kernel:0' shape=(300, 1200) dtype=float32, numpy=\n",
       " array([[ 0.02761403, -0.00876123, -0.00826856, ...,  0.03010221,\n",
       "         -0.01592024, -0.03411111],\n",
       "        [ 0.02410844, -0.00017091,  0.02731592, ..., -0.02529046,\n",
       "         -0.04067018,  0.00043165],\n",
       "        [-0.03754658, -0.01740064,  0.03376217, ...,  0.02027432,\n",
       "         -0.01590033,  0.02850279],\n",
       "        ...,\n",
       "        [-0.00632418, -0.00512423,  0.00692829, ...,  0.01601823,\n",
       "         -0.01544869, -0.04581654],\n",
       "        [-0.01611512,  0.00655094, -0.00566207, ..., -0.03497181,\n",
       "         -0.01326591, -0.03117565],\n",
       "        [ 0.05318772, -0.03274878,  0.01437162, ..., -0.00750109,\n",
       "         -0.02765155,  0.01053311]], dtype=float32)>,\n",
       " <tf.Variable 'lstm_16/bias:0' shape=(1200,) dtype=float32, numpy=\n",
       " array([-0.00508075, -0.00145134, -0.00057273, ..., -0.00526802,\n",
       "        -0.00427781, -0.00762443], dtype=float32)>,\n",
       " <tf.Variable 'lstm_17/kernel:0' shape=(300, 1200) dtype=float32, numpy=\n",
       " array([[-0.06204979,  0.04080816, -0.04659289, ..., -0.03056182,\n",
       "          0.03846778,  0.02835716],\n",
       "        [ 0.0048993 ,  0.01325918, -0.02701606, ...,  0.02158322,\n",
       "          0.0349144 , -0.03876998],\n",
       "        [ 0.02411485,  0.04210512, -0.02073837, ..., -0.05871545,\n",
       "          0.04779734,  0.00176604],\n",
       "        ...,\n",
       "        [-0.06373822,  0.04636883, -0.02723513, ..., -0.04537778,\n",
       "          0.00190617,  0.02326358],\n",
       "        [ 0.01110925,  0.04254657,  0.04928115, ...,  0.06738516,\n",
       "         -0.03349778,  0.0285424 ],\n",
       "        [ 0.05106981,  0.03830792, -0.04435645, ...,  0.0562258 ,\n",
       "          0.0153852 , -0.04671276]], dtype=float32)>,\n",
       " <tf.Variable 'lstm_17/recurrent_kernel:0' shape=(300, 1200) dtype=float32, numpy=\n",
       " array([[-1.1107761e-04,  1.7445171e-02,  2.1987349e-02, ...,\n",
       "         -9.6357167e-03,  5.1211338e-03, -5.2058883e-03],\n",
       "        [-2.3778748e-02, -3.3094827e-02, -3.3232514e-02, ...,\n",
       "          3.4334514e-02, -1.5391103e-02,  1.4290698e-02],\n",
       "        [-7.8208484e-03,  1.3045156e-02, -2.6228705e-02, ...,\n",
       "          1.4753154e-05,  2.7120842e-03,  3.2587573e-02],\n",
       "        ...,\n",
       "        [-5.8047589e-02, -6.6439599e-02, -4.5527451e-02, ...,\n",
       "          2.0603755e-02, -3.7884764e-02,  2.1654185e-02],\n",
       "        [ 2.3734517e-02, -3.9338659e-02,  5.8629052e-03, ...,\n",
       "         -8.4706405e-03,  4.0296823e-02, -3.4341112e-02],\n",
       "        [ 7.7512413e-03,  2.9244158e-02,  1.0012708e-02, ...,\n",
       "          3.4853953e-03,  2.0879256e-02,  6.9966703e-03]], dtype=float32)>,\n",
       " <tf.Variable 'lstm_17/bias:0' shape=(1200,) dtype=float32, numpy=\n",
       " array([-2.4944339e-03, -4.5827050e-03, -4.1033123e-03, ...,\n",
       "        -5.9591333e-04, -5.4959785e-03,  7.0716196e-05], dtype=float32)>,\n",
       " <tf.Variable 'dense_80/kernel:0' shape=(300, 256) dtype=float32, numpy=\n",
       " array([[ 0.00325502, -0.06754365, -0.02079177, ..., -0.06313725,\n",
       "         -0.03127466,  0.05549999],\n",
       "        [-0.09576428, -0.0004118 , -0.00169567, ..., -0.00814322,\n",
       "          0.07791837,  0.04290858],\n",
       "        [-0.01327816, -0.04899957,  0.03292582, ...,  0.05327795,\n",
       "          0.0255449 , -0.04555678],\n",
       "        ...,\n",
       "        [-0.0802132 ,  0.01161109, -0.05752229, ...,  0.00659861,\n",
       "         -0.09746952, -0.10768186],\n",
       "        [-0.05755637,  0.08629041,  0.01845711, ...,  0.07195228,\n",
       "         -0.05207199,  0.06709842],\n",
       "        [ 0.03880353,  0.03514598,  0.04220856, ...,  0.06609228,\n",
       "          0.08253801,  0.0262564 ]], dtype=float32)>,\n",
       " <tf.Variable 'dense_80/bias:0' shape=(256,) dtype=float32, numpy=\n",
       " array([-6.6997730e-03, -4.0794220e-03, -8.4241736e-04,  1.1311907e-03,\n",
       "         4.0539685e-03, -1.8387252e-03,  2.3774074e-03, -1.4595254e-04,\n",
       "        -4.9864380e-03,  4.5756516e-03, -8.1443181e-03, -2.3320401e-03,\n",
       "         2.0391084e-03,  3.9314199e-03,  1.2749922e-03,  7.1915812e-03,\n",
       "         3.6507987e-03, -8.9405607e-03, -3.6207810e-03, -1.2572330e-03,\n",
       "        -3.7407938e-03, -7.3800227e-03, -1.4643803e-03,  4.2357906e-03,\n",
       "        -2.4278117e-03, -1.6290132e-03, -1.5428849e-03, -3.8607360e-04,\n",
       "         3.2572723e-03,  6.9731707e-03, -3.6650237e-03, -4.5474567e-03,\n",
       "         2.0048951e-03,  7.0235590e-03,  5.5886991e-03,  1.1794716e-03,\n",
       "         3.5236652e-03,  2.9866421e-04, -4.9006199e-03,  2.6815571e-03,\n",
       "         8.0346325e-03,  2.6992436e-03, -3.5115774e-03, -6.1696963e-03,\n",
       "         4.8406534e-03, -3.7032042e-03,  4.2360257e-03, -3.7282342e-03,\n",
       "         3.1562545e-03,  5.0667478e-03,  7.2191178e-04, -9.1144189e-05,\n",
       "        -1.8269710e-03,  2.2168928e-03,  6.1337701e-03,  6.5959161e-03,\n",
       "        -4.7482257e-03,  1.6488850e-03,  2.5190026e-03,  1.4216861e-03,\n",
       "        -3.6645397e-03, -2.6908352e-03, -6.2123416e-03, -9.1668603e-04,\n",
       "        -9.4791007e-04, -7.3609315e-04,  3.1271891e-03,  7.6981401e-03,\n",
       "         2.2416264e-03, -3.6377027e-03, -1.3555251e-03, -5.9862793e-03,\n",
       "        -1.0133829e-03,  4.1758157e-03, -6.1599712e-04, -8.1858654e-03,\n",
       "         2.2630403e-03, -2.6389728e-03,  6.2002311e-03, -2.2479042e-03,\n",
       "        -4.5873667e-03,  4.3746619e-03, -3.7259366e-03,  9.9740934e-04,\n",
       "         2.6455473e-03, -4.3988493e-04, -5.7840198e-03,  2.3152214e-03,\n",
       "        -6.2144734e-03,  2.7912583e-03, -1.1330955e-03, -4.1161859e-03,\n",
       "        -1.1871454e-03,  7.4939490e-03,  2.1077257e-03, -1.7082007e-03,\n",
       "        -9.1051182e-04,  6.5462021e-03,  2.6218612e-03,  9.4773981e-04,\n",
       "         6.6055739e-03, -3.7549045e-03, -4.9374620e-03, -1.3176121e-03,\n",
       "        -1.3991576e-03,  5.3916522e-03,  4.2688129e-03,  8.7738235e-04,\n",
       "        -1.0987270e-03, -1.3678430e-03,  7.6520396e-03,  3.9240778e-03,\n",
       "        -1.0033331e-03, -4.6994174e-03,  9.8691462e-04,  2.4943149e-03,\n",
       "        -6.4127608e-03, -6.5146792e-03,  2.7649431e-04,  7.3843263e-03,\n",
       "         9.1489998e-04,  4.0386021e-03, -3.0940585e-03,  3.1480980e-03,\n",
       "        -8.3604492e-03,  4.3256842e-03, -5.8861943e-03, -1.9746127e-03,\n",
       "         8.4015802e-03,  7.2284485e-03, -2.9667339e-04, -2.5236120e-03,\n",
       "        -6.5115443e-03, -4.6291468e-03, -5.9837830e-04,  8.2269581e-03,\n",
       "         8.5186004e-04,  6.9758215e-04, -6.0837232e-03, -4.4971053e-03,\n",
       "         2.9075365e-03,  1.0007659e-04, -6.6937110e-04, -1.8914646e-03,\n",
       "         2.1285575e-03,  2.6496947e-03, -5.3417427e-03,  4.5095156e-03,\n",
       "        -1.1611949e-03,  6.7629763e-03, -7.2375042e-03,  8.9030387e-03,\n",
       "         7.0881243e-03, -5.2154157e-03,  5.4573077e-03, -7.4967719e-03,\n",
       "        -5.2263392e-03,  1.6441038e-03, -3.9188606e-03, -6.7150593e-03,\n",
       "         6.5301434e-04, -5.2568241e-05, -8.8807316e-03, -3.6756364e-03,\n",
       "        -3.1856506e-04, -4.5853881e-03, -3.7488909e-03, -2.0345575e-03,\n",
       "         1.1379295e-03, -2.8923021e-03, -4.4129863e-03, -8.4037660e-03,\n",
       "         4.6733627e-03,  7.5705871e-03, -3.9138766e-03,  1.4474767e-04,\n",
       "        -5.7352255e-03,  3.0287975e-03, -7.0566509e-04, -4.3577049e-03,\n",
       "         5.2846693e-03,  2.8484469e-04,  1.0644060e-03, -2.9450397e-03,\n",
       "        -2.4460913e-03,  5.6272955e-03, -2.3397345e-03,  4.5201336e-03,\n",
       "         4.9121291e-03, -9.1810189e-03,  2.2340310e-03,  2.0249786e-03,\n",
       "        -8.3594385e-04,  3.3188350e-03,  1.0038993e-03, -4.7920677e-03,\n",
       "        -4.7202073e-03, -3.4151780e-03,  5.0861980e-03,  2.1560382e-04,\n",
       "        -2.6177636e-03, -4.5387424e-08, -2.7941636e-04,  5.8515156e-03,\n",
       "         9.5529845e-03,  2.3764500e-03,  4.8783123e-03,  1.2857569e-03,\n",
       "         4.7106836e-03,  4.0911781e-03, -3.8671943e-03,  8.5581752e-04,\n",
       "        -3.0142101e-04, -1.3357816e-04, -4.1753701e-03,  3.9913943e-03,\n",
       "        -5.4629897e-03, -8.2800910e-04,  1.9170074e-03,  3.4795993e-03,\n",
       "         5.6412881e-03, -4.3216292e-03, -9.1888718e-03,  2.1877813e-03,\n",
       "         3.6796874e-03, -4.3905298e-03,  3.7832754e-03,  5.5619758e-03,\n",
       "         8.8712072e-04, -5.2463296e-03, -3.4684916e-03,  7.2426149e-03,\n",
       "         2.4604890e-03, -1.1040346e-03,  6.0669088e-05,  5.7923445e-03,\n",
       "        -4.5183613e-03,  2.2657800e-03, -1.5872295e-03, -9.8585908e-04,\n",
       "         6.9935592e-03,  2.1168746e-03,  5.5143470e-03,  4.0079178e-03,\n",
       "         6.1878660e-03, -8.8942843e-03,  3.6566912e-03,  5.9749293e-03,\n",
       "        -1.2937444e-03,  8.1360457e-04, -2.0666055e-03, -3.7808784e-03,\n",
       "        -2.5981916e-03,  7.2218897e-03, -2.7465662e-03,  5.7319459e-03],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'dense_81/kernel:0' shape=(256, 1) dtype=float32, numpy=\n",
       " array([[ 0.05012912],\n",
       "        [ 0.06238968],\n",
       "        [ 0.03723707],\n",
       "        [-0.01007659],\n",
       "        [ 0.09111905],\n",
       "        [ 0.03985287],\n",
       "        [-0.02777502],\n",
       "        [ 0.14711304],\n",
       "        [ 0.00130547],\n",
       "        [-0.00770664],\n",
       "        [ 0.15032804],\n",
       "        [ 0.07389769],\n",
       "        [-0.05485299],\n",
       "        [ 0.11850718],\n",
       "        [-0.06049052],\n",
       "        [-0.01599673],\n",
       "        [-0.12080693],\n",
       "        [ 0.10199889],\n",
       "        [ 0.06335077],\n",
       "        [-0.05989182],\n",
       "        [ 0.02859224],\n",
       "        [ 0.04017036],\n",
       "        [ 0.05088137],\n",
       "        [-0.11010464],\n",
       "        [ 0.09338953],\n",
       "        [ 0.06889474],\n",
       "        [ 0.05649248],\n",
       "        [ 0.00562947],\n",
       "        [-0.10462456],\n",
       "        [-0.08113989],\n",
       "        [ 0.07403462],\n",
       "        [ 0.01136925],\n",
       "        [-0.1357288 ],\n",
       "        [-0.11600955],\n",
       "        [-0.10341059],\n",
       "        [ 0.01911265],\n",
       "        [ 0.13886544],\n",
       "        [ 0.06042143],\n",
       "        [ 0.07847118],\n",
       "        [-0.0766151 ],\n",
       "        [-0.11543573],\n",
       "        [-0.0069811 ],\n",
       "        [-0.15608524],\n",
       "        [ 0.14253719],\n",
       "        [ 0.1369064 ],\n",
       "        [-0.07855428],\n",
       "        [-0.06231845],\n",
       "        [ 0.13833982],\n",
       "        [ 0.11419612],\n",
       "        [-0.14690019],\n",
       "        [-0.07700657],\n",
       "        [-0.00141964],\n",
       "        [-0.01536784],\n",
       "        [-0.05427501],\n",
       "        [-0.02391074],\n",
       "        [-0.15102535],\n",
       "        [ 0.02818078],\n",
       "        [ 0.13087158],\n",
       "        [ 0.11573007],\n",
       "        [-0.09101944],\n",
       "        [ 0.08199129],\n",
       "        [-0.11255275],\n",
       "        [ 0.06693003],\n",
       "        [ 0.03142461],\n",
       "        [ 0.00641004],\n",
       "        [ 0.06733277],\n",
       "        [-0.07020775],\n",
       "        [-0.04863418],\n",
       "        [-0.02572386],\n",
       "        [-0.11481132],\n",
       "        [-0.05959102],\n",
       "        [ 0.01475571],\n",
       "        [-0.13462654],\n",
       "        [-0.08786488],\n",
       "        [ 0.07207166],\n",
       "        [ 0.02298461],\n",
       "        [-0.02548206],\n",
       "        [ 0.12088403],\n",
       "        [-0.10083563],\n",
       "        [ 0.11195512],\n",
       "        [ 0.07207758],\n",
       "        [ 0.05382213],\n",
       "        [ 0.12355626],\n",
       "        [ 0.11147393],\n",
       "        [-0.02657356],\n",
       "        [-0.09220713],\n",
       "        [ 0.02946143],\n",
       "        [ 0.0900491 ],\n",
       "        [ 0.03043675],\n",
       "        [-0.13233775],\n",
       "        [-0.1463646 ],\n",
       "        [ 0.01288606],\n",
       "        [ 0.00856606],\n",
       "        [-0.10213938],\n",
       "        [-0.10270618],\n",
       "        [-0.13998857],\n",
       "        [-0.11253233],\n",
       "        [-0.03318959],\n",
       "        [-0.00895164],\n",
       "        [ 0.02655291],\n",
       "        [-0.12418228],\n",
       "        [ 0.09409165],\n",
       "        [ 0.06539543],\n",
       "        [-0.01277358],\n",
       "        [ 0.02855011],\n",
       "        [-0.09275354],\n",
       "        [-0.06195923],\n",
       "        [-0.03139959],\n",
       "        [ 0.04902547],\n",
       "        [-0.13421415],\n",
       "        [ 0.00427881],\n",
       "        [-0.00955706],\n",
       "        [ 0.08399567],\n",
       "        [ 0.06112199],\n",
       "        [-0.00843736],\n",
       "        [-0.04532346],\n",
       "        [-0.09156352],\n",
       "        [ 0.05621713],\n",
       "        [ 0.06425849],\n",
       "        [-0.06932805],\n",
       "        [ 0.09801386],\n",
       "        [-0.10212647],\n",
       "        [-0.11537037],\n",
       "        [-0.13465215],\n",
       "        [ 0.10978225],\n",
       "        [-0.01840664],\n",
       "        [ 0.12348303],\n",
       "        [ 0.07531616],\n",
       "        [-0.0223802 ],\n",
       "        [-0.02590583],\n",
       "        [ 0.14530213],\n",
       "        [ 0.06466285],\n",
       "        [ 0.03340738],\n",
       "        [ 0.05433727],\n",
       "        [-0.02930354],\n",
       "        [-0.00239622],\n",
       "        [-0.13118757],\n",
       "        [ 0.1497599 ],\n",
       "        [-0.12853792],\n",
       "        [ 0.12069943],\n",
       "        [-0.08467463],\n",
       "        [ 0.03424928],\n",
       "        [ 0.0884872 ],\n",
       "        [-0.13031033],\n",
       "        [ 0.11728474],\n",
       "        [-0.10445794],\n",
       "        [ 0.12221637],\n",
       "        [-0.1511448 ],\n",
       "        [ 0.05813458],\n",
       "        [-0.11125059],\n",
       "        [ 0.04556435],\n",
       "        [-0.1371488 ],\n",
       "        [-0.1179154 ],\n",
       "        [ 0.09652591],\n",
       "        [-0.08527673],\n",
       "        [ 0.02178123],\n",
       "        [ 0.10904393],\n",
       "        [-0.07587756],\n",
       "        [ 0.11168813],\n",
       "        [ 0.097236  ],\n",
       "        [-0.03263035],\n",
       "        [-0.0456172 ],\n",
       "        [ 0.14404063],\n",
       "        [ 0.09384011],\n",
       "        [ 0.041056  ],\n",
       "        [ 0.03244138],\n",
       "        [-0.0099097 ],\n",
       "        [-0.07011423],\n",
       "        [-0.05193676],\n",
       "        [ 0.04330807],\n",
       "        [-0.09006493],\n",
       "        [ 0.11221043],\n",
       "        [-0.14709572],\n",
       "        [-0.08422363],\n",
       "        [ 0.05246869],\n",
       "        [ 0.09887102],\n",
       "        [ 0.1312523 ],\n",
       "        [ 0.15187743],\n",
       "        [-0.08158397],\n",
       "        [ 0.10997809],\n",
       "        [-0.06506632],\n",
       "        [ 0.01062503],\n",
       "        [-0.00312309],\n",
       "        [ 0.01521923],\n",
       "        [ 0.02808186],\n",
       "        [-0.12305619],\n",
       "        [ 0.05080454],\n",
       "        [-0.0525347 ],\n",
       "        [-0.07696772],\n",
       "        [ 0.06969365],\n",
       "        [-0.06460976],\n",
       "        [ 0.03343654],\n",
       "        [-0.12299376],\n",
       "        [-0.12336051],\n",
       "        [-0.10232439],\n",
       "        [ 0.06197391],\n",
       "        [ 0.00400529],\n",
       "        [ 0.0785986 ],\n",
       "        [-0.10010043],\n",
       "        [-0.11355484],\n",
       "        [-0.06249733],\n",
       "        [ 0.14760853],\n",
       "        [-0.11126363],\n",
       "        [-0.14394712],\n",
       "        [-0.04984435],\n",
       "        [-0.02482778],\n",
       "        [-0.03536336],\n",
       "        [-0.04729782],\n",
       "        [ 0.00896453],\n",
       "        [-0.14964707],\n",
       "        [-0.12567456],\n",
       "        [ 0.06790928],\n",
       "        [ 0.09806196],\n",
       "        [ 0.06040185],\n",
       "        [ 0.12200103],\n",
       "        [-0.08941233],\n",
       "        [ 0.09989319],\n",
       "        [-0.09829042],\n",
       "        [-0.04353457],\n",
       "        [-0.03608001],\n",
       "        [-0.06439296],\n",
       "        [ 0.05487449],\n",
       "        [ 0.10848023],\n",
       "        [-0.12719378],\n",
       "        [ 0.09758043],\n",
       "        [ 0.10679559],\n",
       "        [-0.04145042],\n",
       "        [-0.14857842],\n",
       "        [ 0.0647012 ],\n",
       "        [-0.06756116],\n",
       "        [ 0.0763595 ],\n",
       "        [-0.04250454],\n",
       "        [-0.14130624],\n",
       "        [-0.02822838],\n",
       "        [ 0.09362109],\n",
       "        [-0.09772591],\n",
       "        [ 0.01668534],\n",
       "        [-0.09794589],\n",
       "        [-0.04010106],\n",
       "        [ 0.01668409],\n",
       "        [-0.01552608],\n",
       "        [-0.05675554],\n",
       "        [-0.0743134 ],\n",
       "        [-0.05356597],\n",
       "        [-0.1039491 ],\n",
       "        [ 0.03566836],\n",
       "        [ 0.12427287],\n",
       "        [ 0.04077223],\n",
       "        [-0.01742443],\n",
       "        [-0.07888228],\n",
       "        [ 0.05032634],\n",
       "        [ 0.12979673],\n",
       "        [ 0.14242813],\n",
       "        [-0.11356144],\n",
       "        [-0.0413962 ],\n",
       "        [-0.057929  ]], dtype=float32)>,\n",
       " <tf.Variable 'dense_81/bias:0' shape=(1,) dtype=float32, numpy=array([-0.00977498], dtype=float32)>]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model(test_X, training = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(time_step):\n",
    "    with tf.device('/gpu:1') :\n",
    "        inputs = keras.Input(shape = (time_step,3917,1))\n",
    "        conv1 = keras.layers.Conv2D(filters = 5, kernel_size = (5,10), strides=(1, 5),padding = 'SAME', activation = tf.nn.relu,kernel_regularizer=keras.regularizers.l2(0.001))(inputs)\n",
    "        pool1 = keras.layers.MaxPool2D(pool_size = (1,3),padding = 'SAME')(conv1)\n",
    "        conv2 = keras.layers.Conv2D(filters = 5, kernel_size = (5,10),strides=(1, 5), padding = 'SAME', activation = tf.nn.relu,kernel_regularizer=keras.regularizers.l2(0.001))(pool1)\n",
    "        pool2 = keras.layers.MaxPool2D(pool_size = (1,3),padding = 'same')(conv2)\n",
    "        #pool3 = tf.reshape (pool3,(-1,200,15))\n",
    "        pool2_flat = keras.layers.Flatten()(pool2)\n",
    "        dense3 = keras.layers.Dense(units= 300,kernel_regularizer=keras.regularizers.l2(0.001), activation = tf.nn.tanh)(pool2_flat)\n",
    "        drop3 = keras.layers.Dropout(rate = 0.2)(dense3)\n",
    "        logits = keras.layers.Dense(units = 1)(drop3)\n",
    "    return keras.Model(inputs = inputs, outputs = logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         [(None, 100, 3917, 1)]    0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 100, 784, 5)       255       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 100, 262, 5)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 100, 53, 5)        1255      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 100, 18, 5)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 9000)              0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 300)               2700300   \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 301       \n",
      "=================================================================\n",
      "Total params: 2,702,111\n",
      "Trainable params: 2,702,111\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(100)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Database connection successful!\n",
      "[success] enterprise_absolute_value_indicator\n",
      "[success] enterprise_consensus_estimate\n",
      "[success] enterprise_dividend\n",
      "[success] enterprise_financial_ratio\n",
      "[success] enterprise_financial_statements\n",
      "[success] enterprise_fixed_result_news\n",
      "[success] enterprise_relative_value_indicator\n",
      "[success] enterprise_stock\n",
      "[success] enterprise_tentative_result\n",
      "[success] market_financial_institution_reception_trend\n",
      "[success] market_fund_investors\n",
      "[success] market_money_around_market\n",
      "[success] market_retirement_pension\n",
      "[success] market_stock_cam\n",
      "[success] market_trends_elw\n",
      "[success] market_trends_investor_bond\n",
      "[success] market_trends_investor_dollar\n",
      "[success] market_trends_investor_eur\n",
      "[success] market_trends_investor_gold\n",
      "[success] market_trends_investor_index\n",
      "[success] market_trends_investor_jpy\n",
      "[success] market_trends_investor_pork\n",
      "[success] market_trends_investor_rates\n",
      "[success] market_trends_investor_stock\n",
      "[success] market_trends_program\n",
      "[success] economy_employ\n",
      "[success] economy_finance\n",
      "[success] economy_income\n",
      "[success] economy_industry\n",
      "[success] economy_oversea_statistics\n",
      "[success] economy_prices\n",
      "[success] economy_trade\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_labels, test_X, test_labels = create_CNN_input_data(training_part, 'part_1',target_name,timestep,future_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, train_labels, test_X, test_labels = create_CNN_input_data(training_part, 'part_1',target_name,timestep,future_day)\n",
    "learning_rate = 0.001\n",
    "training_epochs = 200\n",
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1",
   "language": "python",
   "name": "tf1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
