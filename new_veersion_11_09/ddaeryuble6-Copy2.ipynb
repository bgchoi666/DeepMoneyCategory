{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 Bimghi Choi. All Rights Reserved.\n",
    "# 예측 + 투자전략 시스템\n",
    "\n",
    "# _*_ coding: utf-8 _*_\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import os\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]='0, 1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import preprocess as prepro\n",
    "import models\n",
    "import learn\n",
    "from learn import GenerateResult\n",
    "import math\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # 텐서플로가 첫 번째 GPU에 1GB 메모리만 할당하도록 제한\n",
    "  try:\n",
    "#    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')    \n",
    "    tf.config.experimental.set_visible_devices(gpus[1], 'GPU')    \n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[1],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n",
    "  except RuntimeError as e:\n",
    "    # 프로그램 시작시에 가장 장치가 설정되어야만 합니다\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '~/Data/kospi200f_809_0515.csv'\n",
    "item_name = 'kospi200f_reinfo_809'\n",
    "train_start = '2000-01-31'\n",
    "train_end = '2017-01-02'\n",
    "test_start = '2017-01-02'\n",
    "test_end = '2020-05-15'\n",
    "\n",
    "remove_columns = ['date', '종가']\n",
    "target_column = '종가'\n",
    "input_columns = []\n",
    "target_type = 'rate'\n",
    "\n",
    "model_name = 'ddaeryuble6'\n",
    "channel = False\n",
    "\n",
    "trans_day = 1\n",
    "\n",
    "target_alpha = 100\n",
    "future_day = 1\n",
    "train_end_back = -1\n",
    "n_timestep = 100\n",
    "time_interval = 1\n",
    "input_size = 809\n",
    "n_unit = 500\n",
    "batch_size = 20\n",
    "learning_rate = 0.0005\n",
    "epochs = 1000\n",
    "\n",
    "alpha = 0.05\n",
    "beta = 0.01\n",
    "gamma = 0.1\n",
    "\n",
    "comment = \"예측 모델과 best share decision 모델(target = 현재까지의 수익률 합 + 다음 step의 최대 수익률*gamma) 의 reinforcing interaction, random time_interval, loss=mse\"\n",
    "\n",
    "checkpoint_path = model_name + \"/pred\"+str(future_day)+\"_trans\"+str(trans_day)+\".ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = util.read_datafile(file_name)\n",
    "df = dataframe.copy()\n",
    "df['close'] = df[target_column] # 종가 column 추가, 기존 종가는 rate으로 변환 예정\n",
    "a = []\n",
    "b = []\n",
    "#df = prepro.target_conversion(df, target_column, future_day, type=target_type)\n",
    "for i in range(len(df[target_column]) -future_day):\n",
    "    df.loc[i, target_column] = ((df.loc[i + future_day, target_column] - df.loc[i, target_column]) \n",
    "                                / df.loc[i, target_column]) * target_alpha     \n",
    "    df.loc[i, '시가'] = df.loc[i + future_day + train_end_back + 1, '시가']\n",
    "    a.append(max(df.loc[i  + future_day + train_end_back + 1:i + future_day + 1, '고가']))\n",
    "    b.append(min(df.loc[i  + future_day + train_end_back + 1:i + future_day + 1, '저가']))\n",
    "for i in range(len(df['종가']) - future_day):\n",
    "    df.loc[i, '고가'] = a[i]\n",
    "    df.loc[i, '저가'] = b[i]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>시가지수(포인트)</th>\n",
       "      <th>고가지수(포인트)</th>\n",
       "      <th>저가지수(포인트)</th>\n",
       "      <th>종가지수(포인트)</th>\n",
       "      <th>수익률(%)</th>\n",
       "      <th>수익률 (1주)(%)</th>\n",
       "      <th>수익률 (1개월)(%)</th>\n",
       "      <th>수익률 (3개월)(%)</th>\n",
       "      <th>수익률 (6개월)(%)</th>\n",
       "      <th>...</th>\n",
       "      <th>주요상품선물_금(선물)($/ounce)</th>\n",
       "      <th>주요상품선물_은(선물)($/ounce)</th>\n",
       "      <th>주요상품선물_알루미늄(선물)($/ton)</th>\n",
       "      <th>주요상품선물_옥수수(최근월물)(￠/bu)</th>\n",
       "      <th>대두박(￠/bu)</th>\n",
       "      <th>종가</th>\n",
       "      <th>시가</th>\n",
       "      <th>고가</th>\n",
       "      <th>저가</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-31</td>\n",
       "      <td>-0.218225</td>\n",
       "      <td>-0.493106</td>\n",
       "      <td>-0.400439</td>\n",
       "      <td>-0.223565</td>\n",
       "      <td>0.212872</td>\n",
       "      <td>1.352924</td>\n",
       "      <td>-0.851790</td>\n",
       "      <td>-0.382658</td>\n",
       "      <td>-0.162615</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.853010</td>\n",
       "      <td>0.974513</td>\n",
       "      <td>0.827681</td>\n",
       "      <td>0.373581</td>\n",
       "      <td>0.614542</td>\n",
       "      <td>-3.553512</td>\n",
       "      <td>120.85</td>\n",
       "      <td>121.70</td>\n",
       "      <td>115.35</td>\n",
       "      <td>119.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-02-01</td>\n",
       "      <td>-0.213200</td>\n",
       "      <td>-0.230417</td>\n",
       "      <td>-0.373954</td>\n",
       "      <td>-0.595373</td>\n",
       "      <td>-0.407635</td>\n",
       "      <td>1.808646</td>\n",
       "      <td>-1.074203</td>\n",
       "      <td>-1.707405</td>\n",
       "      <td>-0.490731</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039689</td>\n",
       "      <td>0.193571</td>\n",
       "      <td>1.002058</td>\n",
       "      <td>0.594935</td>\n",
       "      <td>0.706918</td>\n",
       "      <td>2.687473</td>\n",
       "      <td>116.55</td>\n",
       "      <td>120.25</td>\n",
       "      <td>115.95</td>\n",
       "      <td>115.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-02-02</td>\n",
       "      <td>-0.208520</td>\n",
       "      <td>-0.448886</td>\n",
       "      <td>-0.367787</td>\n",
       "      <td>-0.200398</td>\n",
       "      <td>0.696677</td>\n",
       "      <td>2.107257</td>\n",
       "      <td>-0.769928</td>\n",
       "      <td>-1.558400</td>\n",
       "      <td>0.074389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.800355</td>\n",
       "      <td>0.031266</td>\n",
       "      <td>0.069626</td>\n",
       "      <td>0.430159</td>\n",
       "      <td>0.725768</td>\n",
       "      <td>0.675390</td>\n",
       "      <td>119.35</td>\n",
       "      <td>123.45</td>\n",
       "      <td>116.80</td>\n",
       "      <td>118.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-02-03</td>\n",
       "      <td>-0.204125</td>\n",
       "      <td>-0.216603</td>\n",
       "      <td>-0.062403</td>\n",
       "      <td>-0.021445</td>\n",
       "      <td>0.366314</td>\n",
       "      <td>1.488978</td>\n",
       "      <td>-0.625155</td>\n",
       "      <td>-1.268930</td>\n",
       "      <td>-0.079967</td>\n",
       "      <td>...</td>\n",
       "      <td>1.515992</td>\n",
       "      <td>0.623370</td>\n",
       "      <td>0.378121</td>\n",
       "      <td>0.084769</td>\n",
       "      <td>0.352875</td>\n",
       "      <td>2.599581</td>\n",
       "      <td>120.40</td>\n",
       "      <td>123.60</td>\n",
       "      <td>119.75</td>\n",
       "      <td>119.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-02-07</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.378008</td>\n",
       "      <td>0.426748</td>\n",
       "      <td>0.592789</td>\n",
       "      <td>0.949260</td>\n",
       "      <td>1.101574</td>\n",
       "      <td>1.281908</td>\n",
       "      <td>-1.288431</td>\n",
       "      <td>1.273730</td>\n",
       "      <td>...</td>\n",
       "      <td>3.856851</td>\n",
       "      <td>0.568635</td>\n",
       "      <td>-0.322023</td>\n",
       "      <td>0.518339</td>\n",
       "      <td>0.702038</td>\n",
       "      <td>-0.899060</td>\n",
       "      <td>122.90</td>\n",
       "      <td>124.10</td>\n",
       "      <td>120.95</td>\n",
       "      <td>122.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2000-02-08</td>\n",
       "      <td>-0.196110</td>\n",
       "      <td>0.353748</td>\n",
       "      <td>0.677387</td>\n",
       "      <td>0.254527</td>\n",
       "      <td>-0.366454</td>\n",
       "      <td>1.138729</td>\n",
       "      <td>0.998930</td>\n",
       "      <td>-1.329397</td>\n",
       "      <td>0.942469</td>\n",
       "      <td>...</td>\n",
       "      <td>2.657684</td>\n",
       "      <td>1.222974</td>\n",
       "      <td>-0.720402</td>\n",
       "      <td>0.474125</td>\n",
       "      <td>0.440887</td>\n",
       "      <td>1.567010</td>\n",
       "      <td>122.40</td>\n",
       "      <td>125.05</td>\n",
       "      <td>120.40</td>\n",
       "      <td>121.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2000-02-09</td>\n",
       "      <td>-0.192447</td>\n",
       "      <td>0.306497</td>\n",
       "      <td>0.819824</td>\n",
       "      <td>0.658684</td>\n",
       "      <td>0.641075</td>\n",
       "      <td>1.076862</td>\n",
       "      <td>1.233568</td>\n",
       "      <td>-1.637689</td>\n",
       "      <td>0.994675</td>\n",
       "      <td>...</td>\n",
       "      <td>3.026664</td>\n",
       "      <td>2.114708</td>\n",
       "      <td>-0.407511</td>\n",
       "      <td>0.622228</td>\n",
       "      <td>0.721716</td>\n",
       "      <td>-0.243605</td>\n",
       "      <td>120.85</td>\n",
       "      <td>125.05</td>\n",
       "      <td>119.85</td>\n",
       "      <td>123.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000-02-10</td>\n",
       "      <td>0.330923</td>\n",
       "      <td>0.617187</td>\n",
       "      <td>0.552762</td>\n",
       "      <td>0.366383</td>\n",
       "      <td>-0.311193</td>\n",
       "      <td>0.651653</td>\n",
       "      <td>0.275024</td>\n",
       "      <td>-1.607973</td>\n",
       "      <td>0.269679</td>\n",
       "      <td>...</td>\n",
       "      <td>3.316674</td>\n",
       "      <td>1.871659</td>\n",
       "      <td>-0.157431</td>\n",
       "      <td>0.985272</td>\n",
       "      <td>1.379493</td>\n",
       "      <td>-1.628002</td>\n",
       "      <td>123.60</td>\n",
       "      <td>124.75</td>\n",
       "      <td>115.25</td>\n",
       "      <td>122.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2000-02-11</td>\n",
       "      <td>0.977388</td>\n",
       "      <td>0.576306</td>\n",
       "      <td>0.048738</td>\n",
       "      <td>-0.013448</td>\n",
       "      <td>-0.428351</td>\n",
       "      <td>0.326747</td>\n",
       "      <td>0.131530</td>\n",
       "      <td>-1.828185</td>\n",
       "      <td>0.546650</td>\n",
       "      <td>...</td>\n",
       "      <td>2.384344</td>\n",
       "      <td>0.988933</td>\n",
       "      <td>-0.154629</td>\n",
       "      <td>0.473525</td>\n",
       "      <td>0.907189</td>\n",
       "      <td>-4.013240</td>\n",
       "      <td>118.85</td>\n",
       "      <td>119.00</td>\n",
       "      <td>110.45</td>\n",
       "      <td>120.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2000-02-14</td>\n",
       "      <td>-0.649462</td>\n",
       "      <td>-0.806483</td>\n",
       "      <td>-0.954529</td>\n",
       "      <td>-1.222378</td>\n",
       "      <td>-1.533955</td>\n",
       "      <td>-1.217328</td>\n",
       "      <td>-0.075927</td>\n",
       "      <td>-2.486079</td>\n",
       "      <td>0.140712</td>\n",
       "      <td>...</td>\n",
       "      <td>1.935983</td>\n",
       "      <td>0.085505</td>\n",
       "      <td>-0.246361</td>\n",
       "      <td>0.529501</td>\n",
       "      <td>0.923675</td>\n",
       "      <td>-4.439655</td>\n",
       "      <td>116.45</td>\n",
       "      <td>116.80</td>\n",
       "      <td>105.25</td>\n",
       "      <td>116.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2000-02-15</td>\n",
       "      <td>-1.074607</td>\n",
       "      <td>-1.359121</td>\n",
       "      <td>-1.825901</td>\n",
       "      <td>-1.959771</td>\n",
       "      <td>-1.104848</td>\n",
       "      <td>-1.601324</td>\n",
       "      <td>-0.717085</td>\n",
       "      <td>-2.587555</td>\n",
       "      <td>-0.669097</td>\n",
       "      <td>...</td>\n",
       "      <td>1.222183</td>\n",
       "      <td>-0.051403</td>\n",
       "      <td>-1.024949</td>\n",
       "      <td>0.518408</td>\n",
       "      <td>1.160291</td>\n",
       "      <td>3.157420</td>\n",
       "      <td>112.05</td>\n",
       "      <td>114.35</td>\n",
       "      <td>105.25</td>\n",
       "      <td>110.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2000-02-16</td>\n",
       "      <td>-1.939741</td>\n",
       "      <td>-2.072462</td>\n",
       "      <td>-2.426875</td>\n",
       "      <td>-1.820694</td>\n",
       "      <td>0.171634</td>\n",
       "      <td>-1.786449</td>\n",
       "      <td>-0.703717</td>\n",
       "      <td>-2.563710</td>\n",
       "      <td>-0.401469</td>\n",
       "      <td>...</td>\n",
       "      <td>1.251580</td>\n",
       "      <td>0.487841</td>\n",
       "      <td>-1.605690</td>\n",
       "      <td>0.609070</td>\n",
       "      <td>0.911960</td>\n",
       "      <td>-1.486664</td>\n",
       "      <td>112.35</td>\n",
       "      <td>115.70</td>\n",
       "      <td>109.95</td>\n",
       "      <td>114.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2000-02-17</td>\n",
       "      <td>-2.062374</td>\n",
       "      <td>-1.576333</td>\n",
       "      <td>-1.627765</td>\n",
       "      <td>-1.281362</td>\n",
       "      <td>0.934836</td>\n",
       "      <td>-1.133411</td>\n",
       "      <td>-0.976304</td>\n",
       "      <td>-1.810490</td>\n",
       "      <td>1.006196</td>\n",
       "      <td>...</td>\n",
       "      <td>1.099126</td>\n",
       "      <td>0.014284</td>\n",
       "      <td>-1.379601</td>\n",
       "      <td>0.356659</td>\n",
       "      <td>1.165770</td>\n",
       "      <td>-0.710164</td>\n",
       "      <td>113.35</td>\n",
       "      <td>115.70</td>\n",
       "      <td>105.95</td>\n",
       "      <td>112.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2000-02-18</td>\n",
       "      <td>-1.203728</td>\n",
       "      <td>-1.119424</td>\n",
       "      <td>-1.312861</td>\n",
       "      <td>-1.662198</td>\n",
       "      <td>-0.622664</td>\n",
       "      <td>-1.227625</td>\n",
       "      <td>-1.271035</td>\n",
       "      <td>-1.891012</td>\n",
       "      <td>0.605513</td>\n",
       "      <td>...</td>\n",
       "      <td>1.361637</td>\n",
       "      <td>0.227203</td>\n",
       "      <td>-1.264041</td>\n",
       "      <td>0.245730</td>\n",
       "      <td>1.381693</td>\n",
       "      <td>-4.604381</td>\n",
       "      <td>109.80</td>\n",
       "      <td>109.80</td>\n",
       "      <td>105.50</td>\n",
       "      <td>111.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2000-02-21</td>\n",
       "      <td>-2.011534</td>\n",
       "      <td>-2.360013</td>\n",
       "      <td>-2.117703</td>\n",
       "      <td>-2.493118</td>\n",
       "      <td>-1.243495</td>\n",
       "      <td>-1.039810</td>\n",
       "      <td>-0.905187</td>\n",
       "      <td>-2.286667</td>\n",
       "      <td>-0.665131</td>\n",
       "      <td>...</td>\n",
       "      <td>1.274963</td>\n",
       "      <td>0.268968</td>\n",
       "      <td>-1.589226</td>\n",
       "      <td>0.185194</td>\n",
       "      <td>1.319954</td>\n",
       "      <td>0.515464</td>\n",
       "      <td>106.85</td>\n",
       "      <td>110.85</td>\n",
       "      <td>105.50</td>\n",
       "      <td>106.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2000-02-22</td>\n",
       "      <td>-2.301061</td>\n",
       "      <td>-2.151107</td>\n",
       "      <td>-2.119718</td>\n",
       "      <td>-2.142064</td>\n",
       "      <td>0.422501</td>\n",
       "      <td>-0.214320</td>\n",
       "      <td>-0.740215</td>\n",
       "      <td>-1.957417</td>\n",
       "      <td>-0.444290</td>\n",
       "      <td>...</td>\n",
       "      <td>1.230409</td>\n",
       "      <td>0.364772</td>\n",
       "      <td>-1.172751</td>\n",
       "      <td>-0.103243</td>\n",
       "      <td>1.100675</td>\n",
       "      <td>3.076923</td>\n",
       "      <td>108.55</td>\n",
       "      <td>111.60</td>\n",
       "      <td>107.10</td>\n",
       "      <td>107.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2000-02-23</td>\n",
       "      <td>-1.670887</td>\n",
       "      <td>-1.656883</td>\n",
       "      <td>-1.569320</td>\n",
       "      <td>-1.267786</td>\n",
       "      <td>1.605126</td>\n",
       "      <td>0.506204</td>\n",
       "      <td>0.050075</td>\n",
       "      <td>-1.494619</td>\n",
       "      <td>-0.699133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.708148</td>\n",
       "      <td>-0.052186</td>\n",
       "      <td>-0.980503</td>\n",
       "      <td>-1.014657</td>\n",
       "      <td>0.126165</td>\n",
       "      <td>-1.537766</td>\n",
       "      <td>110.20</td>\n",
       "      <td>111.60</td>\n",
       "      <td>106.50</td>\n",
       "      <td>110.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2000-02-24</td>\n",
       "      <td>-1.366238</td>\n",
       "      <td>-1.422324</td>\n",
       "      <td>-1.358331</td>\n",
       "      <td>-1.490032</td>\n",
       "      <td>-0.528189</td>\n",
       "      <td>-0.294080</td>\n",
       "      <td>-0.319013</td>\n",
       "      <td>-1.472892</td>\n",
       "      <td>-1.661528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.530452</td>\n",
       "      <td>-1.233068</td>\n",
       "      <td>-0.941193</td>\n",
       "      <td>-0.976769</td>\n",
       "      <td>0.385420</td>\n",
       "      <td>0.045935</td>\n",
       "      <td>107.75</td>\n",
       "      <td>109.85</td>\n",
       "      <td>101.95</td>\n",
       "      <td>108.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2000-02-25</td>\n",
       "      <td>-1.680249</td>\n",
       "      <td>-1.665735</td>\n",
       "      <td>-1.510077</td>\n",
       "      <td>-1.461250</td>\n",
       "      <td>0.033092</td>\n",
       "      <td>0.075897</td>\n",
       "      <td>0.493437</td>\n",
       "      <td>-1.430882</td>\n",
       "      <td>-2.424417</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081697</td>\n",
       "      <td>-2.561532</td>\n",
       "      <td>-1.090127</td>\n",
       "      <td>0.174516</td>\n",
       "      <td>-0.148781</td>\n",
       "      <td>-6.060606</td>\n",
       "      <td>105.95</td>\n",
       "      <td>106.55</td>\n",
       "      <td>100.75</td>\n",
       "      <td>108.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2000-02-28</td>\n",
       "      <td>-1.697353</td>\n",
       "      <td>-2.045428</td>\n",
       "      <td>-2.011407</td>\n",
       "      <td>-2.307633</td>\n",
       "      <td>-1.966604</td>\n",
       "      <td>-0.289909</td>\n",
       "      <td>-1.729787</td>\n",
       "      <td>-1.563181</td>\n",
       "      <td>-3.097790</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.149500</td>\n",
       "      <td>-2.235868</td>\n",
       "      <td>-2.265504</td>\n",
       "      <td>-0.179609</td>\n",
       "      <td>0.061992</td>\n",
       "      <td>0.635386</td>\n",
       "      <td>104.15</td>\n",
       "      <td>113.00</td>\n",
       "      <td>100.75</td>\n",
       "      <td>102.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 815 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  시가지수(포인트)  고가지수(포인트)  저가지수(포인트)  종가지수(포인트)    수익률(%)  \\\n",
       "0   2000-01-31  -0.218225  -0.493106  -0.400439  -0.223565  0.212872   \n",
       "1   2000-02-01  -0.213200  -0.230417  -0.373954  -0.595373 -0.407635   \n",
       "2   2000-02-02  -0.208520  -0.448886  -0.367787  -0.200398  0.696677   \n",
       "3   2000-02-03  -0.204125  -0.216603  -0.062403  -0.021445  0.366314   \n",
       "4   2000-02-07  -0.200000   0.378008   0.426748   0.592789  0.949260   \n",
       "5   2000-02-08  -0.196110   0.353748   0.677387   0.254527 -0.366454   \n",
       "6   2000-02-09  -0.192447   0.306497   0.819824   0.658684  0.641075   \n",
       "7   2000-02-10   0.330923   0.617187   0.552762   0.366383 -0.311193   \n",
       "8   2000-02-11   0.977388   0.576306   0.048738  -0.013448 -0.428351   \n",
       "9   2000-02-14  -0.649462  -0.806483  -0.954529  -1.222378 -1.533955   \n",
       "10  2000-02-15  -1.074607  -1.359121  -1.825901  -1.959771 -1.104848   \n",
       "11  2000-02-16  -1.939741  -2.072462  -2.426875  -1.820694  0.171634   \n",
       "12  2000-02-17  -2.062374  -1.576333  -1.627765  -1.281362  0.934836   \n",
       "13  2000-02-18  -1.203728  -1.119424  -1.312861  -1.662198 -0.622664   \n",
       "14  2000-02-21  -2.011534  -2.360013  -2.117703  -2.493118 -1.243495   \n",
       "15  2000-02-22  -2.301061  -2.151107  -2.119718  -2.142064  0.422501   \n",
       "16  2000-02-23  -1.670887  -1.656883  -1.569320  -1.267786  1.605126   \n",
       "17  2000-02-24  -1.366238  -1.422324  -1.358331  -1.490032 -0.528189   \n",
       "18  2000-02-25  -1.680249  -1.665735  -1.510077  -1.461250  0.033092   \n",
       "19  2000-02-28  -1.697353  -2.045428  -2.011407  -2.307633 -1.966604   \n",
       "\n",
       "    수익률 (1주)(%)  수익률 (1개월)(%)  수익률 (3개월)(%)  수익률 (6개월)(%)  ...  \\\n",
       "0      1.352924     -0.851790     -0.382658     -0.162615  ...   \n",
       "1      1.808646     -1.074203     -1.707405     -0.490731  ...   \n",
       "2      2.107257     -0.769928     -1.558400      0.074389  ...   \n",
       "3      1.488978     -0.625155     -1.268930     -0.079967  ...   \n",
       "4      1.101574      1.281908     -1.288431      1.273730  ...   \n",
       "5      1.138729      0.998930     -1.329397      0.942469  ...   \n",
       "6      1.076862      1.233568     -1.637689      0.994675  ...   \n",
       "7      0.651653      0.275024     -1.607973      0.269679  ...   \n",
       "8      0.326747      0.131530     -1.828185      0.546650  ...   \n",
       "9     -1.217328     -0.075927     -2.486079      0.140712  ...   \n",
       "10    -1.601324     -0.717085     -2.587555     -0.669097  ...   \n",
       "11    -1.786449     -0.703717     -2.563710     -0.401469  ...   \n",
       "12    -1.133411     -0.976304     -1.810490      1.006196  ...   \n",
       "13    -1.227625     -1.271035     -1.891012      0.605513  ...   \n",
       "14    -1.039810     -0.905187     -2.286667     -0.665131  ...   \n",
       "15    -0.214320     -0.740215     -1.957417     -0.444290  ...   \n",
       "16     0.506204      0.050075     -1.494619     -0.699133  ...   \n",
       "17    -0.294080     -0.319013     -1.472892     -1.661528  ...   \n",
       "18     0.075897      0.493437     -1.430882     -2.424417  ...   \n",
       "19    -0.289909     -1.729787     -1.563181     -3.097790  ...   \n",
       "\n",
       "    주요상품선물_금(선물)($/ounce)  주요상품선물_은(선물)($/ounce)  주요상품선물_알루미늄(선물)($/ton)  \\\n",
       "0               -0.853010               0.974513                0.827681   \n",
       "1               -0.039689               0.193571                1.002058   \n",
       "2                0.800355               0.031266                0.069626   \n",
       "3                1.515992               0.623370                0.378121   \n",
       "4                3.856851               0.568635               -0.322023   \n",
       "5                2.657684               1.222974               -0.720402   \n",
       "6                3.026664               2.114708               -0.407511   \n",
       "7                3.316674               1.871659               -0.157431   \n",
       "8                2.384344               0.988933               -0.154629   \n",
       "9                1.935983               0.085505               -0.246361   \n",
       "10               1.222183              -0.051403               -1.024949   \n",
       "11               1.251580               0.487841               -1.605690   \n",
       "12               1.099126               0.014284               -1.379601   \n",
       "13               1.361637               0.227203               -1.264041   \n",
       "14               1.274963               0.268968               -1.589226   \n",
       "15               1.230409               0.364772               -1.172751   \n",
       "16               0.708148              -0.052186               -0.980503   \n",
       "17               0.530452              -1.233068               -0.941193   \n",
       "18              -0.081697              -2.561532               -1.090127   \n",
       "19              -0.149500              -2.235868               -2.265504   \n",
       "\n",
       "    주요상품선물_옥수수(최근월물)(￠/bu)  대두박(￠/bu)        종가      시가      고가      저가  \\\n",
       "0                 0.373581   0.614542 -3.553512  120.85  121.70  115.35   \n",
       "1                 0.594935   0.706918  2.687473  116.55  120.25  115.95   \n",
       "2                 0.430159   0.725768  0.675390  119.35  123.45  116.80   \n",
       "3                 0.084769   0.352875  2.599581  120.40  123.60  119.75   \n",
       "4                 0.518339   0.702038 -0.899060  122.90  124.10  120.95   \n",
       "5                 0.474125   0.440887  1.567010  122.40  125.05  120.40   \n",
       "6                 0.622228   0.721716 -0.243605  120.85  125.05  119.85   \n",
       "7                 0.985272   1.379493 -1.628002  123.60  124.75  115.25   \n",
       "8                 0.473525   0.907189 -4.013240  118.85  119.00  110.45   \n",
       "9                 0.529501   0.923675 -4.439655  116.45  116.80  105.25   \n",
       "10                0.518408   1.160291  3.157420  112.05  114.35  105.25   \n",
       "11                0.609070   0.911960 -1.486664  112.35  115.70  109.95   \n",
       "12                0.356659   1.165770 -0.710164  113.35  115.70  105.95   \n",
       "13                0.245730   1.381693 -4.604381  109.80  109.80  105.50   \n",
       "14                0.185194   1.319954  0.515464  106.85  110.85  105.50   \n",
       "15               -0.103243   1.100675  3.076923  108.55  111.60  107.10   \n",
       "16               -1.014657   0.126165 -1.537766  110.20  111.60  106.50   \n",
       "17               -0.976769   0.385420  0.045935  107.75  109.85  101.95   \n",
       "18                0.174516  -0.148781 -6.060606  105.95  106.55  100.75   \n",
       "19               -0.179609   0.061992  0.635386  104.15  113.00  100.75   \n",
       "\n",
       "     close  \n",
       "0   119.60  \n",
       "1   115.35  \n",
       "2   118.45  \n",
       "3   119.25  \n",
       "4   122.35  \n",
       "5   121.25  \n",
       "6   123.15  \n",
       "7   122.85  \n",
       "8   120.85  \n",
       "9   116.00  \n",
       "10  110.85  \n",
       "11  114.35  \n",
       "12  112.65  \n",
       "13  111.85  \n",
       "14  106.70  \n",
       "15  107.25  \n",
       "16  110.55  \n",
       "17  108.85  \n",
       "18  108.90  \n",
       "19  102.30  \n",
       "\n",
       "[20 rows x 815 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def loss_fn_model1(train_y, preds):\n",
    "    \n",
    "    #preds = tf.cast(preds, dtype=tf.float64)\n",
    "    #train_y = tf.cast(train_y, dtype=tf.float64)\n",
    "    rates = train_y / 100\n",
    "    updown = tf.math.sign(train_y)\n",
    "    profits = 1 + rates*tf.math.sign(preds)\n",
    "    \n",
    "    loss1 = keras.losses.MSE(train_y, preds)\n",
    "    \n",
    "    batches_float32 = tf.cast(tf.shape(train_y)[0], dtype=tf.float32)\n",
    "    batches_int32 = tf.cast(tf.shape(train_y)[0], dtype=tf.int32)\n",
    "\n",
    "    return_plus = 0.000000000\n",
    "    return_minus = 0.000000000\n",
    "    for i in range(batches_int32):\n",
    "        if profits[i, -1, 0] - 1 > 0: return_plus += profits[i, -1, 0]-1      \n",
    "        else: return_minus += 1-profits[i, -1, 0]    \n",
    "\n",
    "    loss2 = (return_plus - return_minus)/batches_float32\n",
    "    \n",
    "    return loss1 - loss2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def loss_fn_model2(m1, m2, train_x, train_y):\n",
    "    \n",
    "    batches = train_y.shape[0]\n",
    "    steps = train_y.shape[1]    \n",
    "    \n",
    "    train_y_close = tf.expand_dims(train_y[:, :, 3], 2)\n",
    "    train_y_close_normal = train_y\n",
    "    \n",
    "    train_y_target = tf.expand_dims(train_y[:, :, 4], 2)\n",
    "    rates = tf.cast(train_y_target / 100, dtype=tf.float64)      \n",
    "    \n",
    "    \n",
    "    # 예측모델(model1)의 output\n",
    "    preds = tf.cast(m1(train_x), dtype=tf.float64)\n",
    "    \n",
    "    # 다음 step의 optimal prediction = rates\n",
    "    new_preds = tf.concat([preds[:, 1:, :], tf.reshape(rates[:, -1, :], [batches, 1, 1])], 1)    \n",
    "    \n",
    "    # 다음 step의 수익률\n",
    "    new_rates = tf.concat([rates[:, 1:, :], tf.reshape(rates[:, -1, :], [batches, 1, 1])], 1)\n",
    "    \n",
    "    # ccurrent step의 수익률\n",
    "    rates = tf.concat([tf.reshape(rates[:, :-1, 0], [batches, steps - 1, 1]), np.zeros((batches, 1, 1))], 1)\n",
    "    \n",
    "    # model2 current input 생성\n",
    "    train_x_m2 = tf.concat([preds, rates], 2)\n",
    "    y = tf.cast(m2(train_x_m2), dtype=tf.float64)\n",
    "    rnum = tf.math.argmax(y, 2)\n",
    "    \n",
    "    # model2 current input 생성\n",
    "    train_x_m2 = tf.concat([new_preds, new_rates], 2)\n",
    "    y2 = tf.cast(m2(train_x_m2)[:, -1, :], dtype=tf.float64)\n",
    "    next_rnum = tf.math.argmax(y2, 1)\n",
    "    \n",
    "\n",
    "    # batch i에서 model2의 target = num * ( 1 + (p(t) - p(t-1)) / P(t-1)) * p(t-1) / p(t-n) + gamma * next_best_num\n",
    "    targets = []\n",
    "    for i in range(batches):\n",
    "        reward_t = 1 + tf.cast(rnum[i, -1], dtype=tf.float64)*tf.math.sign(preds[i, -2, 0]) * \\\n",
    "                       tf.cast((train_y_close[i, -1, 0] - train_y_close[i, -2, 0]) / train_y_close[i, -2, 0], dtype=tf.float64) * \\\n",
    "                       tf.cast(train_y_close[i, -2, 0] / train_y_close[i, 0, 0], dtype=tf.float64)\n",
    " \n",
    "        targets.append(reward_t*(1-gamma) + \\\n",
    "                       gamma*(1 + tf.cast(next_rnum[i], dtype=tf.float64)*tf.math.abs(new_rates[i, -2, 0]) * \\\n",
    "                       tf.cast(train_y_close[i, -1, 0] / train_y_close[i, 1, 0], dtype=tf.float64)))\n",
    "    \n",
    "    targets = tf.reshape(targets, [-1])\n",
    "    \n",
    "    values = []\n",
    "    for i in range(batches):\n",
    "        \n",
    "        profit = 1 + y[i, -1, rnum[i, -1]]*tf.math.sign(preds[i, -2, 0])* \\\n",
    "                       tf.cast((train_y_close[i, -1, 0] - train_y_close[i, -2, 0]) / train_y_close[i, -2, 0], dtype=tf.float64) * \\\n",
    "                       tf.cast(train_y_close[i, -2, 0] / train_y_close[i, 0, 0], dtype=tf.float64)\n",
    "        values.append(profit)        \n",
    "\n",
    "    values = tf.reshape(values, [-1])     \n",
    "    \n",
    "    #targets = np.array(targets) \n",
    "    #Qvalues = np.array(Qvalues)\n",
    "    \n",
    "    loss = keras.losses.MSE(targets, values)\n",
    "  \n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(m1, m2, test_x, test_y):\n",
    "    \n",
    "    train_y_target = np.expand_dims(train_y[:, :, 4], axis=2)\n",
    "    \n",
    "    rates = tf.cast(test_y_target / 100, dtype=tf.float64)\n",
    "    rates_ = tf.reshape(rates[:, :-1, 0], [rates.shape[0], rates.shape[1] -1, 1])\n",
    "    new_rates = tf.concat([rates_, np.zeros((rates.shape[0], 1, 1))], 1)\n",
    "    \n",
    "    preds = tf.cast(m1(test_x), dtype=tf.float64)\n",
    "    \n",
    "    # model2 input 생성\n",
    "    test_x_m2 = tf.concat([preds, new_rates], 2)\n",
    "    y = tf.cast(m2(test_x_m2), dtype=tf.float64)\n",
    "    rnum = tf.math.argmax(y, 2)\n",
    "\n",
    "    return rnum[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def gradient1(model1, model2, input_data, output_data):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn_model2(model1, model2, input_data, output_data)\n",
    "    return tape.gradient(loss, model1.trainable_variables)\n",
    "#@tf.function\n",
    "def gradient2(model1, model2, input_data, output_data):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn_model2(model1, model2, input_data, output_data)\n",
    "    return tape.gradient(loss, model2.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_51 (LSTM)               (None, 100, 10)           520       \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 100, 64)           704       \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 100, 3)            195       \n",
      "=================================================================\n",
      "Total params: 1,419\n",
      "Trainable params: 1,419\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#strategy = tf.distribute.MirroredStrategy()\n",
    "#with strategy.scope():\n",
    "model1 = models.LSTM(n_timestep,input_size,n_unit,regularizers_alpha=0.01,drop_rate=0.5)\n",
    "#model2 = models.LSTM_sigm(n_timestep,2,10,regularizers_alpha=0.01,drop_rate=0)\n",
    "\n",
    "model2 = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(n_timestep, 2)),\n",
    "    tf.keras.layers.dense(200, activation='relu', kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(50, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax'),\n",
    "])\n",
    "model2.summary()\n",
    "\n",
    "    #cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    #    checkpoint_path, verbose=1, save_weights_only=True,\n",
    "        # 다섯 번째 에포크마다 가중치를 저장합니다\n",
    "    #    save_freq=5)\n",
    "\n",
    "model1.compile(optimizer='adam',\n",
    "              loss='mse')\n",
    "                  #callbacks=[cp-callback]\n",
    "              #metrics=['accuracy'])\n",
    "model1.save_weights(checkpoint_path)            \n",
    "\n",
    "#    model2 = models.LSTM(n_timestep,input_size,n_unit,regularizers_alpha=0.01,drop_rate=0.5)\n",
    "#    model2.compile(optimizer='adam',\n",
    "#                  loss='mse')\n",
    "                  #callbacks=[cp-callback]\n",
    "              #metrics=['accuracy'])            \n",
    "\n",
    "#modle_name = model_name + \"tanh\"            \n",
    "#model2.save_weights(\"modle2_\"+ checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2042 samples, validate on 1 samples\n",
      "Epoch 1/3\n",
      "2042/2042 [==============================] - 10s 5ms/sample - loss: 42.3787 - val_loss: 6.8614\n",
      "Epoch 2/3\n",
      "2042/2042 [==============================] - 5s 2ms/sample - loss: 2.7873 - val_loss: 0.7627\n",
      "Epoch 3/3\n",
      "2042/2042 [==============================] - 5s 2ms/sample - loss: 0.5786 - val_loss: 0.3693\n",
      "WARNING:tensorflow:Layer lstm_48 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_51 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "model2 loss = tf.Tensor(6.886223112275488e-08, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(1.4916926748311175e-07, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(2.1792696761144985e-07, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(2.1779909109899482e-07, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(2.1171196234238602e-07, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(2.2285673299513108e-07, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(2.1303025719333397e-07, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(2.1259370351212452e-07, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(2.1093897797805428e-07, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(2.1830109319091605e-07, shape=(), dtype=float64)\n",
      "step :  1\n",
      "step :  2\n",
      "step :  3\n",
      "Training process is stopped early....\n",
      "test dates 2017-01-02~2017-01-02\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([1], shape=(1,), dtype=int64)\n",
      "Train on 450 samples, validate on 1 samples\n",
      "Epoch 1/3\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.1438 - val_loss: 0.1059\n",
      "Epoch 2/3\n",
      "450/450 [==============================] - 1s 2ms/sample - loss: 0.0932 - val_loss: 0.0735\n",
      "Epoch 3/3\n",
      "450/450 [==============================] - 1s 2ms/sample - loss: 0.0790 - val_loss: 0.0590\n",
      "model2 loss = tf.Tensor(9.311041815175107e-07, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(7.657727347831746e-07, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(8.142101316577108e-07, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(8.250018086961142e-07, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(8.712781936369743e-07, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(8.526031425207515e-07, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(9.296000225365473e-07, shape=(), dtype=float64)\n",
      "step :  1\n",
      "step :  2\n",
      "step :  3\n",
      "Training process is stopped early....\n",
      "test dates 2017-01-03~2017-01-03\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([1], shape=(1,), dtype=int64)\n",
      "Train on 450 samples, validate on 1 samples\n",
      "Epoch 1/3\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.1008 - val_loss: 0.0728\n",
      "Epoch 2/3\n",
      "450/450 [==============================] - 1s 2ms/sample - loss: 0.0733 - val_loss: 0.0506\n",
      "Epoch 3/3\n",
      "450/450 [==============================] - 1s 2ms/sample - loss: 0.0616 - val_loss: 0.0446\n",
      "model2 loss = tf.Tensor(2.6354875983978526e-07, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(3.116298769010005e-08, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(3.356146673879235e-08, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(2.674402226548191e-08, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(1.948559594973568e-08, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(4.359222166122828e-08, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(5.8135239172720266e-08, shape=(), dtype=float64)\n",
      "step :  1\n",
      "step :  2\n",
      "step :  3\n",
      "Training process is stopped early....\n",
      "test dates 2017-01-04~2017-01-04\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([1], shape=(1,), dtype=int64)\n",
      "Train on 450 samples, validate on 1 samples\n",
      "Epoch 1/3\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.1099 - val_loss: 0.0789\n",
      "Epoch 2/3\n",
      "450/450 [==============================] - 1s 2ms/sample - loss: 0.0749 - val_loss: 0.0473\n",
      "Epoch 3/3\n",
      "450/450 [==============================] - 1s 2ms/sample - loss: 0.0616 - val_loss: 0.0405\n",
      "model2 loss = tf.Tensor(7.252935292990639e-08, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(7.475106065823928e-08, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(8.041084646048949e-08, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(7.938159868379401e-08, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(7.832257011158359e-08, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(8.105629527126854e-08, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(7.895098430164605e-08, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(7.499403560194857e-08, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(7.564949127451602e-08, shape=(), dtype=float64)\n",
      "step :  1\n",
      "step :  2\n",
      "step :  3\n",
      "Training process is stopped early....\n",
      "test dates 2017-01-05~2017-01-05\n",
      "prediction1 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([1], shape=(1,), dtype=int64)\n",
      "Train on 450 samples, validate on 1 samples\n",
      "Epoch 1/3\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0864 - val_loss: 0.0560\n",
      "Epoch 2/3\n",
      "450/450 [==============================] - 1s 2ms/sample - loss: 0.0646 - val_loss: 0.0394\n",
      "Epoch 3/3\n",
      "450/450 [==============================] - 1s 2ms/sample - loss: 0.0512 - val_loss: 0.0345\n",
      "model2 loss = tf.Tensor(1.484916422118141e-07, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(2.2267248387414412e-07, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(1.8429516678332052e-07, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(2.081895228543439e-07, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(2.2269459187149547e-07, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(2.3567955825853533e-07, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(2.3029806345168975e-07, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(1.963572629814e-07, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(2.155437878557064e-07, shape=(), dtype=float64)\n",
      "step :  1\n",
      "step :  2\n",
      "step :  3\n",
      "Training process is stopped early....\n",
      "test dates 2017-01-06~2017-01-06\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([1], shape=(1,), dtype=int64)\n",
      "Train on 450 samples, validate on 1 samples\n",
      "Epoch 1/3\n",
      "450/450 [==============================] - 1s 3ms/sample - loss: 0.0953 - val_loss: 0.0675\n",
      "Epoch 2/3\n",
      "450/450 [==============================] - 1s 2ms/sample - loss: 0.0643 - val_loss: 0.0425\n",
      "Epoch 3/3\n",
      "450/450 [==============================] - 1s 2ms/sample - loss: 0.0507 - val_loss: 0.0374\n",
      "model2 loss = tf.Tensor(4.500473497931735e-10, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(2.9048657096618116e-09, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(3.815047773788147e-09, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(3.5620966052780473e-09, shape=(), dtype=float64)\n",
      "model2 loss = tf.Tensor(6.343813080694877e-10, shape=(), dtype=float64)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-75-67ffc0db5233>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mgradients1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mgradients2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradient2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradients2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-2a12e87f2e0a>\u001b[0m in \u001b[0;36mgradient2\u001b[0;34m(model1, model2, input_data, output_data)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGradientTape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn_model2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1027\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1030\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/ops/array_grad.py\u001b[0m in \u001b[0;36m_StridedSliceGrad\u001b[0;34m(op, grad)\u001b[0m\n\u001b[1;32m    281\u001b[0m       \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ellipsis_mask\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m       \u001b[0mnew_axis_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_attr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"new_axis_mask\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m       shrink_axis_mask=op.get_attr(\"shrink_axis_mask\")), None, None, None\n\u001b[0m\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/ops/gen_array_ops.py\u001b[0m in \u001b[0;36mstrided_slice_grad\u001b[0;34m(shape, begin, end, strides, dy, begin_mask, end_mask, ellipsis_mask, new_axis_mask, shrink_axis_mask, name)\u001b[0m\n\u001b[1;32m   9693\u001b[0m         \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop_callbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbegin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"begin_mask\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9694\u001b[0m         \u001b[0mbegin_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"end_mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ellipsis_mask\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mellipsis_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 9695\u001b[0;31m         \"new_axis_mask\", new_axis_mask, \"shrink_axis_mask\", shrink_axis_mask)\n\u001b[0m\u001b[1;32m   9696\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   9697\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "current_train_start = df.loc[prepro.date_to_index(df, train_start), 'date']\n",
    "current_train_end = df.loc[prepro.date_to_index(df, train_end), 'date']\n",
    "current_test_start = df.loc[prepro.date_to_index(df, test_start), 'date']\n",
    "current_test_end = df.loc[prepro.date_to_index(df, test_start) + trans_day - 1, 'date']\n",
    "\n",
    "\n",
    "#  각 transfer 구간의 예측값들을 합치기 위하여\n",
    "test_prediction1 = []\n",
    "test_prediction2 = []\n",
    "test_target = []\n",
    "\n",
    "while True:\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "\n",
    "    train_data, test_data = prepro.get_train_test_data(df, target_column, remove_columns, \n",
    "                                                           current_train_start, current_train_end,\n",
    "                                                           current_test_start, current_test_end,\n",
    "                                                           future_day, n_timestep, time_interval)\n",
    "\n",
    "\n",
    "    train_x, train_y = prepro.get_LSTM_dataset(train_data, n_timestep, time_interval, input_size, future_day)\n",
    "    \n",
    "    # randomly selected  step size\n",
    "    #drop_idx = np.random.randint(int(train_y.shape[1]/2))\n",
    "    #train_x[:, :drop_idx, :] = 0\n",
    "    #train_y[:, :drop_idx, :] = 0    \n",
    "    \n",
    "    train_x = train_x[:train_end_back]\n",
    "    train_y = train_y[:train_end_back]\n",
    "    \n",
    "    test_x, test_y = prepro.get_LSTM_dataset(test_data, n_timestep, time_interval, input_size, future_day)\n",
    "    test_y_target = np.expand_dims(test_y[:, :, 4], axis=2)    \n",
    "    \n",
    "    # train, validation set 분리하여 train1은 예측모델로 train2는 손절값 학습 모델로 사용\n",
    "    train1_x, train2_x, train1_y, train2_y = train_test_split(train_x, train_y, test_size=0.5)    \n",
    "    \n",
    "    updown = np.sign(test_y[:, -1, 4]).reshape((-1))\n",
    "    \n",
    "    # random batch_size\n",
    "    #batch_size = np.random.randint(10, 50)    \n",
    "\n",
    "    # model1 training\n",
    "    train1_y_target = np.expand_dims(train1_y[:, :, 4], axis=2)\n",
    "    early_stopping1 = tf.keras.callbacks.EarlyStopping(patience=2, verbose=1)\n",
    "    model1.load_weights(checkpoint_path)\n",
    "    model1.fit(train1_x, train1_y_target, batch_size=batch_size, epochs=3, callbacks=[early_stopping1], validation_data=(test_x, test_y_target))\n",
    "    model1.save_weights(checkpoint_path)\n",
    "    \n",
    "    # model2 training\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)    \n",
    "    early_stopping2 = learn.EarlyStopping(patience=2, verbose=1)\n",
    "    iter = epochs\n",
    "    basic_epochs = tf.cast(epochs / 5, dtype=tf.int32)\n",
    "    for iteration in range(iter):\n",
    "        batch_input, batch_output = learn.next_random_batch(train2_x, train2_y, batch_size)\n",
    "\n",
    "        gradients1 = gradient1(model1, model2, batch_input, batch_output)\n",
    "        optimizer.apply_gradients(zip(gradients1, model1.trainable_variables))\n",
    "        gradients2 = gradient2(model1, model2, batch_input, batch_output)        \n",
    "        optimizer.apply_gradients(zip(gradients2, model2.trainable_variables))\n",
    "\n",
    "        if iteration % 100 == 0:\n",
    "\n",
    "            # model2의 test loss\n",
    "            loss2 = loss_fn_model2(model1, model2, test_x, test_y)\n",
    "            print('model2 loss =', loss2)   \n",
    "\n",
    "        if iteration > iter / 2 and early_stopping2.validate(loss2)==True:\n",
    "            break    \n",
    "\n",
    "    if iter > basic_epochs: iter -= basic_epochs\n",
    "    if iter < basic_epochs: iter = basic_epochs\n",
    "\n",
    "    print('test dates ' + current_test_start + \"~\" + current_test_end)\n",
    "\n",
    "    # prediction1 accuracy\n",
    "    prediction1 = model1.predict(test_x)[:, -1, 0].reshape(-1)\n",
    "    temp = tf.math.multiply(updown, prediction1)\n",
    "    accu = tf.reduce_sum(list(map(lambda x: 1 if x > 0 else 0, temp)))/len(temp)\n",
    "    print('prediction1 accuracy = ', accu)\n",
    "\n",
    "    # reinforced prediction2\n",
    "    prediction2 = test(model1, model2, test_x, test_y)    \n",
    "    print('optimal shares = ', prediction2)\n",
    "\n",
    "    test_prediction1.append(prediction1)\n",
    "    test_prediction2.append(prediction2)\n",
    "\n",
    "    # escape from while\n",
    "    if current_test_end == test_end:\n",
    "     break\n",
    "\n",
    "    #train, start dates shift\n",
    "    current_train_end = df.loc[prepro.date_to_index(df, current_train_end) + trans_day, 'date']\n",
    "    current_train_start = df.loc[prepro.date_to_index(df, current_train_end) - 1000, 'date']\n",
    "    current_test_start = df.loc[prepro.date_to_index(df, current_test_start) + trans_day, 'date']\n",
    "    if prepro.date_to_index(df, test_end) - prepro.date_to_index(df, current_test_start) < trans_day:\n",
    "        current_test_end = test_end\n",
    "    else:\n",
    "        current_test_end = df.loc[prepro.date_to_index(df, current_test_end) + trans_day, 'date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = np.concatenate(test_prediction1)\n",
    "t2 = np.concatenate(test_prediction2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_prediction = np.concatenate(test_prediction, axis=0).reshape((-1, n_timestep, 1))\n",
    "#train_prediction = learn.predict_batch_test(model1, train_x[:batch_size], len(train_x[:batch_size]))\n",
    "\n",
    "# 원 지수, 날짜는 target conversion이 되기 전 dataframe에서 가져옴\n",
    "test_dates, test_base_prices, train_dates, train_base_prices = prepro.get_test_dates_prices(dataframe, test_start, test_end,\n",
    "                                                      train_start, train_end, n_timestep, time_interval, future_day, target_column)\n",
    "\n",
    "# 전체 test_oouput 생성\n",
    "_, test_data = prepro.get_train_test_data(df, target_column, remove_columns,\n",
    "                                                   train_start, train_end,\n",
    "                                                   test_start, test_end,\n",
    "                                                   future_day, n_timestep, time_interval)\n",
    "_, test_y = prepro.get_LSTM_dataset(test_data, n_timestep, time_interval, input_size, future_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원 지수, 날짜는 target conversion이 되기 전 dataframe에서 가져옴 - 종가를 test base price로 하는 경우\n",
    "result = GenerateResult(t1, t2, test_y[:, -1, 4].reshape(-1), test_dates, n_timestep, future_day, trans_day)\n",
    "\n",
    "#result.extract_last_output()\n",
    "result.convert_price(test_base_prices,conversion_type=target_type)\n",
    "\n",
    "# 손익 계산을 위한 데이터 정리\n",
    "test_start_index = prepro.date_to_index(df, test_start)\n",
    "test_end_index = prepro.date_to_index(df, test_end)\n",
    "test_open_prices = list(map(float, df.loc[test_start_index - future_day: test_end_index - future_day, '시가']))\n",
    "test_high_prices = list(map(float, df.loc[test_start_index - future_day: test_end_index - future_day, '고가']))\n",
    "test_low_prices = list(map(float, df.loc[test_start_index - future_day: test_end_index - future_day, '저가']))\n",
    "\n",
    "# 예측 손익 계산\n",
    "profits = np.zeros(len(test_dates))\n",
    "for i in range(len(test_dates)):\n",
    "    if result.test_predict_price[i]-test_open_prices[i] > 0:\n",
    "        profits[i] = result.test_output_price[i] - test_open_prices[i]\n",
    "    else:\n",
    "        profits[i] = test_open_prices[i] - result.test_output_price[i]\n",
    "\n",
    "# 투자 비율 반영 손익 계산\n",
    "profits2 = np.zeros(len(test_dates))\n",
    "for i in range(len(test_dates)):\n",
    "    if result.test_predict_price[i]-test_open_prices[i] > 0:\n",
    "        profits2[i] = (result.test_output_price[i] - test_open_prices[i])*t2[i]\n",
    "    else:\n",
    "        profits2[i] = (test_open_prices[i] - result.test_output_price[i])*t2[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 종가 대비 에측 확률 계싼\n",
    "updown = np.sign(test_y[:, -1, 4]).reshape((-1))\n",
    "temp = tf.math.multiply(updown, t1.reshape((-1)))\n",
    "accu = tf.reduce_sum(list(map(lambda x: 1 if x > 0 else 0, temp)))/len(temp)\n",
    "print('종가 대비 accuracy = ', accu)\n",
    "\n",
    "# 시가 대비 예측 확률\n",
    "cnt  = 0\n",
    "for i in range(len(profits)):\n",
    "    if profits2[i] > 0:\n",
    "        cnt += 1\n",
    "accu = cnt / len(profits2)        \n",
    "print('시가 대비 accuracy = ', accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.evaluation()\n",
    "result.table(test_open_prices, test_high_prices, test_low_prices, profits, profits2)\n",
    "result.save_result(model_name,item_name,n_unit,target_type,batch_size,n_timestep,time_interval,epochs,str(alpha),comment)\n",
    "result.save_visualization()\n",
    "result.save_model(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "current_train_start = df.loc[prepro.date_to_index(df, train_start), 'date']\n",
    "current_train_end = df.loc[prepro.date_to_index(df, train_end), 'date']\n",
    "current_test_start = df.loc[prepro.date_to_index(df, test_start), 'date']\n",
    "current_test_end = df.loc[prepro.date_to_index(df, test_start) + trans_day - 1, 'date']\n",
    "\n",
    "\n",
    "#  각 transfer 구간의 예측값들을 합치기 위하여\n",
    "test_prediction1 = []\n",
    "test_prediction2 = []\n",
    "test_target = []\n",
    "\n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(patience=3, verbose=1)\n",
    "early_stopping = learn.EarlyStopping(patience=2, verbose=1)\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "train_data, test_data = prepro.get_train_test_data(df, target_column, remove_columns, \n",
    "                                                       current_train_start, current_train_end,\n",
    "                                                       current_test_start, current_test_end,\n",
    "                                                       future_day, n_timestep, time_interval)\n",
    "\n",
    "# input_size, columns reset\n",
    "input_size = len(df.columns) - len(remove_columns)\n",
    "input_columns = df.columns.copy()\n",
    "\n",
    "train_x, train_y = prepro.get_LSTM_dataset(train_data, n_timestep, time_interval, input_size, future_day)\n",
    "test_x, test_y = prepro.get_LSTM_dataset(test_data, n_timestep, time_interval, input_size, future_day)\n",
    "\n",
    "#model.compile(optimizer='adam',\n",
    "#          loss=loss_fn)\n",
    "#          #callbacks=[cp-callback]\n",
    "#          #metrics=['accuracy'])\n",
    "\n",
    "# the firs training dataset\n",
    "train_x = train_x[:-future_day]\n",
    "train_y = train_y[:-future_day]    \n",
    "\n",
    "#global_step = tf.train.get_or_create_global_step()\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "#lr_decay = tf.train.exponential_decay(learning_rate, global_step,\n",
    "#                                      train_input.shape[0]/batch_size*5, 0.5, staircase=True)\n",
    "lr_decay = tf.compat.v1.train.exponential_decay(learning_rate,global_step, int(len(train_y)/batch_size), 0.96, staircase=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "#optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "updown = np.sign(test_y[:, -1, 0]).reshape((-1))    \n",
    "epochs = len(train_y)\n",
    "for iteration in range(399):\n",
    "    batch_input, batch_output = learn.next_random_interval_batch(train_x, train_y, batch_size, future_day)\n",
    "\n",
    "    #noise = 2*np.random.randn(batch_size,n_timestep,1)\n",
    "    #batch_output = batch_output+noise\n",
    "    #batch_input = encoder(train_input[idx])\n",
    "    gradients1 = gradient1(model1, model2, batch_input, batch_output)\n",
    "    optimizer.apply_gradients(zip(gradients1, model1.trainable_variables))\n",
    "    \n",
    "    targets = tf.reshape(train_y[:, -1, 0], [-1])\n",
    "    rates = targets / 100\n",
    "    preds = tf.reshape(model1(train_x, training=False)[:, -1, 0], [-1])\n",
    "    \n",
    "    n = len(targets)    \n",
    "    returns = [1.0]\n",
    "    losses = []\n",
    "    for i in range(n - 1):\n",
    "       \n",
    "        # average_return, std of returns, remaining days, preds[0] \n",
    "        state = []\n",
    "        \n",
    "        random_rates = []\n",
    "        for k in range(i+1):\n",
    "            random_rates.append(rates[k])\n",
    "        # 실재 수익률을 기반으로 random 수익률 생성, 예측에 의한 porfits 생성 \n",
    "        for k in range(i+1, n):\n",
    "            random_rates.append(tf.random.normal((), mean=targets[k] / 100, stddev=0.1, dtype=tf.float64))\n",
    "        profits = tf.convert_to_tensor(random_rates, dtype=tf.float64)*tf.cast(preds, dtype=tf.float64) + 1\n",
    "        \n",
    "        # 현재까지의 예측에 의한 수익률 기하평균 구하기\n",
    "        returns_past = []\n",
    "        for k in range(i+1):\n",
    "            returns_past.append(profits[k])\n",
    "        avg_return = tf.cast(tf.math.reduce_prod(returns_past)**(1/(i+1)), dtype=tf.float64)\n",
    "        \n",
    "        state.append(avg_return)\n",
    "        state.append(tf.math.reduce_std(returns))\n",
    "        state.append((n - i) / n)\n",
    "        state.append(preds[i])\n",
    "        state = np.array(state).reshape((1, 4))\n",
    "     \n",
    "        # 목표일까지의 기대 기하 평균 수익률 구하기 \n",
    "        returns_future = []\n",
    "        for j in range(i+1, n):\n",
    "            returns_future.append(profits[j])\n",
    "        avg_return_future = tf.math.reduce_prod(returns_future)**(1/(n-i-1))\n",
    "\n",
    "        # 예측 기하 평균 수익률과 기대 기하 평균 수익률의 MSE\n",
    "        value = tf.cast(model2(state, training=True)[0, 0], dtype=tf.float64)\n",
    "        losses.append((value - avg_return_future)**2)\n",
    "        if n == 3: break\n",
    "    print(\"losses\", losses)\n",
    "    print(\"value\", value)    \n",
    "    with tf.GradientTape() as tape:\n",
    "        gradients2 = tape.gradient(tf.math.reduce_sum(losses), model2.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients2, model2.trainable_variables))\n",
    "    \n",
    "\n",
    "    \n",
    "    if iteration % 100 == 0:\n",
    "        #test_MSE = model.evaluate(test_x, test_y)\n",
    "        prediction = model1.predict(test_x)\n",
    "        prediction_MSE = sum((updown - np.sign(prediction[:, -1, 0]).reshape(-1))**2)/len(test_y)\n",
    "        print('prediction_MSE =', prediction_MSE)\n",
    "\n",
    "    if iteration > epochs / 2 and early_stopping.validate(prediction_MSE)==True:\n",
    "        break\n",
    "epochs -= epochs / 5\n",
    "if epochs <= 0: epochs = 100\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow)",
   "language": "python",
   "name": "conda_tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
