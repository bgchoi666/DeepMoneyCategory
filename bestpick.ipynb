{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 Bimghi Choi. All Rights Reserved.\n",
    "# 예측 + 투자전략 시스템\n",
    "\n",
    "# _*_ coding: utf-8 _*_\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import os\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]='0, 1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import preprocess as prepro\n",
    "import models\n",
    "import learn\n",
    "from learn import GenerateResult\n",
    "import math\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # 텐서플로가 첫 번째 GPU에 1GB 메모리만 할당하도록 제한\n",
    "  try:\n",
    "#    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')    \n",
    "    tf.config.experimental.set_visible_devices(gpus[1], 'GPU')    \n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[1],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=10240)])\n",
    "  except RuntimeError as e:\n",
    "    # 프로그램 시작시에 가장 장치가 설정되어야만 합니다\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bayesian opt로 최적화된 parameter\n",
    "학습기간 2000-01-31 ~ 2017-01-02\n",
    "test기간 2017-01-02 ~ 2029-05-15\n",
    "\n",
    "2일 예측을 1일 예측으로 사용\n",
    "\n",
    "steps 30, interval 1, units 1024, batch size 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '~/Data/kospi200f_809_0515.csv'\n",
    "item_name = 'kospi200f_809_0515'\n",
    "train_start = '2000-01-31'\n",
    "train_end = '2017-01-02'\n",
    "test_start = '2017-01-02'\n",
    "test_end = '2020-05-15'\n",
    "\n",
    "remove_columns = ['date', '종가']\n",
    "target_column = '종가'\n",
    "input_columns = []\n",
    "target_type = 'rate'\n",
    "\n",
    "model_name = 'bestpick'\n",
    "channel = False\n",
    "\n",
    "trans_day = 1\n",
    "\n",
    "target_alpha = 100\n",
    "future_day = 2\n",
    "train_end_back = -1\n",
    "n_timestep = 30\n",
    "time_interval = 2\n",
    "input_size = 809\n",
    "n_unit = 1024\n",
    "batch_size = 10\n",
    "learning_rate = 0.0005\n",
    "epochs = 2000\n",
    "\n",
    "alpha = 0.5\n",
    "beta = 0.1\n",
    "\n",
    "comment = \"2일 에측을 위해 1일 예측 학습, trainset을 2개로 shuffle로 분리, 그 중 하나만 예측 모델로 학습, 50번 반목하여 best pick, time_interval = 1, 2, 3 병합, loss=model1_1\"\n",
    "best_pick_iter = 50\n",
    "\n",
    "from datetime import datetime\n",
    "now = datetime.now().strftime('%Y-%m-%d-%H:%M:%S')\n",
    "checkpoint_path = model_name + \"/pred\"+str(future_day)+\":\"+now+\".ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = util.read_datafile(file_name)\n",
    "df = dataframe.copy()\n",
    "df['close'] = df[target_column] # 종가 column 추가, 지존 종가는 rate으로 변환 예정\n",
    "\n",
    "#df = prepro.target_conversion(df, target_column, future_day, type=target_type)\n",
    "a = []\n",
    "b = []\n",
    "for i in range(len(df[target_column]) -future_day):\n",
    "    df.loc[i, target_column] = ((df.loc[i + future_day, target_column] - df.loc[i, target_column]) \n",
    "                                / df.loc[i, target_column]) * target_alpha \n",
    "    df.loc[i, '시가'] = df.loc[i + future_day + train_end_back + 1, '시가']\n",
    "    a.append(max(df.loc[i  + future_day + train_end_back + 1:i + future_day + 1, '고가']))\n",
    "    b.append(min(df.loc[i  + future_day + train_end_back + 1:i + future_day + 1, '저가']))\n",
    "for i in range(len(df['종가']) - future_day):\n",
    "    df.loc[i, '고가'] = a[i]\n",
    "    df.loc[i, '저가'] = b[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>시가지수(포인트)</th>\n",
       "      <th>고가지수(포인트)</th>\n",
       "      <th>저가지수(포인트)</th>\n",
       "      <th>종가지수(포인트)</th>\n",
       "      <th>수익률(%)</th>\n",
       "      <th>수익률 (1주)(%)</th>\n",
       "      <th>수익률 (1개월)(%)</th>\n",
       "      <th>수익률 (3개월)(%)</th>\n",
       "      <th>수익률 (6개월)(%)</th>\n",
       "      <th>...</th>\n",
       "      <th>주요상품선물_금(선물)($/ounce)</th>\n",
       "      <th>주요상품선물_은(선물)($/ounce)</th>\n",
       "      <th>주요상품선물_알루미늄(선물)($/ton)</th>\n",
       "      <th>주요상품선물_옥수수(최근월물)(￠/bu)</th>\n",
       "      <th>대두박(￠/bu)</th>\n",
       "      <th>종가</th>\n",
       "      <th>시가</th>\n",
       "      <th>고가</th>\n",
       "      <th>저가</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-31</td>\n",
       "      <td>-0.218225</td>\n",
       "      <td>-0.493106</td>\n",
       "      <td>-0.400439</td>\n",
       "      <td>-0.223565</td>\n",
       "      <td>0.212872</td>\n",
       "      <td>1.352924</td>\n",
       "      <td>-0.851790</td>\n",
       "      <td>-0.382658</td>\n",
       "      <td>-0.162615</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.853010</td>\n",
       "      <td>0.974513</td>\n",
       "      <td>0.827681</td>\n",
       "      <td>0.373581</td>\n",
       "      <td>0.614542</td>\n",
       "      <td>-0.961538</td>\n",
       "      <td>116.55</td>\n",
       "      <td>120.25</td>\n",
       "      <td>115.95</td>\n",
       "      <td>119.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-02-01</td>\n",
       "      <td>-0.213200</td>\n",
       "      <td>-0.230417</td>\n",
       "      <td>-0.373954</td>\n",
       "      <td>-0.595373</td>\n",
       "      <td>-0.407635</td>\n",
       "      <td>1.808646</td>\n",
       "      <td>-1.074203</td>\n",
       "      <td>-1.707405</td>\n",
       "      <td>-0.490731</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039689</td>\n",
       "      <td>0.193571</td>\n",
       "      <td>1.002058</td>\n",
       "      <td>0.594935</td>\n",
       "      <td>0.706918</td>\n",
       "      <td>3.381014</td>\n",
       "      <td>119.35</td>\n",
       "      <td>123.45</td>\n",
       "      <td>116.80</td>\n",
       "      <td>115.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-02-02</td>\n",
       "      <td>-0.208520</td>\n",
       "      <td>-0.448886</td>\n",
       "      <td>-0.367787</td>\n",
       "      <td>-0.200398</td>\n",
       "      <td>0.696677</td>\n",
       "      <td>2.107257</td>\n",
       "      <td>-0.769928</td>\n",
       "      <td>-1.558400</td>\n",
       "      <td>0.074389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.800355</td>\n",
       "      <td>0.031266</td>\n",
       "      <td>0.069626</td>\n",
       "      <td>0.430159</td>\n",
       "      <td>0.725768</td>\n",
       "      <td>3.292528</td>\n",
       "      <td>120.40</td>\n",
       "      <td>123.60</td>\n",
       "      <td>119.75</td>\n",
       "      <td>118.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-02-03</td>\n",
       "      <td>-0.204125</td>\n",
       "      <td>-0.216603</td>\n",
       "      <td>-0.062403</td>\n",
       "      <td>-0.021445</td>\n",
       "      <td>0.366314</td>\n",
       "      <td>1.488978</td>\n",
       "      <td>-0.625155</td>\n",
       "      <td>-1.268930</td>\n",
       "      <td>-0.079967</td>\n",
       "      <td>...</td>\n",
       "      <td>1.515992</td>\n",
       "      <td>0.623370</td>\n",
       "      <td>0.378121</td>\n",
       "      <td>0.084769</td>\n",
       "      <td>0.352875</td>\n",
       "      <td>1.677149</td>\n",
       "      <td>122.90</td>\n",
       "      <td>124.10</td>\n",
       "      <td>120.95</td>\n",
       "      <td>119.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-02-07</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.378008</td>\n",
       "      <td>0.426748</td>\n",
       "      <td>0.592789</td>\n",
       "      <td>0.949260</td>\n",
       "      <td>1.101574</td>\n",
       "      <td>1.281908</td>\n",
       "      <td>-1.288431</td>\n",
       "      <td>1.273730</td>\n",
       "      <td>...</td>\n",
       "      <td>3.856851</td>\n",
       "      <td>0.568635</td>\n",
       "      <td>-0.322023</td>\n",
       "      <td>0.518339</td>\n",
       "      <td>0.702038</td>\n",
       "      <td>0.653862</td>\n",
       "      <td>122.40</td>\n",
       "      <td>125.05</td>\n",
       "      <td>120.40</td>\n",
       "      <td>122.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2000-02-08</td>\n",
       "      <td>-0.196110</td>\n",
       "      <td>0.353748</td>\n",
       "      <td>0.677387</td>\n",
       "      <td>0.254527</td>\n",
       "      <td>-0.366454</td>\n",
       "      <td>1.138729</td>\n",
       "      <td>0.998930</td>\n",
       "      <td>-1.329397</td>\n",
       "      <td>0.942469</td>\n",
       "      <td>...</td>\n",
       "      <td>2.657684</td>\n",
       "      <td>1.222974</td>\n",
       "      <td>-0.720402</td>\n",
       "      <td>0.474125</td>\n",
       "      <td>0.440887</td>\n",
       "      <td>1.319588</td>\n",
       "      <td>120.85</td>\n",
       "      <td>125.05</td>\n",
       "      <td>119.85</td>\n",
       "      <td>121.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2000-02-09</td>\n",
       "      <td>-0.192447</td>\n",
       "      <td>0.306497</td>\n",
       "      <td>0.819824</td>\n",
       "      <td>0.658684</td>\n",
       "      <td>0.641075</td>\n",
       "      <td>1.076862</td>\n",
       "      <td>1.233568</td>\n",
       "      <td>-1.637689</td>\n",
       "      <td>0.994675</td>\n",
       "      <td>...</td>\n",
       "      <td>3.026664</td>\n",
       "      <td>2.114708</td>\n",
       "      <td>-0.407511</td>\n",
       "      <td>0.622228</td>\n",
       "      <td>0.721716</td>\n",
       "      <td>-1.867641</td>\n",
       "      <td>123.60</td>\n",
       "      <td>124.75</td>\n",
       "      <td>115.25</td>\n",
       "      <td>123.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000-02-10</td>\n",
       "      <td>0.330923</td>\n",
       "      <td>0.617187</td>\n",
       "      <td>0.552762</td>\n",
       "      <td>0.366383</td>\n",
       "      <td>-0.311193</td>\n",
       "      <td>0.651653</td>\n",
       "      <td>0.275024</td>\n",
       "      <td>-1.607973</td>\n",
       "      <td>0.269679</td>\n",
       "      <td>...</td>\n",
       "      <td>3.316674</td>\n",
       "      <td>1.871659</td>\n",
       "      <td>-0.157431</td>\n",
       "      <td>0.985272</td>\n",
       "      <td>1.379493</td>\n",
       "      <td>-5.575906</td>\n",
       "      <td>118.85</td>\n",
       "      <td>119.00</td>\n",
       "      <td>110.45</td>\n",
       "      <td>122.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2000-02-11</td>\n",
       "      <td>0.977388</td>\n",
       "      <td>0.576306</td>\n",
       "      <td>0.048738</td>\n",
       "      <td>-0.013448</td>\n",
       "      <td>-0.428351</td>\n",
       "      <td>0.326747</td>\n",
       "      <td>0.131530</td>\n",
       "      <td>-1.828185</td>\n",
       "      <td>0.546650</td>\n",
       "      <td>...</td>\n",
       "      <td>2.384344</td>\n",
       "      <td>0.988933</td>\n",
       "      <td>-0.154629</td>\n",
       "      <td>0.473525</td>\n",
       "      <td>0.907189</td>\n",
       "      <td>-8.274721</td>\n",
       "      <td>116.45</td>\n",
       "      <td>116.80</td>\n",
       "      <td>105.25</td>\n",
       "      <td>120.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2000-02-14</td>\n",
       "      <td>-0.649462</td>\n",
       "      <td>-0.806483</td>\n",
       "      <td>-0.954529</td>\n",
       "      <td>-1.222378</td>\n",
       "      <td>-1.533955</td>\n",
       "      <td>-1.217328</td>\n",
       "      <td>-0.075927</td>\n",
       "      <td>-2.486079</td>\n",
       "      <td>0.140712</td>\n",
       "      <td>...</td>\n",
       "      <td>1.935983</td>\n",
       "      <td>0.085505</td>\n",
       "      <td>-0.246361</td>\n",
       "      <td>0.529501</td>\n",
       "      <td>0.923675</td>\n",
       "      <td>-1.422414</td>\n",
       "      <td>112.05</td>\n",
       "      <td>114.35</td>\n",
       "      <td>105.25</td>\n",
       "      <td>116.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2000-02-15</td>\n",
       "      <td>-1.074607</td>\n",
       "      <td>-1.359121</td>\n",
       "      <td>-1.825901</td>\n",
       "      <td>-1.959771</td>\n",
       "      <td>-1.104848</td>\n",
       "      <td>-1.601324</td>\n",
       "      <td>-0.717085</td>\n",
       "      <td>-2.587555</td>\n",
       "      <td>-0.669097</td>\n",
       "      <td>...</td>\n",
       "      <td>1.222183</td>\n",
       "      <td>-0.051403</td>\n",
       "      <td>-1.024949</td>\n",
       "      <td>0.518408</td>\n",
       "      <td>1.160291</td>\n",
       "      <td>1.623816</td>\n",
       "      <td>112.35</td>\n",
       "      <td>115.70</td>\n",
       "      <td>109.95</td>\n",
       "      <td>110.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2000-02-16</td>\n",
       "      <td>-1.939741</td>\n",
       "      <td>-2.072462</td>\n",
       "      <td>-2.426875</td>\n",
       "      <td>-1.820694</td>\n",
       "      <td>0.171634</td>\n",
       "      <td>-1.786449</td>\n",
       "      <td>-0.703717</td>\n",
       "      <td>-2.563710</td>\n",
       "      <td>-0.401469</td>\n",
       "      <td>...</td>\n",
       "      <td>1.251580</td>\n",
       "      <td>0.487841</td>\n",
       "      <td>-1.605690</td>\n",
       "      <td>0.609070</td>\n",
       "      <td>0.911960</td>\n",
       "      <td>-2.186270</td>\n",
       "      <td>113.35</td>\n",
       "      <td>115.70</td>\n",
       "      <td>105.95</td>\n",
       "      <td>114.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2000-02-17</td>\n",
       "      <td>-2.062374</td>\n",
       "      <td>-1.576333</td>\n",
       "      <td>-1.627765</td>\n",
       "      <td>-1.281362</td>\n",
       "      <td>0.934836</td>\n",
       "      <td>-1.133411</td>\n",
       "      <td>-0.976304</td>\n",
       "      <td>-1.810490</td>\n",
       "      <td>1.006196</td>\n",
       "      <td>...</td>\n",
       "      <td>1.099126</td>\n",
       "      <td>0.014284</td>\n",
       "      <td>-1.379601</td>\n",
       "      <td>0.356659</td>\n",
       "      <td>1.165770</td>\n",
       "      <td>-5.281846</td>\n",
       "      <td>109.80</td>\n",
       "      <td>109.80</td>\n",
       "      <td>105.50</td>\n",
       "      <td>112.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2000-02-18</td>\n",
       "      <td>-1.203728</td>\n",
       "      <td>-1.119424</td>\n",
       "      <td>-1.312861</td>\n",
       "      <td>-1.662198</td>\n",
       "      <td>-0.622664</td>\n",
       "      <td>-1.227625</td>\n",
       "      <td>-1.271035</td>\n",
       "      <td>-1.891012</td>\n",
       "      <td>0.605513</td>\n",
       "      <td>...</td>\n",
       "      <td>1.361637</td>\n",
       "      <td>0.227203</td>\n",
       "      <td>-1.264041</td>\n",
       "      <td>0.245730</td>\n",
       "      <td>1.381693</td>\n",
       "      <td>-4.112651</td>\n",
       "      <td>106.85</td>\n",
       "      <td>110.85</td>\n",
       "      <td>105.50</td>\n",
       "      <td>111.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2000-02-21</td>\n",
       "      <td>-2.011534</td>\n",
       "      <td>-2.360013</td>\n",
       "      <td>-2.117703</td>\n",
       "      <td>-2.493118</td>\n",
       "      <td>-1.243495</td>\n",
       "      <td>-1.039810</td>\n",
       "      <td>-0.905187</td>\n",
       "      <td>-2.286667</td>\n",
       "      <td>-0.665131</td>\n",
       "      <td>...</td>\n",
       "      <td>1.274963</td>\n",
       "      <td>0.268968</td>\n",
       "      <td>-1.589226</td>\n",
       "      <td>0.185194</td>\n",
       "      <td>1.319954</td>\n",
       "      <td>3.608247</td>\n",
       "      <td>108.55</td>\n",
       "      <td>111.60</td>\n",
       "      <td>107.10</td>\n",
       "      <td>106.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2000-02-22</td>\n",
       "      <td>-2.301061</td>\n",
       "      <td>-2.151107</td>\n",
       "      <td>-2.119718</td>\n",
       "      <td>-2.142064</td>\n",
       "      <td>0.422501</td>\n",
       "      <td>-0.214320</td>\n",
       "      <td>-0.740215</td>\n",
       "      <td>-1.957417</td>\n",
       "      <td>-0.444290</td>\n",
       "      <td>...</td>\n",
       "      <td>1.230409</td>\n",
       "      <td>0.364772</td>\n",
       "      <td>-1.172751</td>\n",
       "      <td>-0.103243</td>\n",
       "      <td>1.100675</td>\n",
       "      <td>1.491841</td>\n",
       "      <td>110.20</td>\n",
       "      <td>111.60</td>\n",
       "      <td>106.50</td>\n",
       "      <td>107.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2000-02-23</td>\n",
       "      <td>-1.670887</td>\n",
       "      <td>-1.656883</td>\n",
       "      <td>-1.569320</td>\n",
       "      <td>-1.267786</td>\n",
       "      <td>1.605126</td>\n",
       "      <td>0.506204</td>\n",
       "      <td>0.050075</td>\n",
       "      <td>-1.494619</td>\n",
       "      <td>-0.699133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.708148</td>\n",
       "      <td>-0.052186</td>\n",
       "      <td>-0.980503</td>\n",
       "      <td>-1.014657</td>\n",
       "      <td>0.126165</td>\n",
       "      <td>-1.492537</td>\n",
       "      <td>107.75</td>\n",
       "      <td>109.85</td>\n",
       "      <td>101.95</td>\n",
       "      <td>110.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2000-02-24</td>\n",
       "      <td>-1.366238</td>\n",
       "      <td>-1.422324</td>\n",
       "      <td>-1.358331</td>\n",
       "      <td>-1.490032</td>\n",
       "      <td>-0.528189</td>\n",
       "      <td>-0.294080</td>\n",
       "      <td>-0.319013</td>\n",
       "      <td>-1.472892</td>\n",
       "      <td>-1.661528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.530452</td>\n",
       "      <td>-1.233068</td>\n",
       "      <td>-0.941193</td>\n",
       "      <td>-0.976769</td>\n",
       "      <td>0.385420</td>\n",
       "      <td>-6.017455</td>\n",
       "      <td>105.95</td>\n",
       "      <td>106.55</td>\n",
       "      <td>100.75</td>\n",
       "      <td>108.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2000-02-25</td>\n",
       "      <td>-1.680249</td>\n",
       "      <td>-1.665735</td>\n",
       "      <td>-1.510077</td>\n",
       "      <td>-1.461250</td>\n",
       "      <td>0.033092</td>\n",
       "      <td>0.075897</td>\n",
       "      <td>0.493437</td>\n",
       "      <td>-1.430882</td>\n",
       "      <td>-2.424417</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081697</td>\n",
       "      <td>-2.561532</td>\n",
       "      <td>-1.090127</td>\n",
       "      <td>0.174516</td>\n",
       "      <td>-0.148781</td>\n",
       "      <td>-5.463728</td>\n",
       "      <td>104.15</td>\n",
       "      <td>113.00</td>\n",
       "      <td>100.75</td>\n",
       "      <td>108.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2000-02-28</td>\n",
       "      <td>-1.697353</td>\n",
       "      <td>-2.045428</td>\n",
       "      <td>-2.011407</td>\n",
       "      <td>-2.307633</td>\n",
       "      <td>-1.966604</td>\n",
       "      <td>-0.289909</td>\n",
       "      <td>-1.729787</td>\n",
       "      <td>-1.563181</td>\n",
       "      <td>-3.097790</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.149500</td>\n",
       "      <td>-2.235868</td>\n",
       "      <td>-2.265504</td>\n",
       "      <td>-0.179609</td>\n",
       "      <td>0.061992</td>\n",
       "      <td>9.921799</td>\n",
       "      <td>106.50</td>\n",
       "      <td>113.75</td>\n",
       "      <td>106.35</td>\n",
       "      <td>102.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 815 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  시가지수(포인트)  고가지수(포인트)  저가지수(포인트)  종가지수(포인트)    수익률(%)  \\\n",
       "0   2000-01-31  -0.218225  -0.493106  -0.400439  -0.223565  0.212872   \n",
       "1   2000-02-01  -0.213200  -0.230417  -0.373954  -0.595373 -0.407635   \n",
       "2   2000-02-02  -0.208520  -0.448886  -0.367787  -0.200398  0.696677   \n",
       "3   2000-02-03  -0.204125  -0.216603  -0.062403  -0.021445  0.366314   \n",
       "4   2000-02-07  -0.200000   0.378008   0.426748   0.592789  0.949260   \n",
       "5   2000-02-08  -0.196110   0.353748   0.677387   0.254527 -0.366454   \n",
       "6   2000-02-09  -0.192447   0.306497   0.819824   0.658684  0.641075   \n",
       "7   2000-02-10   0.330923   0.617187   0.552762   0.366383 -0.311193   \n",
       "8   2000-02-11   0.977388   0.576306   0.048738  -0.013448 -0.428351   \n",
       "9   2000-02-14  -0.649462  -0.806483  -0.954529  -1.222378 -1.533955   \n",
       "10  2000-02-15  -1.074607  -1.359121  -1.825901  -1.959771 -1.104848   \n",
       "11  2000-02-16  -1.939741  -2.072462  -2.426875  -1.820694  0.171634   \n",
       "12  2000-02-17  -2.062374  -1.576333  -1.627765  -1.281362  0.934836   \n",
       "13  2000-02-18  -1.203728  -1.119424  -1.312861  -1.662198 -0.622664   \n",
       "14  2000-02-21  -2.011534  -2.360013  -2.117703  -2.493118 -1.243495   \n",
       "15  2000-02-22  -2.301061  -2.151107  -2.119718  -2.142064  0.422501   \n",
       "16  2000-02-23  -1.670887  -1.656883  -1.569320  -1.267786  1.605126   \n",
       "17  2000-02-24  -1.366238  -1.422324  -1.358331  -1.490032 -0.528189   \n",
       "18  2000-02-25  -1.680249  -1.665735  -1.510077  -1.461250  0.033092   \n",
       "19  2000-02-28  -1.697353  -2.045428  -2.011407  -2.307633 -1.966604   \n",
       "\n",
       "    수익률 (1주)(%)  수익률 (1개월)(%)  수익률 (3개월)(%)  수익률 (6개월)(%)  ...  \\\n",
       "0      1.352924     -0.851790     -0.382658     -0.162615  ...   \n",
       "1      1.808646     -1.074203     -1.707405     -0.490731  ...   \n",
       "2      2.107257     -0.769928     -1.558400      0.074389  ...   \n",
       "3      1.488978     -0.625155     -1.268930     -0.079967  ...   \n",
       "4      1.101574      1.281908     -1.288431      1.273730  ...   \n",
       "5      1.138729      0.998930     -1.329397      0.942469  ...   \n",
       "6      1.076862      1.233568     -1.637689      0.994675  ...   \n",
       "7      0.651653      0.275024     -1.607973      0.269679  ...   \n",
       "8      0.326747      0.131530     -1.828185      0.546650  ...   \n",
       "9     -1.217328     -0.075927     -2.486079      0.140712  ...   \n",
       "10    -1.601324     -0.717085     -2.587555     -0.669097  ...   \n",
       "11    -1.786449     -0.703717     -2.563710     -0.401469  ...   \n",
       "12    -1.133411     -0.976304     -1.810490      1.006196  ...   \n",
       "13    -1.227625     -1.271035     -1.891012      0.605513  ...   \n",
       "14    -1.039810     -0.905187     -2.286667     -0.665131  ...   \n",
       "15    -0.214320     -0.740215     -1.957417     -0.444290  ...   \n",
       "16     0.506204      0.050075     -1.494619     -0.699133  ...   \n",
       "17    -0.294080     -0.319013     -1.472892     -1.661528  ...   \n",
       "18     0.075897      0.493437     -1.430882     -2.424417  ...   \n",
       "19    -0.289909     -1.729787     -1.563181     -3.097790  ...   \n",
       "\n",
       "    주요상품선물_금(선물)($/ounce)  주요상품선물_은(선물)($/ounce)  주요상품선물_알루미늄(선물)($/ton)  \\\n",
       "0               -0.853010               0.974513                0.827681   \n",
       "1               -0.039689               0.193571                1.002058   \n",
       "2                0.800355               0.031266                0.069626   \n",
       "3                1.515992               0.623370                0.378121   \n",
       "4                3.856851               0.568635               -0.322023   \n",
       "5                2.657684               1.222974               -0.720402   \n",
       "6                3.026664               2.114708               -0.407511   \n",
       "7                3.316674               1.871659               -0.157431   \n",
       "8                2.384344               0.988933               -0.154629   \n",
       "9                1.935983               0.085505               -0.246361   \n",
       "10               1.222183              -0.051403               -1.024949   \n",
       "11               1.251580               0.487841               -1.605690   \n",
       "12               1.099126               0.014284               -1.379601   \n",
       "13               1.361637               0.227203               -1.264041   \n",
       "14               1.274963               0.268968               -1.589226   \n",
       "15               1.230409               0.364772               -1.172751   \n",
       "16               0.708148              -0.052186               -0.980503   \n",
       "17               0.530452              -1.233068               -0.941193   \n",
       "18              -0.081697              -2.561532               -1.090127   \n",
       "19              -0.149500              -2.235868               -2.265504   \n",
       "\n",
       "    주요상품선물_옥수수(최근월물)(￠/bu)  대두박(￠/bu)        종가      시가      고가      저가  \\\n",
       "0                 0.373581   0.614542 -0.961538  116.55  120.25  115.95   \n",
       "1                 0.594935   0.706918  3.381014  119.35  123.45  116.80   \n",
       "2                 0.430159   0.725768  3.292528  120.40  123.60  119.75   \n",
       "3                 0.084769   0.352875  1.677149  122.90  124.10  120.95   \n",
       "4                 0.518339   0.702038  0.653862  122.40  125.05  120.40   \n",
       "5                 0.474125   0.440887  1.319588  120.85  125.05  119.85   \n",
       "6                 0.622228   0.721716 -1.867641  123.60  124.75  115.25   \n",
       "7                 0.985272   1.379493 -5.575906  118.85  119.00  110.45   \n",
       "8                 0.473525   0.907189 -8.274721  116.45  116.80  105.25   \n",
       "9                 0.529501   0.923675 -1.422414  112.05  114.35  105.25   \n",
       "10                0.518408   1.160291  1.623816  112.35  115.70  109.95   \n",
       "11                0.609070   0.911960 -2.186270  113.35  115.70  105.95   \n",
       "12                0.356659   1.165770 -5.281846  109.80  109.80  105.50   \n",
       "13                0.245730   1.381693 -4.112651  106.85  110.85  105.50   \n",
       "14                0.185194   1.319954  3.608247  108.55  111.60  107.10   \n",
       "15               -0.103243   1.100675  1.491841  110.20  111.60  106.50   \n",
       "16               -1.014657   0.126165 -1.492537  107.75  109.85  101.95   \n",
       "17               -0.976769   0.385420 -6.017455  105.95  106.55  100.75   \n",
       "18                0.174516  -0.148781 -5.463728  104.15  113.00  100.75   \n",
       "19               -0.179609   0.061992  9.921799  106.50  113.75  106.35   \n",
       "\n",
       "     close  \n",
       "0   119.60  \n",
       "1   115.35  \n",
       "2   118.45  \n",
       "3   119.25  \n",
       "4   122.35  \n",
       "5   121.25  \n",
       "6   123.15  \n",
       "7   122.85  \n",
       "8   120.85  \n",
       "9   116.00  \n",
       "10  110.85  \n",
       "11  114.35  \n",
       "12  112.65  \n",
       "13  111.85  \n",
       "14  106.70  \n",
       "15  107.25  \n",
       "16  110.55  \n",
       "17  108.85  \n",
       "18  108.90  \n",
       "19  102.30  \n",
       "\n",
       "[20 rows x 815 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', '시가지수(포인트)', '고가지수(포인트)', '저가지수(포인트)', '종가지수(포인트)', '수익률(%)',\n",
       "       '수익률 (1주)(%)', '수익률 (1개월)(%)', '수익률 (3개월)(%)', '수익률 (6개월)(%)',\n",
       "       ...\n",
       "       '주요상품선물_금(선물)($/ounce)', '주요상품선물_은(선물)($/ounce)',\n",
       "       '주요상품선물_알루미늄(선물)($/ton)', '주요상품선물_옥수수(최근월물)(￠/bu)', '대두박(￠/bu)', '종가',\n",
       "       '시가', '고가', '저가', 'close'],\n",
       "      dtype='object', length=815)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_list = df.columns\n",
    "column_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas_profiling\n",
    "#data = df.loc[:, ['미국 SP 500 Index(종가)(Pt)', '거래량(주)', '외국인보유비중(%)', '종가']]\n",
    "#data = data[:-future_day]\n",
    "#pr = data.profile_report()\n",
    "#pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start_index = max(df.loc[df['date']<=train_start].index) + time_interval*(n_timestep-1) - 1\n",
    "train_end_index = max(df.loc[df['date']<=train_end].index)\n",
    "base_prices = tf.reduce_mean(df.loc[train_start_index:train_end_index+1, '시가'])  \n",
    "\n",
    "@tf.function\n",
    "def loss_fn_model1_1(targets, preds):\n",
    " \n",
    "    loss0 = tf.keras.losses.MSE(targets, preds)\n",
    "    \n",
    "    preds = tf.reshape(preds[:, n_timestep-1, :], [-1])\n",
    "    targets = tf.reshape(targets[:, n_timestep-1, :], [-1])\n",
    "    \n",
    "   \n",
    "    if alpha != 0:\n",
    "        # add RRL cost - maximize downside sharp ratio\n",
    "\n",
    "        # 1 if (pred - base) * (target - base) > 0, -1 otherwise\n",
    "        F = tf.math.sign(targets*preds)\n",
    "        F = tf.reshape(F, [-1])\n",
    "\n",
    "        # calc returns from each step in batches\n",
    "        R = tf.math.divide(tf.math.multiply(tf.math.abs(targets), (F - 0.00003)), base_prices)\n",
    "        R = tf.reshape(R, [-1])\n",
    "\n",
    "        # calc downside sharp ratio\n",
    "\n",
    "        # downside returns\n",
    "        DR = tf.minimum(0.0, R)\n",
    "        DR = tf.reshape(DR, [-1])\n",
    "\n",
    "        # calc. downside sharp ratio\n",
    "        #s = []\n",
    "        #for i in range(batch_size):\n",
    "        #   std =  tf.keras.backend.std(DR[i, :, 0])\n",
    "        #   s.append(tf.reduce_mean(R[i, :, 0])/tf.maximum(0.01, std))\n",
    "\n",
    "        # calc. downside sharp ratio\n",
    "        loss1 = tf.reduce_mean(R) / (tf.keras.backend.std(DR) + 0.001)\n",
    "    else:\n",
    "        loss1 = 0\n",
    "\n",
    "    \"\"\"\n",
    "    # average profits, loss\n",
    "    avg_plusR = [0.0]\n",
    "    avg_minusR = [0.0]\n",
    "\n",
    "    global num_of_profits\n",
    "    global num_of_losses\n",
    "\n",
    "    num_of_profits = 0\n",
    "    num_of_losses = 0\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        res = tf.cond(R[i, num_steps - 1, 0] > 0, lambda: return_one(), lambda: return_zero())\n",
    "        if res == 1:\n",
    "            avg_plusR.append(R[i, num_steps - 1, 0])\n",
    "        else:\n",
    "            avg_minusR.append(R[i, num_steps - 1, 0])\n",
    "    avg_profit = tf.reduce_mean(avg_plusR) \n",
    "    avg_loss = tf.reduce_mean(avg_minusR) \n",
    "    \"\"\"\n",
    "\n",
    "    if beta != 0:\n",
    "        #compute maximum drawdown\n",
    "\n",
    "        #accm_profit = [0.0]\n",
    "        #for i in range(batch_size):\n",
    "        #    for j in range(num_steps):\n",
    "        #        r = tf.cond((predict_prices[i, num_steps-1, 0] - base_prices[i, num_steps-1, 0]) *\n",
    "        #                   (target_prices[i, num_steps-1, 0] - base_prices[i, num_steps-1, 0]) > 0,\n",
    "        #                   lambda: return_one(),\n",
    "        #                   lambda: return_zero())\n",
    "        #        if r == 1: accm_profit.append(accm_profit[i*num_steps + j] + tf.abs(target_prices[i, j, 0] - base_prices[i, j, 0]))\n",
    "        #        else:      accm_profit.append(accm_profit[i*num_steps + j] - tf.abs(target_prices[i, j, 0] - base_prices[i, j, 0]))\n",
    "\n",
    "        accm_profit = [0.0 for i in range(batch_size)]\n",
    "        for i in range(batch_size):\n",
    "            if i == 0:\n",
    "                accm_profit[0] = tf.sign(preds * targets) * tf.math.abs(targets)\n",
    "            else:\n",
    "                accm_profit[i] = accm_profit[i-1] + tf.sign(preds * targets) * tf.math.abs(targets)\n",
    "        loss2 = (tf.reduce_max(accm_profit) - tf.reduce_min(accm_profit))/batch_size\n",
    "    else:\n",
    "        loss2 = 0\n",
    "\n",
    "    return loss0 + beta*loss2 - alpha*loss1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "손절이익이 최대가 되도록 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def loss_fn_model2(m1, m2, train_x, train_y):\n",
    "    \n",
    "    train_y_target = np.expand_dims(train_y[:, :, 4], axis=2)\n",
    "    \n",
    "    rates = train_y_target / 100\n",
    "    updown = tf.math.sign(train_y_target)\n",
    "    preds = tf.cast(tf.math.sign(m1(train_x)), dtype=tf.float64)\n",
    "    profits = tf.cast(1 + tf.convert_to_tensor(rates, dtype=tf.float64)*preds, dtype=tf.float64)\n",
    "    \n",
    "    batches = tf.cast(train_y.shape[0], dtype=tf.int32)\n",
    "    steps = train_y.shape[1]     \n",
    "    \n",
    "    # 시가, 고가, 저가\n",
    "    train_open = np.expand_dims(train_y[:, :, 0], axis=2)   \n",
    "    train_high = np.expand_dims(train_y[:, :, 1], axis=2)\n",
    "    train_low = np.expand_dims(train_y[:, :, 2], axis=2)\n",
    "    train_base = np.expand_dims(train_y[:, :, 3], axis=2)\n",
    "    train_real = train_base * (rates + 1)\n",
    "    \n",
    "    targets = train_y_target.copy()\n",
    "    targets[:, -max(1, int(future_day/time_interval)):, :] = 0\n",
    "    train_x_m2 = tf.concat([preds, targets, train_open, train_high, train_low], 2)\n",
    "    loss_cuts = tf.cast(tf.reshape(m2(train_x_m2)[:, -1, 0], [-1]), dtype=tf.float64)\n",
    "    \n",
    "    # 손절 손익 계산\n",
    "    loss_cut_targets = []\n",
    "    for i in range(batches):\n",
    "\n",
    "        if preds[i, -1, 0] > 0:\n",
    "            if rates[i, -1, 0] > 0: \n",
    "                loss_cut_targets.append(train_open[i, -1, 0] - train_low[i, -1, 0])\n",
    "            else:\n",
    "                loss_cut_targets.append((train_base[i, -1, 0] - train_real[i, -1, 0])*0.5)\n",
    "        else:\n",
    "            if rates[i, -1, 0] > 0: \n",
    "                loss_cut_targets.append((train_real[i, -1, 0] - train_base[i, -1, 0])*0.5)\n",
    "            else:\n",
    "                loss_cut_targets.append(train_high[i, -1, 0] - train_open[i, -1, 0])\n",
    "                    \n",
    "    return tf.keras.losses.MSE(loss_cuts, loss_cut_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def gradient2(model1, model2, input_data, output_data):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn_model2(model1, model2, input_data, output_data)\n",
    "    return tape.gradient(loss, model2.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel1 = tf.keras.Sequential([\\n    tf.keras.Input(shape=(n_timestep, input_size)),\\n    tf.keras.layers.LSTM(n_unit, return_sequences=True),\\n    tf.keras.layers.LSTM(n_unit, return_sequences=True),    \\n    tf.keras.layers.Dense(64, activation='relu'),\\n    tf.keras.layers.Dense(1)\\n])\\nmodel1.summary()\\n\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#strategy = tf.distribute.MirroredStrategy()\n",
    "#with strategy.scope():\n",
    "\"\"\"\n",
    "model1 = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(n_timestep, input_size)),\n",
    "    tf.keras.layers.LSTM(n_unit, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(n_unit, return_sequences=True),    \n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model1.summary()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 시작\n",
      "2020-06-26:00:44:27\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 0 학습 종료.......\n",
      "2020-06-26:01:24:27\n",
      "종가대비 예측확률 =  tf.Tensor(0.5327669902912622, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.5097087378640777\n",
      "시가대비 best accuracy =  0.5097087378640777\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 1 학습 종료.......\n",
      "2020-06-26:02:00:48\n",
      "종가대비 예측확률 =  tf.Tensor(0.5097087378640777, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.5\n",
      "시가대비 best accuracy =  0.5097087378640777\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 2 학습 종료.......\n",
      "2020-06-26:02:36:51\n",
      "종가대비 예측확률 =  tf.Tensor(0.5133495145631068, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.5072815533980582\n",
      "시가대비 best accuracy =  0.5097087378640777\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 3 학습 종료.......\n",
      "2020-06-26:03:13:14\n",
      "종가대비 예측확률 =  tf.Tensor(0.4987864077669903, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.49029126213592233\n",
      "시가대비 best accuracy =  0.5097087378640777\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 4 학습 종료.......\n",
      "2020-06-26:03:49:31\n",
      "종가대비 예측확률 =  tf.Tensor(0.5351941747572816, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.5182038834951457\n",
      "시가대비 best accuracy =  0.5182038834951457\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 5 학습 종료.......\n",
      "2020-06-26:04:25:39\n",
      "종가대비 예측확률 =  tf.Tensor(0.5145631067961165, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.5121359223300971\n",
      "시가대비 best accuracy =  0.5182038834951457\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 6 학습 종료.......\n",
      "2020-06-26:05:01:52\n",
      "종가대비 예측확률 =  tf.Tensor(0.4890776699029126, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.49514563106796117\n",
      "시가대비 best accuracy =  0.5182038834951457\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 7 학습 종료.......\n",
      "2020-06-26:05:38:12\n",
      "종가대비 예측확률 =  tf.Tensor(0.5303398058252428, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.5242718446601942\n",
      "시가대비 best accuracy =  0.5242718446601942\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 8 학습 종료.......\n",
      "2020-06-26:06:14:33\n",
      "종가대비 예측확률 =  tf.Tensor(0.529126213592233, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.5121359223300971\n",
      "시가대비 best accuracy =  0.5242718446601942\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 9 학습 종료.......\n",
      "2020-06-26:06:51:07\n",
      "종가대비 예측확률 =  tf.Tensor(0.5072815533980582, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.5012135922330098\n",
      "시가대비 best accuracy =  0.5242718446601942\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 10 학습 종료.......\n",
      "2020-06-26:07:27:41\n",
      "종가대비 예측확률 =  tf.Tensor(0.5169902912621359, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.4963592233009709\n",
      "시가대비 best accuracy =  0.5242718446601942\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 11 학습 종료.......\n",
      "2020-06-26:08:04:38\n",
      "종가대비 예측확률 =  tf.Tensor(0.5109223300970874, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.5206310679611651\n",
      "시가대비 best accuracy =  0.5242718446601942\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 12 학습 종료.......\n",
      "2020-06-26:08:41:41\n",
      "종가대비 예측확률 =  tf.Tensor(0.5121359223300971, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.508495145631068\n",
      "시가대비 best accuracy =  0.5242718446601942\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 13 학습 종료.......\n",
      "2020-06-26:09:20:56\n",
      "종가대비 예측확률 =  tf.Tensor(0.5133495145631068, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.508495145631068\n",
      "시가대비 best accuracy =  0.5242718446601942\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 14 학습 종료.......\n",
      "2020-06-26:10:01:29\n",
      "종가대비 예측확률 =  tf.Tensor(0.5097087378640777, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.508495145631068\n",
      "시가대비 best accuracy =  0.5242718446601942\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 15 학습 종료.......\n",
      "2020-06-26:10:42:05\n",
      "종가대비 예측확률 =  tf.Tensor(0.5121359223300971, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.4987864077669903\n",
      "시가대비 best accuracy =  0.5242718446601942\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 16 학습 종료.......\n",
      "2020-06-26:11:22:47\n",
      "종가대비 예측확률 =  tf.Tensor(0.4878640776699029, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.5315533980582524\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 17 학습 종료.......\n",
      "2020-06-26:12:03:34\n",
      "종가대비 예측확률 =  tf.Tensor(0.5230582524271845, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.508495145631068\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dates 2020-04-07~2020-04-07\n",
      "구간 18 학습 종료.......\n",
      "2020-06-26:12:44:20\n",
      "종가대비 예측확률 =  tf.Tensor(0.5012135922330098, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.5145631067961165\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 19 학습 종료.......\n",
      "2020-06-26:13:25:22\n",
      "종가대비 예측확률 =  tf.Tensor(0.5351941747572816, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.5230582524271845\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 20 학습 종료.......\n",
      "2020-06-26:14:06:54\n",
      "종가대비 예측확률 =  tf.Tensor(0.5206310679611651, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.5097087378640777\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 21 학습 종료.......\n",
      "2020-06-26:14:48:23\n",
      "종가대비 예측확률 =  tf.Tensor(0.5, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.5109223300970874\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 22 학습 종료.......\n",
      "2020-06-26:15:30:05\n",
      "종가대비 예측확률 =  tf.Tensor(0.4975728155339806, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.5060679611650486\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 23 학습 종료.......\n",
      "2020-06-26:16:11:40\n",
      "종가대비 예측확률 =  tf.Tensor(0.5194174757281553, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.508495145631068\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 24 학습 종료.......\n",
      "2020-06-26:16:53:29\n",
      "종가대비 예측확률 =  tf.Tensor(0.49150485436893204, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.5036407766990292\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 25 학습 종료.......\n",
      "2020-06-26:17:35:22\n",
      "종가대비 예측확률 =  tf.Tensor(0.5327669902912622, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.4963592233009709\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 26 학습 종료.......\n",
      "2020-06-26:18:17:07\n",
      "종가대비 예측확률 =  tf.Tensor(0.5412621359223301, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.49393203883495146\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 27 학습 종료.......\n",
      "2020-06-26:18:58:56\n",
      "종가대비 예측확률 =  tf.Tensor(0.5242718446601942, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.5194174757281553\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 28 학습 종료.......\n",
      "2020-06-26:19:40:58\n",
      "종가대비 예측확률 =  tf.Tensor(0.5133495145631068, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.5072815533980582\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 29 학습 종료.......\n",
      "2020-06-26:20:23:01\n",
      "종가대비 예측확률 =  tf.Tensor(0.5, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.4963592233009709\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 30 학습 종료.......\n",
      "2020-06-26:21:05:19\n",
      "종가대비 예측확률 =  tf.Tensor(0.4987864077669903, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.4830097087378641\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 31 학습 종료.......\n",
      "2020-06-26:21:47:43\n",
      "종가대비 예측확률 =  tf.Tensor(0.5024271844660194, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.4878640776699029\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 32 학습 종료.......\n",
      "2020-06-26:22:30:38\n",
      "종가대비 예측확률 =  tf.Tensor(0.5133495145631068, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.5060679611650486\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 33 학습 종료.......\n",
      "2020-06-26:23:12:50\n",
      "종가대비 예측확률 =  tf.Tensor(0.5060679611650486, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.5218446601941747\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 34 학습 종료.......\n",
      "2020-06-26:23:54:23\n",
      "종가대비 예측확률 =  tf.Tensor(0.5194174757281553, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.5266990291262136\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 35 학습 종료.......\n",
      "2020-06-27:00:35:56\n",
      "종가대비 예측확률 =  tf.Tensor(0.5218446601941747, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.5206310679611651\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 36 학습 종료.......\n",
      "2020-06-27:01:17:28\n",
      "종가대비 예측확률 =  tf.Tensor(0.49393203883495146, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.48058252427184467\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 37 학습 종료.......\n",
      "2020-06-27:01:59:54\n",
      "종가대비 예측확률 =  tf.Tensor(0.5097087378640777, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.49393203883495146\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 38 학습 종료.......\n",
      "2020-06-27:02:43:06\n",
      "종가대비 예측확률 =  tf.Tensor(0.5072815533980582, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.5121359223300971\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 39 학습 종료.......\n",
      "2020-06-27:03:26:32\n",
      "종가대비 예측확률 =  tf.Tensor(0.5, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.5024271844660194\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 40 학습 종료.......\n",
      "2020-06-27:04:10:35\n",
      "종가대비 예측확률 =  tf.Tensor(0.5230582524271845, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.508495145631068\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 41 학습 종료.......\n",
      "2020-06-27:04:51:50\n",
      "종가대비 예측확률 =  tf.Tensor(0.5266990291262136, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.5060679611650486\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 42 학습 종료.......\n",
      "2020-06-27:05:31:29\n",
      "종가대비 예측확률 =  tf.Tensor(0.5072815533980582, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.5145631067961165\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 43 학습 종료.......\n",
      "2020-06-27:06:11:18\n",
      "종가대비 예측확률 =  tf.Tensor(0.5048543689320388, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.5036407766990292\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 44 학습 종료.......\n",
      "2020-06-27:06:51:22\n",
      "종가대비 예측확률 =  tf.Tensor(0.4975728155339806, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.4878640776699029\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 45 학습 종료.......\n",
      "2020-06-27:07:31:25\n",
      "종가대비 예측확률 =  tf.Tensor(0.5182038834951457, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.4987864077669903\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 46 학습 종료.......\n",
      "2020-06-27:08:11:36\n",
      "종가대비 예측확률 =  tf.Tensor(0.5024271844660194, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.5024271844660194\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 47 학습 종료.......\n",
      "2020-06-27:08:51:50\n",
      "종가대비 예측확률 =  tf.Tensor(0.5230582524271845, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.5097087378640777\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 48 학습 종료.......\n",
      "2020-06-27:09:32:01\n",
      "종가대비 예측확률 =  tf.Tensor(0.5060679611650486, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.5072815533980582\n",
      "시가대비 best accuracy =  0.5315533980582524\n",
      "test dates 2017-05-30~2017-05-30\n",
      "test dates 2017-10-27~2017-10-27\n",
      "test dates 2018-03-26~2018-03-26\n",
      "test dates 2018-08-21~2018-08-21\n",
      "test dates 2019-01-18~2019-01-18\n",
      "test dates 2019-06-18~2019-06-18\n",
      "test dates 2019-11-12~2019-11-12\n",
      "test dates 2020-04-07~2020-04-07\n",
      "구간 49 학습 종료.......\n",
      "2020-06-27:10:12:21\n",
      "종가대비 예측확률 =  tf.Tensor(0.5133495145631068, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.508495145631068\n",
      "시가대비 best accuracy =  0.5315533980582524\n"
     ]
    }
   ],
   "source": [
    "print(\"학습 시작\")\n",
    "print(datetime.now().strftime(\"%Y-%m-%d:%H:%M:%S\"))\n",
    "\n",
    "best_prediction = []\n",
    "best_accu = 0\n",
    "for n in range(best_pick_iter):\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "    model1 = models.LSTM(n_timestep,input_size,n_unit,regularizers_alpha=0.01,drop_rate=0.5)\n",
    "\n",
    "    model1.compile(optimizer='adam',\n",
    "                  loss=loss_fn_model1_1)\n",
    "                      #callbacks=[cp-callback]\n",
    "                  #metrics=['accuracy'])\n",
    "    model1.save_weights(checkpoint_path) \n",
    "    \n",
    "    current_train_start = df.loc[prepro.date_to_index(df, train_start), 'date']\n",
    "    current_train_end = df.loc[prepro.date_to_index(df, train_end), 'date']\n",
    "    current_test_start = df.loc[prepro.date_to_index(df, test_start), 'date']\n",
    "    current_test_end = df.loc[prepro.date_to_index(df, test_start) + trans_day - 1, 'date']\n",
    "\n",
    "    #  각 transfer 구간의 예측값들을 합치기 위하여\n",
    "    test_prediction = []\n",
    "    cnt = 0\n",
    "    while True:\n",
    "        cnt += 1\n",
    "        gc.collect()\n",
    "\n",
    "        time_interval = np.random.randint(1, 4)\n",
    "        \n",
    "        train_data, test_data = prepro.get_train_test_data(df, target_column, remove_columns, \n",
    "                                                               current_train_start, current_train_end,\n",
    "                                                               current_test_start, current_test_end,\n",
    "                                                               future_day, n_timestep, time_interval)\n",
    "\n",
    "        # 전체 train, test dataset 생성\n",
    "        train_x, train_y = prepro.get_LSTM_dataset(train_data, n_timestep, time_interval, input_size, future_day)\n",
    "        train_x = train_x[:train_end_back]\n",
    "        train_y = train_y[:train_end_back]\n",
    "\n",
    "        test_x, test_y = prepro.get_LSTM_dataset(test_data, n_timestep, time_interval, input_size, future_day)\n",
    "        test_y_target = np.expand_dims(test_y[:, :, 4], axis=2)    \n",
    "\n",
    "        # train, validation set 분리하여 train1은 예측모델로 train2는 손절값 학습 모델로 사용\n",
    "        train1_x, train2_x, train1_y, train2_y = train_test_split(train_x, train_y, test_size=0.65)\n",
    "\n",
    "\n",
    "        # the model1 training\n",
    "        train1_y_target = np.expand_dims(train1_y[:, :, 4], axis=2)\n",
    "        early_stopping1 = tf.keras.callbacks.EarlyStopping(patience=2, verbose=0)\n",
    "        model1.load_weights(checkpoint_path)\n",
    "        model1.fit(train1_x, train1_y_target, batch_size=batch_size, verbose=0, epochs=3, callbacks=[early_stopping1], validation_data=(test_x, test_y_target))\n",
    "        model1.save_weights(checkpoint_path)\n",
    "\n",
    "        if cnt % 100 == 0:\n",
    "            print('test dates ' + current_test_start + \"~\" + current_test_end)\n",
    "\n",
    "        # prediction1 accuracy\n",
    "        updown = np.sign(test_y[:, -1, 4]).reshape((-1))\n",
    "        prediction = model1.predict(test_x)[:, -1, 0].reshape(-1)\n",
    "        #temp = tf.math.multiply(updown, prediction1)\n",
    "        #accu = tf.reduce_sum(list(map(lambda x: 1 if x > 0 else 0, temp)))/len(temp)\n",
    "        #print('prediction1 accuracy = ', accu)\n",
    "\n",
    "        test_prediction.append(prediction)\n",
    "        #test_prediction2.append(prediction2)\n",
    "\n",
    "        # escape from while\n",
    "        if current_test_end == test_end:\n",
    "         break\n",
    "\n",
    "        #train, start dates shift\n",
    "        current_train_end = df.loc[prepro.date_to_index(df, current_train_end) + trans_day, 'date']\n",
    "        current_train_start = df.loc[prepro.date_to_index(df, current_train_end) - 1000, 'date']\n",
    "        current_test_start = df.loc[prepro.date_to_index(df, current_test_start) + trans_day, 'date']\n",
    "        if prepro.date_to_index(df, test_end) - prepro.date_to_index(df, current_test_start) < trans_day:\n",
    "            current_test_end = test_end\n",
    "        else:\n",
    "            current_test_end = df.loc[prepro.date_to_index(df, current_test_end) + trans_day, 'date']\n",
    "    \n",
    "    t = np.concatenate(test_prediction)    \n",
    "    \n",
    "    print(\"구간 \" + str(n) + \" 학습 종료.......\")\n",
    "    print(datetime.now().strftime(\"%Y-%m-%d:%H:%M:%S\"))\n",
    "    \n",
    "    # 구간 n에 대한 accuracy 계산을 위한 data\n",
    "    test_start_index = prepro.date_to_index(df, test_start)\n",
    "    test_end_index = prepro.date_to_index(df, test_end)\n",
    "    test_open_prices = list(map(float, df.loc[test_start_index - future_day: test_end_index - future_day, '시가']))\n",
    "    test_base_prices = np.array(list(map(float, dataframe.loc[test_start_index - future_day: test_end_index - future_day, '종가']))) \n",
    "    test_output_prices = np.array(list(map(float, dataframe.loc[test_start_index: test_end_index, '종가'])))   \n",
    "    test_predict_prices = test_base_prices * (np.array(t)/100 + 1)\n",
    "\n",
    "    # 전체 test_oouput 생성\n",
    "    _, test_data = prepro.get_train_test_data(df, target_column, remove_columns,\n",
    "                                                       train_start, train_end,\n",
    "                                                       test_start, test_end,\n",
    "                                                       future_day, n_timestep, time_interval)\n",
    "    _, test_y = prepro.get_LSTM_dataset(test_data, n_timestep, time_interval, input_size, future_day)    \n",
    "    \n",
    "    # 예측확률 계산 -종가 대비\n",
    "    updown = np.sign(test_y[:, -1, 4]).reshape((-1))\n",
    "    temp = tf.math.multiply(updown, t.reshape((-1)))\n",
    "    close_accu = tf.reduce_sum(list(map(lambda x: 1 if x > 0 else 0, temp)))/len(temp)\n",
    "    print('종가대비 예측확률 = ', close_accu) \n",
    "    \n",
    "    # 예측확률 계산 -시가 대비\n",
    "    cnt = 0\n",
    "    for i in range(len(test_predict_prices)):\n",
    "        if (test_predict_prices[i]-test_open_prices[i])*(test_output_prices[i]-test_open_prices[i]) > 0:\n",
    "            cnt += 1\n",
    "    accu = cnt/len(test_predict_prices)\n",
    "    print('시가대비 예측확률 = ', accu)    \n",
    "    \n",
    "    # best 에측 확률 저장\n",
    "    if accu > best_accu: \n",
    "        best_accu = accu\n",
    "        best_prediction = t\n",
    "\n",
    "    print('시가대비 best accuracy = ', best_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "종가대비 best 예측확률 =  tf.Tensor(0.4878640776699029, shape=(), dtype=float64)\n"
     ]
    }
   ],
   "source": [
    "updown = np.sign(test_y[:, -1, 4]).reshape((-1))\n",
    "#calculate accuracy\n",
    "temp = tf.math.multiply(updown, best_prediction.reshape((-1)))\n",
    "accu = tf.reduce_sum(list(map(lambda x: 1 if x > 0 else 0, temp)))/len(temp)\n",
    "print('종가대비 best 예측확률 = ', accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 손절 손익 계산\\nprofits2 = np.zeros(len(test_dates))\\nfor i in range(len(test_dates)):\\n    if t2[i] <= 0: profits2[i] = profits[i]\\n    else:\\n        if result.test_predict_price[i] - test_open_prices[i] > 0:\\n            if test_open_prices[i] - test_low_prices[i] > t2[i]:\\n                profits2[i] = -t2[i]\\n            else:\\n                profits2[i] = profits[i]\\n        else:\\n            if test_high_prices[i] - test_open_prices[i] > t2[i]:\\n                profits2[i] = -t2[i]\\n            else:\\n                profits2[i] = profits[i]\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 원 지수, 날짜는 target conversion이 되기 전 dataframe에서 가져옴 - 종가를 test base price로 하는 경우\n",
    "test_dates, test_base_prices, train_dates, train_base_prices = prepro.get_test_dates_prices(dataframe, test_start, test_end,\n",
    "                                                      train_start, train_end, n_timestep, time_interval, future_day, target_column)\n",
    "result = GenerateResult(best_prediction, best_prediction, test_y[:, -1, 4].reshape(-1), test_dates, n_timestep, future_day, train_end_back, trans_day)\n",
    "\n",
    "#result.extract_last_output()\n",
    "result.convert_price(test_base_prices,conversion_type=target_type)\n",
    "\n",
    "# 손익 계산\n",
    "test_start_index = prepro.date_to_index(df, test_start)\n",
    "test_end_index = prepro.date_to_index(df, test_end)\n",
    "test_open_prices = list(map(float, df.loc[test_start_index - future_day: test_end_index - future_day, '시가']))\n",
    "test_high_prices = list(map(float, df.loc[test_start_index - future_day: test_end_index - future_day, '고가']))\n",
    "test_low_prices = list(map(float, df.loc[test_start_index - future_day: test_end_index - future_day, '저가']))\n",
    "profits = np.zeros(len(test_dates))\n",
    "for i in range(len(test_dates)):\n",
    "    if result.test_predict_price[i]-test_open_prices[i] > 0:\n",
    "        profits[i] = result.test_output_price[i] - test_open_prices[i]\n",
    "    else:\n",
    "        profits[i] = test_open_prices[i] - result.test_output_price[i]\n",
    "\"\"\"\n",
    "# 손절 손익 계산\n",
    "profits2 = np.zeros(len(test_dates))\n",
    "for i in range(len(test_dates)):\n",
    "    if t2[i] <= 0: profits2[i] = profits[i]\n",
    "    else:\n",
    "        if result.test_predict_price[i] - test_open_prices[i] > 0:\n",
    "            if test_open_prices[i] - test_low_prices[i] > t2[i]:\n",
    "                profits2[i] = -t2[i]\n",
    "            else:\n",
    "                profits2[i] = profits[i]\n",
    "        else:\n",
    "            if test_high_prices[i] - test_open_prices[i] > t2[i]:\n",
    "                profits2[i] = -t2[i]\n",
    "            else:\n",
    "                profits2[i] = profits[i]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "info : bestpick_kospi200f_809_0515_06-27:10:12_2_0.538\n",
      "MSE : 26.1444 , Accuracy : 0.538\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = np.array(result.test_predict_price) - np.array(test_open_prices)\n",
    "trues = np.array(result.test_output_price) - np.array(test_open_prices)\n",
    "result.evaluation(preds, trues)\n",
    "result.table(test_open_prices, test_high_prices, test_low_prices, profits, profits)\n",
    "result.save_result(model_name,item_name,n_unit,target_type,batch_size,n_timestep,time_interval,epochs,str(alpha),comment)\n",
    "result.save_visualization()\n",
    "#result.save_model(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncurrent_train_start = df.loc[prepro.date_to_index(df, train_start), \\'date\\']\\ncurrent_train_end = df.loc[prepro.date_to_index(df, train_end), \\'date\\']\\ncurrent_test_start = df.loc[prepro.date_to_index(df, test_start), \\'date\\']\\ncurrent_test_end = df.loc[prepro.date_to_index(df, test_start) + trans_day - 1, \\'date\\']\\n\\n\\n#  각 transfer 구간의 예측값들을 합치기 위하여\\ntest_prediction1 = []\\ntest_prediction2 = []\\ntest_target = []\\n\\n# early_stopping = tf.keras.callbacks.EarlyStopping(patience=3, verbose=1)\\nearly_stopping = learn.EarlyStopping(patience=2, verbose=1)\\n\\n\\ngc.collect()\\n\\ntrain_data, test_data = prepro.get_train_test_data(df, target_column, remove_columns, \\n                                                       current_train_start, current_train_end,\\n                                                       current_test_start, current_test_end,\\n                                                       future_day, n_timestep, time_interval)\\n\\n# input_size, columns reset\\ninput_size = len(df.columns) - len(remove_columns)\\ninput_columns = df.columns.copy()\\n\\ntrain_x, train_y = prepro.get_LSTM_dataset(train_data, n_timestep, time_interval, input_size, future_day)\\ntest_x, test_y = prepro.get_LSTM_dataset(test_data, n_timestep, time_interval, input_size, future_day)\\n\\n#model.compile(optimizer=\\'adam\\',\\n#          loss=loss_fn)\\n#          #callbacks=[cp-callback]\\n#          #metrics=[\\'accuracy\\'])\\n\\n# the firs training dataset\\ntrain_x = train_x[:-future_day]\\ntrain_y = train_y[:-future_day]    \\n\\n#global_step = tf.train.get_or_create_global_step()\\nglobal_step = tf.Variable(0, trainable=False)\\n#lr_decay = tf.train.exponential_decay(learning_rate, global_step,\\n#                                      train_input.shape[0]/batch_size*5, 0.5, staircase=True)\\nlr_decay = tf.compat.v1.train.exponential_decay(learning_rate,global_step, int(len(train_y)/batch_size), 0.96, staircase=True)\\noptimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\\n#optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\\n\\nupdown = np.sign(test_y[:, -1, 0]).reshape((-1))    \\nepochs = len(train_y)\\nfor iteration in range(399):\\n    batch_input, batch_output = learn.next_random_interval_batch(train_x, train_y, batch_size, future_day)\\n\\n    #noise = 2*np.random.randn(batch_size,n_timestep,1)\\n    #batch_output = batch_output+noise\\n    #batch_input = encoder(train_input[idx])\\n    gradients1 = gradient1(model1, model2, batch_input, batch_output)\\n    optimizer.apply_gradients(zip(gradients1, model1.trainable_variables))\\n    \\n    targets = tf.reshape(train_y[:, -1, 0], [-1])\\n    rates = targets / 100\\n    preds = tf.reshape(model1(train_x, training=False)[:, -1, 0], [-1])\\n    \\n    n = len(targets)    \\n    returns = [1.0]\\n    losses = []\\n    for i in range(n - 1):\\n       \\n        # average_return, std of returns, remaining days, preds[0] \\n        state = []\\n        \\n        random_rates = []\\n        for k in range(i+1):\\n            random_rates.append(rates[k])\\n        # 실재 수익률을 기반으로 random 수익률 생성, 예측에 의한 porfits 생성 \\n        for k in range(i+1, n):\\n            random_rates.append(tf.random.normal((), mean=targets[k] / 100, stddev=0.1, dtype=tf.float64))\\n        profits = tf.convert_to_tensor(random_rates, dtype=tf.float64)*tf.cast(preds, dtype=tf.float64) + 1\\n        \\n        # 현재까지의 예측에 의한 수익률 기하평균 구하기\\n        returns_past = []\\n        for k in range(i+1):\\n            returns_past.append(profits[k])\\n        avg_return = tf.cast(tf.math.reduce_prod(returns_past)**(1/(i+1)), dtype=tf.float64)\\n        \\n        state.append(avg_return)\\n        state.append(tf.math.reduce_std(returns))\\n        state.append((n - i) / n)\\n        state.append(preds[i])\\n        state = np.array(state).reshape((1, 4))\\n     \\n        # 목표일까지의 기대 기하 평균 수익률 구하기 \\n        returns_future = []\\n        for j in range(i+1, n):\\n            returns_future.append(profits[j])\\n        avg_return_future = tf.math.reduce_prod(returns_future)**(1/(n-i-1))\\n\\n        # 예측 기하 평균 수익률과 기대 기하 평균 수익률의 MSE\\n        value = tf.cast(model2(state, training=True)[0, 0], dtype=tf.float64)\\n        losses.append((value - avg_return_future)**2)\\n        if n == 3: break\\n    print(\"losses\", losses)\\n    print(\"value\", value)    \\n    with tf.GradientTape() as tape:\\n        gradients2 = tape.gradient(tf.math.reduce_sum(losses), model2.trainable_variables)\\n    optimizer.apply_gradients(zip(gradients2, model2.trainable_variables))\\n    \\n\\n    \\n    if iteration % 100 == 0:\\n        #test_MSE = model.evaluate(test_x, test_y)\\n        prediction = model1.predict(test_x)\\n        prediction_MSE = sum((updown - np.sign(prediction[:, -1, 0]).reshape(-1))**2)/len(test_y)\\n        print(\\'prediction_MSE =\\', prediction_MSE)\\n\\n    if iteration > epochs / 2 and early_stopping.validate(prediction_MSE)==True:\\n        break\\nepochs -= epochs / 5\\nif epochs <= 0: epochs = 100\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "current_train_start = df.loc[prepro.date_to_index(df, train_start), 'date']\n",
    "current_train_end = df.loc[prepro.date_to_index(df, train_end), 'date']\n",
    "current_test_start = df.loc[prepro.date_to_index(df, test_start), 'date']\n",
    "current_test_end = df.loc[prepro.date_to_index(df, test_start) + trans_day - 1, 'date']\n",
    "\n",
    "\n",
    "#  각 transfer 구간의 예측값들을 합치기 위하여\n",
    "test_prediction1 = []\n",
    "test_prediction2 = []\n",
    "test_target = []\n",
    "\n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(patience=3, verbose=1)\n",
    "early_stopping = learn.EarlyStopping(patience=2, verbose=1)\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "train_data, test_data = prepro.get_train_test_data(df, target_column, remove_columns, \n",
    "                                                       current_train_start, current_train_end,\n",
    "                                                       current_test_start, current_test_end,\n",
    "                                                       future_day, n_timestep, time_interval)\n",
    "\n",
    "# input_size, columns reset\n",
    "input_size = len(df.columns) - len(remove_columns)\n",
    "input_columns = df.columns.copy()\n",
    "\n",
    "train_x, train_y = prepro.get_LSTM_dataset(train_data, n_timestep, time_interval, input_size, future_day)\n",
    "test_x, test_y = prepro.get_LSTM_dataset(test_data, n_timestep, time_interval, input_size, future_day)\n",
    "\n",
    "#model.compile(optimizer='adam',\n",
    "#          loss=loss_fn)\n",
    "#          #callbacks=[cp-callback]\n",
    "#          #metrics=['accuracy'])\n",
    "\n",
    "# the firs training dataset\n",
    "train_x = train_x[:-future_day]\n",
    "train_y = train_y[:-future_day]    \n",
    "\n",
    "#global_step = tf.train.get_or_create_global_step()\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "#lr_decay = tf.train.exponential_decay(learning_rate, global_step,\n",
    "#                                      train_input.shape[0]/batch_size*5, 0.5, staircase=True)\n",
    "lr_decay = tf.compat.v1.train.exponential_decay(learning_rate,global_step, int(len(train_y)/batch_size), 0.96, staircase=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "#optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "updown = np.sign(test_y[:, -1, 0]).reshape((-1))    \n",
    "epochs = len(train_y)\n",
    "for iteration in range(399):\n",
    "    batch_input, batch_output = learn.next_random_interval_batch(train_x, train_y, batch_size, future_day)\n",
    "\n",
    "    #noise = 2*np.random.randn(batch_size,n_timestep,1)\n",
    "    #batch_output = batch_output+noise\n",
    "    #batch_input = encoder(train_input[idx])\n",
    "    gradients1 = gradient1(model1, model2, batch_input, batch_output)\n",
    "    optimizer.apply_gradients(zip(gradients1, model1.trainable_variables))\n",
    "    \n",
    "    targets = tf.reshape(train_y[:, -1, 0], [-1])\n",
    "    rates = targets / 100\n",
    "    preds = tf.reshape(model1(train_x, training=False)[:, -1, 0], [-1])\n",
    "    \n",
    "    n = len(targets)    \n",
    "    returns = [1.0]\n",
    "    losses = []\n",
    "    for i in range(n - 1):\n",
    "       \n",
    "        # average_return, std of returns, remaining days, preds[0] \n",
    "        state = []\n",
    "        \n",
    "        random_rates = []\n",
    "        for k in range(i+1):\n",
    "            random_rates.append(rates[k])\n",
    "        # 실재 수익률을 기반으로 random 수익률 생성, 예측에 의한 porfits 생성 \n",
    "        for k in range(i+1, n):\n",
    "            random_rates.append(tf.random.normal((), mean=targets[k] / 100, stddev=0.1, dtype=tf.float64))\n",
    "        profits = tf.convert_to_tensor(random_rates, dtype=tf.float64)*tf.cast(preds, dtype=tf.float64) + 1\n",
    "        \n",
    "        # 현재까지의 예측에 의한 수익률 기하평균 구하기\n",
    "        returns_past = []\n",
    "        for k in range(i+1):\n",
    "            returns_past.append(profits[k])\n",
    "        avg_return = tf.cast(tf.math.reduce_prod(returns_past)**(1/(i+1)), dtype=tf.float64)\n",
    "        \n",
    "        state.append(avg_return)\n",
    "        state.append(tf.math.reduce_std(returns))\n",
    "        state.append((n - i) / n)\n",
    "        state.append(preds[i])\n",
    "        state = np.array(state).reshape((1, 4))\n",
    "     \n",
    "        # 목표일까지의 기대 기하 평균 수익률 구하기 \n",
    "        returns_future = []\n",
    "        for j in range(i+1, n):\n",
    "            returns_future.append(profits[j])\n",
    "        avg_return_future = tf.math.reduce_prod(returns_future)**(1/(n-i-1))\n",
    "\n",
    "        # 예측 기하 평균 수익률과 기대 기하 평균 수익률의 MSE\n",
    "        value = tf.cast(model2(state, training=True)[0, 0], dtype=tf.float64)\n",
    "        losses.append((value - avg_return_future)**2)\n",
    "        if n == 3: break\n",
    "    print(\"losses\", losses)\n",
    "    print(\"value\", value)    \n",
    "    with tf.GradientTape() as tape:\n",
    "        gradients2 = tape.gradient(tf.math.reduce_sum(losses), model2.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients2, model2.trainable_variables))\n",
    "    \n",
    "\n",
    "    \n",
    "    if iteration % 100 == 0:\n",
    "        #test_MSE = model.evaluate(test_x, test_y)\n",
    "        prediction = model1.predict(test_x)\n",
    "        prediction_MSE = sum((updown - np.sign(prediction[:, -1, 0]).reshape(-1))**2)/len(test_y)\n",
    "        print('prediction_MSE =', prediction_MSE)\n",
    "\n",
    "    if iteration > epochs / 2 and early_stopping.validate(prediction_MSE)==True:\n",
    "        break\n",
    "epochs -= epochs / 5\n",
    "if epochs <= 0: epochs = 100\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow)",
   "language": "python",
   "name": "conda_tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
