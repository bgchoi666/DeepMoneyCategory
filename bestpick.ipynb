{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 Bimghi Choi. All Rights Reserved.\n",
    "# 예측 + 투자전략 시스템\n",
    "\n",
    "# _*_ coding: utf-8 _*_\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import os\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]='0, 1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import preprocess as prepro\n",
    "import models\n",
    "import learn\n",
    "from learn import GenerateResult\n",
    "import math\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # 텐서플로가 첫 번째 GPU에 1GB 메모리만 할당하도록 제한\n",
    "  try:\n",
    "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')    \n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=10240)])\n",
    "  except RuntimeError as e:\n",
    "    # 프로그램 시작시에 가장 장치가 설정되어야만 합니다\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "bayesian opt로 최적화된 parameter\n",
    "학습기간 2000-01-31 ~ 2017-01-02\n",
    "test기간 2017-01-02 ~ 2029-05-15\n",
    "\n",
    "2일 예측을 1일 예측으로 사용\n",
    "\n",
    "steps 30, interval 1, units 1024, batch size 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '~/Data/kospi200f_809_0515.csv'\n",
    "item_name = 'kospi200f_809_0515'\n",
    "train_start = '2000-01-31'\n",
    "train_end = '2019-01-02'\n",
    "test_start = '2019-01-02'\n",
    "test_end = '2020-05-15'\n",
    "\n",
    "remove_columns = ['date', '종가']\n",
    "target_column = '종가'\n",
    "input_columns = []\n",
    "target_type = 'rate'\n",
    "\n",
    "model_name = 'bestpick'\n",
    "channel = False\n",
    "\n",
    "trans_day = 1\n",
    "\n",
    "target_alpha = 100\n",
    "future_day = 2\n",
    "train_end_back = -1\n",
    "n_timestep = 20\n",
    "time_interval = 1\n",
    "input_size = 809\n",
    "n_unit = 500\n",
    "batch_size = 30\n",
    "learning_rate = 0.0005\n",
    "epochs = 500\n",
    "\n",
    "alpha = 0.05\n",
    "beta = 0.01\n",
    "gamma = 1\n",
    "\n",
    "comment = \"2일 에측을 위해 1일 예측 학습, trainset을 2개로 shuffle로 분리, 그 중 하나를 예측 모델로 학습, 그 다음 dataset로 예측 모델, 자산 배분 모델(sigmoid) 번갈아 학습50번 반목하여 best pick, random time_interval, steps, batch_size. loss=loss_fn_model1_1\"\n",
    "best_pick_iter = 50\n",
    "\n",
    "from datetime import datetime\n",
    "now = datetime.now().strftime('%Y-%m-%d-%H:%M:%S')\n",
    "checkpoint_path = model_name + \"/pred\"+str(future_day)+\":\"+now+\".ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = util.read_datafile(file_name)\n",
    "df = dataframe.copy()\n",
    "df['close'] = df[target_column] # 종가 column 추가, 지존 종가는 rate으로 변환 예정\n",
    "\n",
    "#df = prepro.target_conversion(df, target_column, future_day, type=target_type)\n",
    "a = []\n",
    "b = []\n",
    "for i in range(len(df[target_column]) -future_day):\n",
    "    df.loc[i, target_column] = ((df.loc[i + future_day, target_column] - df.loc[i, target_column]) \n",
    "                                / df.loc[i, target_column]) * target_alpha \n",
    "    df.loc[i, '시가'] = df.loc[i + future_day + train_end_back + 1, '시가']\n",
    "    a.append(max(df.loc[i  + future_day + train_end_back + 1:i + future_day + 1, '고가']))\n",
    "    b.append(min(df.loc[i  + future_day + train_end_back + 1:i + future_day + 1, '저가']))\n",
    "for i in range(len(df['종가']) - future_day):\n",
    "    df.loc[i, '고가'] = a[i]\n",
    "    df.loc[i, '저가'] = b[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>시가지수(포인트)</th>\n",
       "      <th>고가지수(포인트)</th>\n",
       "      <th>저가지수(포인트)</th>\n",
       "      <th>종가지수(포인트)</th>\n",
       "      <th>수익률(%)</th>\n",
       "      <th>수익률 (1주)(%)</th>\n",
       "      <th>수익률 (1개월)(%)</th>\n",
       "      <th>수익률 (3개월)(%)</th>\n",
       "      <th>수익률 (6개월)(%)</th>\n",
       "      <th>...</th>\n",
       "      <th>주요상품선물_금(선물)($/ounce)</th>\n",
       "      <th>주요상품선물_은(선물)($/ounce)</th>\n",
       "      <th>주요상품선물_알루미늄(선물)($/ton)</th>\n",
       "      <th>주요상품선물_옥수수(최근월물)(￠/bu)</th>\n",
       "      <th>대두박(￠/bu)</th>\n",
       "      <th>종가</th>\n",
       "      <th>시가</th>\n",
       "      <th>고가</th>\n",
       "      <th>저가</th>\n",
       "      <th>close</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-31</td>\n",
       "      <td>-0.218225</td>\n",
       "      <td>-0.493106</td>\n",
       "      <td>-0.400439</td>\n",
       "      <td>-0.223565</td>\n",
       "      <td>0.212872</td>\n",
       "      <td>1.352924</td>\n",
       "      <td>-0.851790</td>\n",
       "      <td>-0.382658</td>\n",
       "      <td>-0.162615</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.853010</td>\n",
       "      <td>0.974513</td>\n",
       "      <td>0.827681</td>\n",
       "      <td>0.373581</td>\n",
       "      <td>0.614542</td>\n",
       "      <td>-0.961538</td>\n",
       "      <td>116.55</td>\n",
       "      <td>120.25</td>\n",
       "      <td>115.95</td>\n",
       "      <td>119.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-02-01</td>\n",
       "      <td>-0.213200</td>\n",
       "      <td>-0.230417</td>\n",
       "      <td>-0.373954</td>\n",
       "      <td>-0.595373</td>\n",
       "      <td>-0.407635</td>\n",
       "      <td>1.808646</td>\n",
       "      <td>-1.074203</td>\n",
       "      <td>-1.707405</td>\n",
       "      <td>-0.490731</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039689</td>\n",
       "      <td>0.193571</td>\n",
       "      <td>1.002058</td>\n",
       "      <td>0.594935</td>\n",
       "      <td>0.706918</td>\n",
       "      <td>3.381014</td>\n",
       "      <td>119.35</td>\n",
       "      <td>123.45</td>\n",
       "      <td>116.80</td>\n",
       "      <td>115.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-02-02</td>\n",
       "      <td>-0.208520</td>\n",
       "      <td>-0.448886</td>\n",
       "      <td>-0.367787</td>\n",
       "      <td>-0.200398</td>\n",
       "      <td>0.696677</td>\n",
       "      <td>2.107257</td>\n",
       "      <td>-0.769928</td>\n",
       "      <td>-1.558400</td>\n",
       "      <td>0.074389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.800355</td>\n",
       "      <td>0.031266</td>\n",
       "      <td>0.069626</td>\n",
       "      <td>0.430159</td>\n",
       "      <td>0.725768</td>\n",
       "      <td>3.292528</td>\n",
       "      <td>120.40</td>\n",
       "      <td>123.60</td>\n",
       "      <td>119.75</td>\n",
       "      <td>118.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-02-03</td>\n",
       "      <td>-0.204125</td>\n",
       "      <td>-0.216603</td>\n",
       "      <td>-0.062403</td>\n",
       "      <td>-0.021445</td>\n",
       "      <td>0.366314</td>\n",
       "      <td>1.488978</td>\n",
       "      <td>-0.625155</td>\n",
       "      <td>-1.268930</td>\n",
       "      <td>-0.079967</td>\n",
       "      <td>...</td>\n",
       "      <td>1.515992</td>\n",
       "      <td>0.623370</td>\n",
       "      <td>0.378121</td>\n",
       "      <td>0.084769</td>\n",
       "      <td>0.352875</td>\n",
       "      <td>1.677149</td>\n",
       "      <td>122.90</td>\n",
       "      <td>124.10</td>\n",
       "      <td>120.95</td>\n",
       "      <td>119.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-02-07</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.378008</td>\n",
       "      <td>0.426748</td>\n",
       "      <td>0.592789</td>\n",
       "      <td>0.949260</td>\n",
       "      <td>1.101574</td>\n",
       "      <td>1.281908</td>\n",
       "      <td>-1.288431</td>\n",
       "      <td>1.273730</td>\n",
       "      <td>...</td>\n",
       "      <td>3.856851</td>\n",
       "      <td>0.568635</td>\n",
       "      <td>-0.322023</td>\n",
       "      <td>0.518339</td>\n",
       "      <td>0.702038</td>\n",
       "      <td>0.653862</td>\n",
       "      <td>122.40</td>\n",
       "      <td>125.05</td>\n",
       "      <td>120.40</td>\n",
       "      <td>122.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2000-02-08</td>\n",
       "      <td>-0.196110</td>\n",
       "      <td>0.353748</td>\n",
       "      <td>0.677387</td>\n",
       "      <td>0.254527</td>\n",
       "      <td>-0.366454</td>\n",
       "      <td>1.138729</td>\n",
       "      <td>0.998930</td>\n",
       "      <td>-1.329397</td>\n",
       "      <td>0.942469</td>\n",
       "      <td>...</td>\n",
       "      <td>2.657684</td>\n",
       "      <td>1.222974</td>\n",
       "      <td>-0.720402</td>\n",
       "      <td>0.474125</td>\n",
       "      <td>0.440887</td>\n",
       "      <td>1.319588</td>\n",
       "      <td>120.85</td>\n",
       "      <td>125.05</td>\n",
       "      <td>119.85</td>\n",
       "      <td>121.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2000-02-09</td>\n",
       "      <td>-0.192447</td>\n",
       "      <td>0.306497</td>\n",
       "      <td>0.819824</td>\n",
       "      <td>0.658684</td>\n",
       "      <td>0.641075</td>\n",
       "      <td>1.076862</td>\n",
       "      <td>1.233568</td>\n",
       "      <td>-1.637689</td>\n",
       "      <td>0.994675</td>\n",
       "      <td>...</td>\n",
       "      <td>3.026664</td>\n",
       "      <td>2.114708</td>\n",
       "      <td>-0.407511</td>\n",
       "      <td>0.622228</td>\n",
       "      <td>0.721716</td>\n",
       "      <td>-1.867641</td>\n",
       "      <td>123.60</td>\n",
       "      <td>124.75</td>\n",
       "      <td>115.25</td>\n",
       "      <td>123.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000-02-10</td>\n",
       "      <td>0.330923</td>\n",
       "      <td>0.617187</td>\n",
       "      <td>0.552762</td>\n",
       "      <td>0.366383</td>\n",
       "      <td>-0.311193</td>\n",
       "      <td>0.651653</td>\n",
       "      <td>0.275024</td>\n",
       "      <td>-1.607973</td>\n",
       "      <td>0.269679</td>\n",
       "      <td>...</td>\n",
       "      <td>3.316674</td>\n",
       "      <td>1.871659</td>\n",
       "      <td>-0.157431</td>\n",
       "      <td>0.985272</td>\n",
       "      <td>1.379493</td>\n",
       "      <td>-5.575906</td>\n",
       "      <td>118.85</td>\n",
       "      <td>119.00</td>\n",
       "      <td>110.45</td>\n",
       "      <td>122.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2000-02-11</td>\n",
       "      <td>0.977388</td>\n",
       "      <td>0.576306</td>\n",
       "      <td>0.048738</td>\n",
       "      <td>-0.013448</td>\n",
       "      <td>-0.428351</td>\n",
       "      <td>0.326747</td>\n",
       "      <td>0.131530</td>\n",
       "      <td>-1.828185</td>\n",
       "      <td>0.546650</td>\n",
       "      <td>...</td>\n",
       "      <td>2.384344</td>\n",
       "      <td>0.988933</td>\n",
       "      <td>-0.154629</td>\n",
       "      <td>0.473525</td>\n",
       "      <td>0.907189</td>\n",
       "      <td>-8.274721</td>\n",
       "      <td>116.45</td>\n",
       "      <td>116.80</td>\n",
       "      <td>105.25</td>\n",
       "      <td>120.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2000-02-14</td>\n",
       "      <td>-0.649462</td>\n",
       "      <td>-0.806483</td>\n",
       "      <td>-0.954529</td>\n",
       "      <td>-1.222378</td>\n",
       "      <td>-1.533955</td>\n",
       "      <td>-1.217328</td>\n",
       "      <td>-0.075927</td>\n",
       "      <td>-2.486079</td>\n",
       "      <td>0.140712</td>\n",
       "      <td>...</td>\n",
       "      <td>1.935983</td>\n",
       "      <td>0.085505</td>\n",
       "      <td>-0.246361</td>\n",
       "      <td>0.529501</td>\n",
       "      <td>0.923675</td>\n",
       "      <td>-1.422414</td>\n",
       "      <td>112.05</td>\n",
       "      <td>114.35</td>\n",
       "      <td>105.25</td>\n",
       "      <td>116.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2000-02-15</td>\n",
       "      <td>-1.074607</td>\n",
       "      <td>-1.359121</td>\n",
       "      <td>-1.825901</td>\n",
       "      <td>-1.959771</td>\n",
       "      <td>-1.104848</td>\n",
       "      <td>-1.601324</td>\n",
       "      <td>-0.717085</td>\n",
       "      <td>-2.587555</td>\n",
       "      <td>-0.669097</td>\n",
       "      <td>...</td>\n",
       "      <td>1.222183</td>\n",
       "      <td>-0.051403</td>\n",
       "      <td>-1.024949</td>\n",
       "      <td>0.518408</td>\n",
       "      <td>1.160291</td>\n",
       "      <td>1.623816</td>\n",
       "      <td>112.35</td>\n",
       "      <td>115.70</td>\n",
       "      <td>109.95</td>\n",
       "      <td>110.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2000-02-16</td>\n",
       "      <td>-1.939741</td>\n",
       "      <td>-2.072462</td>\n",
       "      <td>-2.426875</td>\n",
       "      <td>-1.820694</td>\n",
       "      <td>0.171634</td>\n",
       "      <td>-1.786449</td>\n",
       "      <td>-0.703717</td>\n",
       "      <td>-2.563710</td>\n",
       "      <td>-0.401469</td>\n",
       "      <td>...</td>\n",
       "      <td>1.251580</td>\n",
       "      <td>0.487841</td>\n",
       "      <td>-1.605690</td>\n",
       "      <td>0.609070</td>\n",
       "      <td>0.911960</td>\n",
       "      <td>-2.186270</td>\n",
       "      <td>113.35</td>\n",
       "      <td>115.70</td>\n",
       "      <td>105.95</td>\n",
       "      <td>114.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2000-02-17</td>\n",
       "      <td>-2.062374</td>\n",
       "      <td>-1.576333</td>\n",
       "      <td>-1.627765</td>\n",
       "      <td>-1.281362</td>\n",
       "      <td>0.934836</td>\n",
       "      <td>-1.133411</td>\n",
       "      <td>-0.976304</td>\n",
       "      <td>-1.810490</td>\n",
       "      <td>1.006196</td>\n",
       "      <td>...</td>\n",
       "      <td>1.099126</td>\n",
       "      <td>0.014284</td>\n",
       "      <td>-1.379601</td>\n",
       "      <td>0.356659</td>\n",
       "      <td>1.165770</td>\n",
       "      <td>-5.281846</td>\n",
       "      <td>109.80</td>\n",
       "      <td>109.80</td>\n",
       "      <td>105.50</td>\n",
       "      <td>112.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2000-02-18</td>\n",
       "      <td>-1.203728</td>\n",
       "      <td>-1.119424</td>\n",
       "      <td>-1.312861</td>\n",
       "      <td>-1.662198</td>\n",
       "      <td>-0.622664</td>\n",
       "      <td>-1.227625</td>\n",
       "      <td>-1.271035</td>\n",
       "      <td>-1.891012</td>\n",
       "      <td>0.605513</td>\n",
       "      <td>...</td>\n",
       "      <td>1.361637</td>\n",
       "      <td>0.227203</td>\n",
       "      <td>-1.264041</td>\n",
       "      <td>0.245730</td>\n",
       "      <td>1.381693</td>\n",
       "      <td>-4.112651</td>\n",
       "      <td>106.85</td>\n",
       "      <td>110.85</td>\n",
       "      <td>105.50</td>\n",
       "      <td>111.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2000-02-21</td>\n",
       "      <td>-2.011534</td>\n",
       "      <td>-2.360013</td>\n",
       "      <td>-2.117703</td>\n",
       "      <td>-2.493118</td>\n",
       "      <td>-1.243495</td>\n",
       "      <td>-1.039810</td>\n",
       "      <td>-0.905187</td>\n",
       "      <td>-2.286667</td>\n",
       "      <td>-0.665131</td>\n",
       "      <td>...</td>\n",
       "      <td>1.274963</td>\n",
       "      <td>0.268968</td>\n",
       "      <td>-1.589226</td>\n",
       "      <td>0.185194</td>\n",
       "      <td>1.319954</td>\n",
       "      <td>3.608247</td>\n",
       "      <td>108.55</td>\n",
       "      <td>111.60</td>\n",
       "      <td>107.10</td>\n",
       "      <td>106.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2000-02-22</td>\n",
       "      <td>-2.301061</td>\n",
       "      <td>-2.151107</td>\n",
       "      <td>-2.119718</td>\n",
       "      <td>-2.142064</td>\n",
       "      <td>0.422501</td>\n",
       "      <td>-0.214320</td>\n",
       "      <td>-0.740215</td>\n",
       "      <td>-1.957417</td>\n",
       "      <td>-0.444290</td>\n",
       "      <td>...</td>\n",
       "      <td>1.230409</td>\n",
       "      <td>0.364772</td>\n",
       "      <td>-1.172751</td>\n",
       "      <td>-0.103243</td>\n",
       "      <td>1.100675</td>\n",
       "      <td>1.491841</td>\n",
       "      <td>110.20</td>\n",
       "      <td>111.60</td>\n",
       "      <td>106.50</td>\n",
       "      <td>107.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2000-02-23</td>\n",
       "      <td>-1.670887</td>\n",
       "      <td>-1.656883</td>\n",
       "      <td>-1.569320</td>\n",
       "      <td>-1.267786</td>\n",
       "      <td>1.605126</td>\n",
       "      <td>0.506204</td>\n",
       "      <td>0.050075</td>\n",
       "      <td>-1.494619</td>\n",
       "      <td>-0.699133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.708148</td>\n",
       "      <td>-0.052186</td>\n",
       "      <td>-0.980503</td>\n",
       "      <td>-1.014657</td>\n",
       "      <td>0.126165</td>\n",
       "      <td>-1.492537</td>\n",
       "      <td>107.75</td>\n",
       "      <td>109.85</td>\n",
       "      <td>101.95</td>\n",
       "      <td>110.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2000-02-24</td>\n",
       "      <td>-1.366238</td>\n",
       "      <td>-1.422324</td>\n",
       "      <td>-1.358331</td>\n",
       "      <td>-1.490032</td>\n",
       "      <td>-0.528189</td>\n",
       "      <td>-0.294080</td>\n",
       "      <td>-0.319013</td>\n",
       "      <td>-1.472892</td>\n",
       "      <td>-1.661528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.530452</td>\n",
       "      <td>-1.233068</td>\n",
       "      <td>-0.941193</td>\n",
       "      <td>-0.976769</td>\n",
       "      <td>0.385420</td>\n",
       "      <td>-6.017455</td>\n",
       "      <td>105.95</td>\n",
       "      <td>106.55</td>\n",
       "      <td>100.75</td>\n",
       "      <td>108.85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2000-02-25</td>\n",
       "      <td>-1.680249</td>\n",
       "      <td>-1.665735</td>\n",
       "      <td>-1.510077</td>\n",
       "      <td>-1.461250</td>\n",
       "      <td>0.033092</td>\n",
       "      <td>0.075897</td>\n",
       "      <td>0.493437</td>\n",
       "      <td>-1.430882</td>\n",
       "      <td>-2.424417</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.081697</td>\n",
       "      <td>-2.561532</td>\n",
       "      <td>-1.090127</td>\n",
       "      <td>0.174516</td>\n",
       "      <td>-0.148781</td>\n",
       "      <td>-5.463728</td>\n",
       "      <td>104.15</td>\n",
       "      <td>113.00</td>\n",
       "      <td>100.75</td>\n",
       "      <td>108.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2000-02-28</td>\n",
       "      <td>-1.697353</td>\n",
       "      <td>-2.045428</td>\n",
       "      <td>-2.011407</td>\n",
       "      <td>-2.307633</td>\n",
       "      <td>-1.966604</td>\n",
       "      <td>-0.289909</td>\n",
       "      <td>-1.729787</td>\n",
       "      <td>-1.563181</td>\n",
       "      <td>-3.097790</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.149500</td>\n",
       "      <td>-2.235868</td>\n",
       "      <td>-2.265504</td>\n",
       "      <td>-0.179609</td>\n",
       "      <td>0.061992</td>\n",
       "      <td>9.921799</td>\n",
       "      <td>106.50</td>\n",
       "      <td>113.75</td>\n",
       "      <td>106.35</td>\n",
       "      <td>102.30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 815 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  시가지수(포인트)  고가지수(포인트)  저가지수(포인트)  종가지수(포인트)    수익률(%)  \\\n",
       "0   2000-01-31  -0.218225  -0.493106  -0.400439  -0.223565  0.212872   \n",
       "1   2000-02-01  -0.213200  -0.230417  -0.373954  -0.595373 -0.407635   \n",
       "2   2000-02-02  -0.208520  -0.448886  -0.367787  -0.200398  0.696677   \n",
       "3   2000-02-03  -0.204125  -0.216603  -0.062403  -0.021445  0.366314   \n",
       "4   2000-02-07  -0.200000   0.378008   0.426748   0.592789  0.949260   \n",
       "5   2000-02-08  -0.196110   0.353748   0.677387   0.254527 -0.366454   \n",
       "6   2000-02-09  -0.192447   0.306497   0.819824   0.658684  0.641075   \n",
       "7   2000-02-10   0.330923   0.617187   0.552762   0.366383 -0.311193   \n",
       "8   2000-02-11   0.977388   0.576306   0.048738  -0.013448 -0.428351   \n",
       "9   2000-02-14  -0.649462  -0.806483  -0.954529  -1.222378 -1.533955   \n",
       "10  2000-02-15  -1.074607  -1.359121  -1.825901  -1.959771 -1.104848   \n",
       "11  2000-02-16  -1.939741  -2.072462  -2.426875  -1.820694  0.171634   \n",
       "12  2000-02-17  -2.062374  -1.576333  -1.627765  -1.281362  0.934836   \n",
       "13  2000-02-18  -1.203728  -1.119424  -1.312861  -1.662198 -0.622664   \n",
       "14  2000-02-21  -2.011534  -2.360013  -2.117703  -2.493118 -1.243495   \n",
       "15  2000-02-22  -2.301061  -2.151107  -2.119718  -2.142064  0.422501   \n",
       "16  2000-02-23  -1.670887  -1.656883  -1.569320  -1.267786  1.605126   \n",
       "17  2000-02-24  -1.366238  -1.422324  -1.358331  -1.490032 -0.528189   \n",
       "18  2000-02-25  -1.680249  -1.665735  -1.510077  -1.461250  0.033092   \n",
       "19  2000-02-28  -1.697353  -2.045428  -2.011407  -2.307633 -1.966604   \n",
       "\n",
       "    수익률 (1주)(%)  수익률 (1개월)(%)  수익률 (3개월)(%)  수익률 (6개월)(%)  ...  \\\n",
       "0      1.352924     -0.851790     -0.382658     -0.162615  ...   \n",
       "1      1.808646     -1.074203     -1.707405     -0.490731  ...   \n",
       "2      2.107257     -0.769928     -1.558400      0.074389  ...   \n",
       "3      1.488978     -0.625155     -1.268930     -0.079967  ...   \n",
       "4      1.101574      1.281908     -1.288431      1.273730  ...   \n",
       "5      1.138729      0.998930     -1.329397      0.942469  ...   \n",
       "6      1.076862      1.233568     -1.637689      0.994675  ...   \n",
       "7      0.651653      0.275024     -1.607973      0.269679  ...   \n",
       "8      0.326747      0.131530     -1.828185      0.546650  ...   \n",
       "9     -1.217328     -0.075927     -2.486079      0.140712  ...   \n",
       "10    -1.601324     -0.717085     -2.587555     -0.669097  ...   \n",
       "11    -1.786449     -0.703717     -2.563710     -0.401469  ...   \n",
       "12    -1.133411     -0.976304     -1.810490      1.006196  ...   \n",
       "13    -1.227625     -1.271035     -1.891012      0.605513  ...   \n",
       "14    -1.039810     -0.905187     -2.286667     -0.665131  ...   \n",
       "15    -0.214320     -0.740215     -1.957417     -0.444290  ...   \n",
       "16     0.506204      0.050075     -1.494619     -0.699133  ...   \n",
       "17    -0.294080     -0.319013     -1.472892     -1.661528  ...   \n",
       "18     0.075897      0.493437     -1.430882     -2.424417  ...   \n",
       "19    -0.289909     -1.729787     -1.563181     -3.097790  ...   \n",
       "\n",
       "    주요상품선물_금(선물)($/ounce)  주요상품선물_은(선물)($/ounce)  주요상품선물_알루미늄(선물)($/ton)  \\\n",
       "0               -0.853010               0.974513                0.827681   \n",
       "1               -0.039689               0.193571                1.002058   \n",
       "2                0.800355               0.031266                0.069626   \n",
       "3                1.515992               0.623370                0.378121   \n",
       "4                3.856851               0.568635               -0.322023   \n",
       "5                2.657684               1.222974               -0.720402   \n",
       "6                3.026664               2.114708               -0.407511   \n",
       "7                3.316674               1.871659               -0.157431   \n",
       "8                2.384344               0.988933               -0.154629   \n",
       "9                1.935983               0.085505               -0.246361   \n",
       "10               1.222183              -0.051403               -1.024949   \n",
       "11               1.251580               0.487841               -1.605690   \n",
       "12               1.099126               0.014284               -1.379601   \n",
       "13               1.361637               0.227203               -1.264041   \n",
       "14               1.274963               0.268968               -1.589226   \n",
       "15               1.230409               0.364772               -1.172751   \n",
       "16               0.708148              -0.052186               -0.980503   \n",
       "17               0.530452              -1.233068               -0.941193   \n",
       "18              -0.081697              -2.561532               -1.090127   \n",
       "19              -0.149500              -2.235868               -2.265504   \n",
       "\n",
       "    주요상품선물_옥수수(최근월물)(￠/bu)  대두박(￠/bu)        종가      시가      고가      저가  \\\n",
       "0                 0.373581   0.614542 -0.961538  116.55  120.25  115.95   \n",
       "1                 0.594935   0.706918  3.381014  119.35  123.45  116.80   \n",
       "2                 0.430159   0.725768  3.292528  120.40  123.60  119.75   \n",
       "3                 0.084769   0.352875  1.677149  122.90  124.10  120.95   \n",
       "4                 0.518339   0.702038  0.653862  122.40  125.05  120.40   \n",
       "5                 0.474125   0.440887  1.319588  120.85  125.05  119.85   \n",
       "6                 0.622228   0.721716 -1.867641  123.60  124.75  115.25   \n",
       "7                 0.985272   1.379493 -5.575906  118.85  119.00  110.45   \n",
       "8                 0.473525   0.907189 -8.274721  116.45  116.80  105.25   \n",
       "9                 0.529501   0.923675 -1.422414  112.05  114.35  105.25   \n",
       "10                0.518408   1.160291  1.623816  112.35  115.70  109.95   \n",
       "11                0.609070   0.911960 -2.186270  113.35  115.70  105.95   \n",
       "12                0.356659   1.165770 -5.281846  109.80  109.80  105.50   \n",
       "13                0.245730   1.381693 -4.112651  106.85  110.85  105.50   \n",
       "14                0.185194   1.319954  3.608247  108.55  111.60  107.10   \n",
       "15               -0.103243   1.100675  1.491841  110.20  111.60  106.50   \n",
       "16               -1.014657   0.126165 -1.492537  107.75  109.85  101.95   \n",
       "17               -0.976769   0.385420 -6.017455  105.95  106.55  100.75   \n",
       "18                0.174516  -0.148781 -5.463728  104.15  113.00  100.75   \n",
       "19               -0.179609   0.061992  9.921799  106.50  113.75  106.35   \n",
       "\n",
       "     close  \n",
       "0   119.60  \n",
       "1   115.35  \n",
       "2   118.45  \n",
       "3   119.25  \n",
       "4   122.35  \n",
       "5   121.25  \n",
       "6   123.15  \n",
       "7   122.85  \n",
       "8   120.85  \n",
       "9   116.00  \n",
       "10  110.85  \n",
       "11  114.35  \n",
       "12  112.65  \n",
       "13  111.85  \n",
       "14  106.70  \n",
       "15  107.25  \n",
       "16  110.55  \n",
       "17  108.85  \n",
       "18  108.90  \n",
       "19  102.30  \n",
       "\n",
       "[20 rows x 815 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', '시가지수(포인트)', '고가지수(포인트)', '저가지수(포인트)', '종가지수(포인트)', '수익률(%)',\n",
       "       '수익률 (1주)(%)', '수익률 (1개월)(%)', '수익률 (3개월)(%)', '수익률 (6개월)(%)',\n",
       "       ...\n",
       "       '주요상품선물_금(선물)($/ounce)', '주요상품선물_은(선물)($/ounce)',\n",
       "       '주요상품선물_알루미늄(선물)($/ton)', '주요상품선물_옥수수(최근월물)(￠/bu)', '대두박(￠/bu)', '종가',\n",
       "       '시가', '고가', '저가', 'close'],\n",
       "      dtype='object', length=815)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_list = df.columns\n",
    "column_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas_profiling\n",
    "#data = df.loc[:, ['미국 SP 500 Index(종가)(Pt)', '거래량(주)', '외국인보유비중(%)', '종가']]\n",
    "#data = data[:-future_day]\n",
    "#pr = data.profile_report()\n",
    "#pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start_index = max(df.loc[df['date']<=train_start].index) + time_interval*(n_timestep-1) - 1\n",
    "train_end_index = max(df.loc[df['date']<=train_end].index)\n",
    "base_prices = tf.reduce_mean(df.loc[train_start_index:train_end_index+1, '시가'])  \n",
    "\n",
    "@tf.function\n",
    "def loss_fn_model1_1(targets, preds):\n",
    " \n",
    "    loss0 = tf.keras.losses.MSE(targets, preds)\n",
    "    \n",
    "    preds = tf.reshape(preds[:, n_timestep-1, :], [-1])\n",
    "    targets = tf.reshape(targets[:, n_timestep-1, :], [-1])\n",
    "    \n",
    "   \n",
    "    if alpha != 0:\n",
    "        # add RRL cost - maximize downside sharp ratio\n",
    "\n",
    "        # 1 if (pred - base) * (target - base) > 0, -1 otherwise\n",
    "        F = tf.math.sign(targets*preds)\n",
    "        F = tf.reshape(F, [-1])\n",
    "\n",
    "        # calc returns from each step in batches\n",
    "        R = tf.math.divide(tf.math.multiply(tf.math.abs(targets), (F - 0.00003)), base_prices)\n",
    "        R = tf.reshape(R, [-1])\n",
    "\n",
    "        # calc downside sharp ratio\n",
    "\n",
    "        # downside returns\n",
    "        DR = tf.minimum(0.0, R)\n",
    "        DR = tf.reshape(DR, [-1])\n",
    "\n",
    "        # calc. downside sharp ratio\n",
    "        #s = []\n",
    "        #for i in range(batch_size):\n",
    "        #   std =  tf.keras.backend.std(DR[i, :, 0])\n",
    "        #   s.append(tf.reduce_mean(R[i, :, 0])/tf.maximum(0.01, std))\n",
    "\n",
    "        # calc. downside sharp ratio\n",
    "        loss1 = tf.reduce_mean(R) / (tf.keras.backend.std(DR) + 0.001)\n",
    "    else:\n",
    "        loss1 = 0\n",
    "\n",
    "    \"\"\"\n",
    "    # average profits, loss\n",
    "    avg_plusR = [0.0]\n",
    "    avg_minusR = [0.0]\n",
    "\n",
    "    global num_of_profits\n",
    "    global num_of_losses\n",
    "\n",
    "    num_of_profits = 0\n",
    "    num_of_losses = 0\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        res = tf.cond(R[i, num_steps - 1, 0] > 0, lambda: return_one(), lambda: return_zero())\n",
    "        if res == 1:\n",
    "            avg_plusR.append(R[i, num_steps - 1, 0])\n",
    "        else:\n",
    "            avg_minusR.append(R[i, num_steps - 1, 0])\n",
    "    avg_profit = tf.reduce_mean(avg_plusR) \n",
    "    avg_loss = tf.reduce_mean(avg_minusR) \n",
    "    \"\"\"\n",
    "\n",
    "    if beta != 0:\n",
    "        #compute maximum drawdown\n",
    "\n",
    "        #accm_profit = [0.0]\n",
    "        #for i in range(batch_size):\n",
    "        #    for j in range(num_steps):\n",
    "        #        r = tf.cond((predict_prices[i, num_steps-1, 0] - base_prices[i, num_steps-1, 0]) *\n",
    "        #                   (target_prices[i, num_steps-1, 0] - base_prices[i, num_steps-1, 0]) > 0,\n",
    "        #                   lambda: return_one(),\n",
    "        #                   lambda: return_zero())\n",
    "        #        if r == 1: accm_profit.append(accm_profit[i*num_steps + j] + tf.abs(target_prices[i, j, 0] - base_prices[i, j, 0]))\n",
    "        #        else:      accm_profit.append(accm_profit[i*num_steps + j] - tf.abs(target_prices[i, j, 0] - base_prices[i, j, 0]))\n",
    "\n",
    "        accm_profit = [0.0 for i in range(batch_size)]\n",
    "        for i in range(batch_size):\n",
    "            if i == 0:\n",
    "                accm_profit[0] = tf.sign(preds * targets) * tf.math.abs(targets)\n",
    "            else:\n",
    "                accm_profit[i] = accm_profit[i-1] + tf.sign(preds * targets) * tf.math.abs(targets)\n",
    "        loss2 = (tf.reduce_max(accm_profit) - tf.reduce_min(accm_profit))/batch_size\n",
    "    else:\n",
    "        loss2 = 0\n",
    "\n",
    "    return loss0 + beta*loss2 - alpha*loss1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "자산 배분 0, 1/4, 2/4. 3/4. 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def loss_fn_model2(m1, m2, train_x, train_y):\n",
    "    \"\"\"\n",
    "    batches = train_y.shape[0]\n",
    "    steps = train_y.shape[1]    \n",
    "    \n",
    "    train_y_target = np.expand_dims(train_y[:, :, 4], axis=2)\n",
    "    \n",
    "    rates = tf.cast(train_y_target / 100, dtype=tf.float64)\n",
    "    rates_last = tf.reshape(rates[:, -1, 0], [-1])\n",
    "    rates = tf.reshape(rates[:, :-1, 0], [batches, steps - 1, 1])\n",
    "    rates = tf.concat([rates, np.zeros((batches, 1, 1))], 1)\n",
    "    updown = tf.math.sign(train_y_target)\n",
    "    preds = tf.cast(m1(train_x), dtype=tf.float64)\n",
    "    profits = tf.cast(1 + tf.convert_to_tensor(rates, dtype=tf.float64)*tf.math.sign(preds), dtype=tf.float64)\n",
    "    \n",
    "    \n",
    "\n",
    "    # model2 input 생성\n",
    "    train_x_m2 = tf.concat([preds, rates], 2)\n",
    "    y = tf.cast(m2(train_x_m2)[:, -1, :], dtype=tf.float64)\n",
    "    shares = tf.math.argmax(y, 1)\n",
    "    \n",
    "    # the number of shares 계산\n",
    "    profits = []\n",
    "    for i in range(batches):\n",
    "        profits.append(1.0 + (y[i, shares[i]])*rates[i]*tf.math.sign(preds[i]))\n",
    "    profits = tf.reshape(profits, [batches, steps, 1])    \n",
    "\n",
    "    # batch i에서 model2의 target = phi(1 + shares*rates*preds) * max(1 + shares*rates*gamma)\n",
    "    targets = []\n",
    "    for i in range(batches):\n",
    " \n",
    "        targets.append((tf.math.reduce_prod(profits[i, :-1, 0])*(1 + abs(rates_last[i])*gamma)-1)*10)\n",
    "    \n",
    "    Qvalues = []\n",
    "    for i in range(batches):\n",
    "        \n",
    "        Qvalues.append((tf.math.reduce_prod(profits[i, :-1, 0])-1)*10)        \n",
    "\n",
    "    #targets = np.array(targets) \n",
    "    #Qvalues = np.array(Qvalues)\n",
    "    \n",
    "    loss = keras.losses.MSE(targets, Qvalues)\n",
    "  \n",
    "    return loss\n",
    "    \"\"\"\n",
    "    batches = train_y.shape[0]\n",
    "    steps = train_y.shape[1]    \n",
    "    \n",
    "   \n",
    "    train_y_target = tf.expand_dims(train_y[:, :, 4], 2)\n",
    "    \n",
    "    rates = tf.cast(train_y_target / 100, dtype=tf.float64)\n",
    "    rates_last = tf.reshape(rates[:, -1, 0], [-1])\n",
    "    rates = tf.reshape(rates[:, :-1, 0], [batches, steps - 1, 1])\n",
    "    rates = tf.concat([rates, np.zeros((batches, 1, 1))], 1)\n",
    "    updown = tf.math.sign(train_y_target)\n",
    "    preds = tf.cast(m1(train_x), dtype=tf.float64)\n",
    "    #profits = tf.cast(tf.convert_to_tensor(rates, dtype=tf.float64)*tf.math.sign(preds), dtype=tf.float64)\n",
    "    \n",
    "    \n",
    "\n",
    "    # model2 input 생성\n",
    "    train_x_m2 = tf.concat([preds, rates], 2)\n",
    "    y = tf.reshape(tf.cast(m2(train_x_m2)[:, :, 0], dtype=tf.float64), [batches, steps, 1])\n",
    "    \n",
    "   \n",
    "    # the number of shares 계산\n",
    "    profits = []\n",
    "    for i in range(batches):\n",
    "        profits.append(y[i]*rates[i]*tf.math.sign(preds[i]))\n",
    "    profits = tf.reshape(profits, [batches, steps, 1])\n",
    "\n",
    "    \n",
    "    # maximum profits\n",
    "    max_profits = []\n",
    "    for i in range(batches):\n",
    "\n",
    "        max_profits.append(tf.math.reduce_sum(tf.math.abs(rates[i, :, 0])))\n",
    "    \n",
    "   \n",
    "    Qvalues = []\n",
    "    for i in range(batches):\n",
    "        \n",
    "        Qvalues.append(tf.math.reduce_sum(profits[i, :, 0]))        \n",
    "\n",
    "    \n",
    "    loss = keras.losses.MSE(max_profits, Qvalues)\n",
    "  \n",
    "    return loss    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(m1, m2, test_x, test_y):\n",
    "    \"\"\"\n",
    "    train_y_target = np.expand_dims(train_y[:, :, 4], axis=2)\n",
    "    \n",
    "    rates = tf.cast(test_y_target / 100, dtype=tf.float64)\n",
    "    rates = tf.reshape(rates[:, :-1, 0], [rates.shape[0], rates.shape[1] -1, 1])\n",
    "    rates = tf.concat([rates, np.zeros((rates.shape[0], 1, 1))], 1)\n",
    "    preds = tf.cast(m1(test_x), dtype=tf.float64)\n",
    "\n",
    "    # model2 input 생성\n",
    "    test_x_m2 = tf.concat([preds, rates], 2)\n",
    "    y = m2(test_x_m2)[:, -1, :]\n",
    "    shares = tf.math.argmax(y, 1)\n",
    "\n",
    "    return shares\n",
    "    \"\"\"\n",
    "    test_y_target = np.expand_dims(test_y[:, :, 4], axis=2)\n",
    "    \n",
    "    rates = tf.cast(test_y_target / 100, dtype=tf.float64)\n",
    "    rates = tf.reshape(rates[:, :-1, 0], [rates.shape[0], rates.shape[1] -1, 1])\n",
    "    rates = tf.concat([rates, np.zeros((rates.shape[0], 1, 1))], 1)\n",
    "    preds = tf.cast(m1(test_x), dtype=tf.float64)\n",
    "\n",
    "    # model2 input 생성\n",
    "    test_x_m2 = tf.concat([preds, rates], 2)\n",
    "    y = m2(test_x_m2)[:, -1, :]\n",
    "    #shares = tf.math.argmax(y, 1)\n",
    "\n",
    "    return y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def gradient1(model1, model2, input_data, output_data):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn_model2(model1, model2, input_data, output_data)\n",
    "    return tape.gradient(loss, model1.trainable_variables)\n",
    "#@tf.function\n",
    "def gradient2(model1, model2, input_data, output_data):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn_model2(model1, model2, input_data, output_data)\n",
    "    return tape.gradient(loss, model2.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nmodel1 = tf.keras.Sequential([\\n    tf.keras.Input(shape=(n_timestep, input_size)),\\n    tf.keras.layers.LSTM(n_unit, return_sequences=True),\\n    tf.keras.layers.LSTM(n_unit, return_sequences=True),    \\n    tf.keras.layers.Dense(64, activation='relu'),\\n    tf.keras.layers.Dense(1)\\n])\\nmodel1.summary()\\n\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#strategy = tf.distribute.MirroredStrategy()\n",
    "#with strategy.scope():\n",
    "\"\"\"\n",
    "model1 = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(n_timestep, input_size)),\n",
    "    tf.keras.layers.LSTM(n_unit, return_sequences=True),\n",
    "    tf.keras.layers.LSTM(n_unit, return_sequences=True),    \n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model1.summary()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "학습 시작\n",
      "2020-11-03:18:40:37\n",
      "WARNING:tensorflow:Layer lstm is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.0002088266797012014, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.00024092976480347237, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.024459362624091853, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 0 학습 종료.......\n",
      "2020-11-03:22:38:36\n",
      "종가대비 예측확률 =  tf.Tensor(0.5074183976261127, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.4896142433234421\n",
      "시가대비 best profits_accuracy =  0\n",
      "시가대비 자산 배분 반영 best profits =  0.0\n",
      "WARNING:tensorflow:Layer lstm_3 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_5 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(6.535778305689068e-05, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(9.658078642315182e-05, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(1.0072363528997677e-11, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[0.9999995]], shape=(1, 1), dtype=float32)\n",
      "구간 1 학습 종료.......\n",
      "2020-11-04:02:00:25\n",
      "종가대비 예측확률 =  tf.Tensor(0.5014836795252225, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.4599406528189911\n",
      "시가대비 best profits_accuracy =  0\n",
      "시가대비 자산 배분 반영 best profits =  0.0\n",
      "WARNING:tensorflow:Layer lstm_6 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_8 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.0024563526042028342, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.002654825440174826, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.08934401282153819, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 2 학습 종료.......\n",
      "2020-11-04:05:48:15\n",
      "종가대비 예측확률 =  tf.Tensor(0.47181008902077154, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.4629080118694362\n",
      "시가대비 best profits_accuracy =  0\n",
      "시가대비 자산 배분 반영 best profits =  0.0\n",
      "WARNING:tensorflow:Layer lstm_9 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_11 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.00020605623700115813, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(6.1027241886770756e-05, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(4.555343347147941e-12, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[0.9999999]], shape=(1, 1), dtype=float32)\n",
      "구간 3 학습 종료.......\n",
      "2020-11-04:09:09:34\n",
      "종가대비 예측확률 =  tf.Tensor(0.49851632047477745, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.47181008902077154\n",
      "시가대비 best profits_accuracy =  0\n",
      "시가대비 자산 배분 반영 best profits =  0.0\n",
      "WARNING:tensorflow:Layer lstm_12 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_14 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(3.24038457560026e-05, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(1.3968775204970847e-07, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(2.247323461400628e-12, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 4 학습 종료.......\n",
      "2020-11-04:13:06:15\n",
      "종가대비 예측확률 =  tf.Tensor(0.5074183976261127, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.4688427299703264\n",
      "시가대비 best profits_accuracy =  0\n",
      "시가대비 자산 배분 반영 best profits =  0.0\n",
      "WARNING:tensorflow:Layer lstm_15 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_17 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(8.282846373816465e-12, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.0006109866217796288, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(3.2914268206404363e-12, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 5 학습 종료.......\n",
      "2020-11-04:17:02:37\n",
      "종가대비 예측확률 =  tf.Tensor(0.4688427299703264, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.4896142433234421\n",
      "시가대비 best profits_accuracy =  0\n",
      "시가대비 자산 배분 반영 best profits =  0.0\n",
      "WARNING:tensorflow:Layer lstm_18 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_20 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(9.095998796166503e-05, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(9.172365198295165e-05, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(1.0249288923183567e-11, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[0.99999964]], shape=(1, 1), dtype=float32)\n",
      "구간 6 학습 종료.......\n",
      "2020-11-04:20:50:13\n",
      "종가대비 예측확률 =  tf.Tensor(0.4896142433234421, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.47774480712166173\n",
      "시가대비 best profits_accuracy =  0.47774480712166173\n",
      "시가대비 자산 배분 반영 best profits =  [8.617082]\n",
      "WARNING:tensorflow:Layer lstm_21 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_23 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(6.534507115434066e-05, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.0005075690824568639, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(5.301121625515322e-12, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 7 학습 종료.......\n",
      "2020-11-05:00:49:28\n",
      "종가대비 예측확률 =  tf.Tensor(0.4807121661721068, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.4658753709198813\n",
      "시가대비 best profits_accuracy =  0.47774480712166173\n",
      "시가대비 자산 배분 반영 best profits =  [8.617082]\n",
      "WARNING:tensorflow:Layer lstm_24 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_26 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.0005338956909224734, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(1.3653783379238477e-05, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(3.656375218649038e-12, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 8 학습 종료.......\n",
      "2020-11-05:04:48:26\n",
      "종가대비 예측확률 =  tf.Tensor(0.49258160237388726, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.4599406528189911\n",
      "시가대비 best profits_accuracy =  0.47774480712166173\n",
      "시가대비 자산 배분 반영 best profits =  [8.617082]\n",
      "WARNING:tensorflow:Layer lstm_27 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_29 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.00016154605972773056, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(5.701704196237135e-07, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(3.616060911318024e-13, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 9 학습 종료.......\n",
      "2020-11-05:08:20:36\n",
      "종가대비 예측확률 =  tf.Tensor(0.4688427299703264, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.4658753709198813\n",
      "시가대비 best profits_accuracy =  0.47774480712166173\n",
      "시가대비 자산 배분 반영 best profits =  [8.617082]\n",
      "WARNING:tensorflow:Layer lstm_30 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_32 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(1.4201576177235722e-11, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.003041210706586008, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(6.0146785222595804e-12, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 10 학습 종료.......\n",
      "2020-11-05:12:18:46\n",
      "종가대비 예측확률 =  tf.Tensor(0.49554896142433236, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.44510385756676557\n",
      "시가대비 best profits_accuracy =  0.47774480712166173\n",
      "시가대비 자산 배분 반영 best profits =  [8.617082]\n",
      "WARNING:tensorflow:Layer lstm_33 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_35 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.002263924960238951, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.0003837039644810889, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.02445929937587429, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 11 학습 종료.......\n",
      "2020-11-05:16:16:02\n",
      "종가대비 예측확률 =  tf.Tensor(0.516320474777448, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.49851632047477745\n",
      "시가대비 best profits_accuracy =  0.47774480712166173\n",
      "시가대비 자산 배분 반영 best profits =  [8.617082]\n",
      "WARNING:tensorflow:Layer lstm_36 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_38 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(3.903723948249511e-12, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(8.638217716456259e-05, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(3.562606394066661e-12, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 12 학습 종료.......\n",
      "2020-11-05:19:38:14\n",
      "종가대비 예측확률 =  tf.Tensor(0.49851632047477745, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.456973293768546\n",
      "시가대비 best profits_accuracy =  0.47774480712166173\n",
      "시가대비 자산 배분 반영 best profits =  [8.617082]\n",
      "WARNING:tensorflow:Layer lstm_39 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_41 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(1.4830865672265408e-05, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(8.692819522411902e-05, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(4.381332528049646e-14, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 13 학습 종료.......\n",
      "2020-11-05:22:59:09\n",
      "종가대비 예측확률 =  tf.Tensor(0.4332344213649852, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.4391691394658754\n",
      "시가대비 best profits_accuracy =  0.47774480712166173\n",
      "시가대비 자산 배분 반영 best profits =  [8.617082]\n",
      "WARNING:tensorflow:Layer lstm_42 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_44 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(3.241415321258611e-05, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.0026115383429893353, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(1.9431476341285917e-12, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 14 학습 종료.......\n",
      "2020-11-06:02:58:37\n",
      "종가대비 예측확률 =  tf.Tensor(0.5133531157270029, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.5103857566765578\n",
      "시가대비 best profits_accuracy =  0.5103857566765578\n",
      "시가대비 자산 배분 반영 best profits =  [30.287588]\n",
      "WARNING:tensorflow:Layer lstm_45 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_47 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(1.7924291219608564e-05, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(5.644934348525727e-07, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(3.6799834388476115e-12, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 15 학습 종료.......\n",
      "2020-11-06:07:01:58\n",
      "종가대비 예측확률 =  tf.Tensor(0.49851632047477745, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.4540059347181009\n",
      "시가대비 best profits_accuracy =  0.5103857566765578\n",
      "시가대비 자산 배분 반영 best profits =  [30.287588]\n",
      "WARNING:tensorflow:Layer lstm_48 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_50 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.0007309223306806625, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(9.242818769310162e-11, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(1.3469854960234064e-10, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 16 학습 종료.......\n",
      "2020-11-06:10:31:47\n",
      "종가대비 예측확률 =  tf.Tensor(0.5400593471810089, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.456973293768546\n",
      "시가대비 best profits_accuracy =  0.5103857566765578\n",
      "시가대비 자산 배분 반영 best profits =  [30.287588]\n",
      "WARNING:tensorflow:Layer lstm_51 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_53 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.0043114918943117815, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(1.659976345348182e-05, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(3.328738304839719e-12, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 17 학습 종료.......\n",
      "2020-11-06:14:01:24\n",
      "종가대비 예측확률 =  tf.Tensor(0.49554896142433236, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.4540059347181009\n",
      "시가대비 best profits_accuracy =  0.5103857566765578\n",
      "시가대비 자산 배분 반영 best profits =  [30.287588]\n",
      "WARNING:tensorflow:Layer lstm_54 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_56 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.000789301936603335, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(2.376985544951724e-12, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(8.678581943646186e-12, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 18 학습 종료.......\n",
      "2020-11-06:17:56:53\n",
      "종가대비 예측확률 =  tf.Tensor(0.5074183976261127, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.4836795252225519\n",
      "시가대비 best profits_accuracy =  0.5103857566765578\n",
      "시가대비 자산 배분 반영 best profits =  [30.287588]\n",
      "WARNING:tensorflow:Layer lstm_57 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_59 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.0015603967603551522, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(9.918181954513739e-13, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(3.4971424239337254e-12, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 19 학습 종료.......\n",
      "2020-11-06:21:52:56\n",
      "종가대비 예측확률 =  tf.Tensor(0.4391691394658754, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.49851632047477745\n",
      "시가대비 best profits_accuracy =  0.5103857566765578\n",
      "시가대비 자산 배분 반영 best profits =  [30.287588]\n",
      "WARNING:tensorflow:Layer lstm_60 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_62 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.0006068791225951525, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.0005076284099552057, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.0844897701698962, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 20 학습 종료.......\n",
      "2020-11-07:01:50:28\n",
      "종가대비 예측확률 =  tf.Tensor(0.516320474777448, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.4896142433234421\n",
      "시가대비 best profits_accuracy =  0.5103857566765578\n",
      "시가대비 자산 배분 반영 best profits =  [30.287588]\n",
      "WARNING:tensorflow:Layer lstm_63 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_65 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.00016734595363639455, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(5.060051102985643e-13, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.02445905545863046, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 21 학습 종료.......\n",
      "2020-11-07:05:18:57\n",
      "종가대비 예측확률 =  tf.Tensor(0.5044510385756676, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.456973293768546\n",
      "시가대비 best profits_accuracy =  0.5103857566765578\n",
      "시가대비 자산 배분 반영 best profits =  [30.287588]\n",
      "WARNING:tensorflow:Layer lstm_66 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_68 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(3.246925257526924e-05, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.00024100792932899896, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(2.433834688754406e-11, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 22 학습 종료.......\n",
      "2020-11-07:08:53:25\n",
      "종가대비 예측확률 =  tf.Tensor(0.4421364985163205, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.4896142433234421\n",
      "시가대비 best profits_accuracy =  0.5103857566765578\n",
      "시가대비 자산 배분 반영 best profits =  [30.287588]\n",
      "WARNING:tensorflow:Layer lstm_69 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_71 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(1.0301282070530847e-11, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.00024093620092679676, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(6.1853785664205025e-12, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 23 학습 종료.......\n",
      "2020-11-07:12:51:45\n",
      "종가대비 예측확률 =  tf.Tensor(0.5014836795252225, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.49258160237388726\n",
      "시가대비 best profits_accuracy =  0.5103857566765578\n",
      "시가대비 자산 배분 반영 best profits =  [30.287588]\n",
      "WARNING:tensorflow:Layer lstm_72 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_74 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.0007306946838802071, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(1.6545158967289512e-05, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(4.513805343920675e-12, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 24 학습 종료.......\n",
      "2020-11-07:16:53:35\n",
      "종가대비 예측확률 =  tf.Tensor(0.5489614243323442, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.4807121661721068\n",
      "시가대비 best profits_accuracy =  0.5103857566765578\n",
      "시가대비 자산 배분 반영 best profits =  [30.287588]\n",
      "WARNING:tensorflow:Layer lstm_75 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_77 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.00046118493787169226, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(2.0961983823016693e-06, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(5.9324683670783844e-12, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 25 학습 종료.......\n",
      "2020-11-07:20:20:27\n",
      "종가대비 예측확률 =  tf.Tensor(0.47181008902077154, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.47181008902077154\n",
      "시가대비 best profits_accuracy =  0.5103857566765578\n",
      "시가대비 자산 배분 반영 best profits =  [30.287588]\n",
      "WARNING:tensorflow:Layer lstm_78 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_80 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(1.5431787293993044e-07, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(1.4818908074464933e-07, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.06310664104151588, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 26 학습 종료.......\n",
      "2020-11-07:23:47:20\n",
      "종가대비 예측확률 =  tf.Tensor(0.486646884272997, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.486646884272997\n",
      "시가대비 best profits_accuracy =  0.5103857566765578\n",
      "시가대비 자산 배분 반영 best profits =  [30.287588]\n",
      "WARNING:tensorflow:Layer lstm_81 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_83 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(4.8728389809552415e-11, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.0007861447186816974, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(2.5816154980808916e-12, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 27 학습 종료.......\n",
      "2020-11-08:03:43:50\n",
      "종가대비 예측확률 =  tf.Tensor(0.5311572700296736, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.49258160237388726\n",
      "시가대비 best profits_accuracy =  0.5103857566765578\n",
      "시가대비 자산 배분 반영 best profits =  [30.287588]\n",
      "WARNING:tensorflow:Layer lstm_84 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_86 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(3.2635596791584204e-05, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(2.128856939714205e-05, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(6.779792194725231e-05, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 28 학습 종료.......\n",
      "2020-11-08:07:15:14\n",
      "종가대비 예측확률 =  tf.Tensor(0.5222551928783383, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.4688427299703264\n",
      "시가대비 best profits_accuracy =  0.5103857566765578\n",
      "시가대비 자산 배분 반영 best profits =  [30.287588]\n",
      "WARNING:tensorflow:Layer lstm_87 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_89 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(3.2418140890304774e-05, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.00038372464559580013, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(3.751911057771689e-12, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 29 학습 종료.......\n",
      "2020-11-08:11:16:47\n",
      "종가대비 예측확률 =  tf.Tensor(0.5014836795252225, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.47774480712166173\n",
      "시가대비 best profits_accuracy =  0.5103857566765578\n",
      "시가대비 자산 배분 반영 best profits =  [30.287588]\n",
      "WARNING:tensorflow:Layer lstm_90 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_92 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.0009032773386217607, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(1.9761573273491164e-07, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(1.3229765202257313e-13, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 30 학습 종료.......\n",
      "2020-11-08:14:51:32\n",
      "종가대비 예측확률 =  tf.Tensor(0.5103857566765578, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.4658753709198813\n",
      "시가대비 best profits_accuracy =  0.5103857566765578\n",
      "시가대비 자산 배분 반영 best profits =  [30.287588]\n",
      "WARNING:tensorflow:Layer lstm_93 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_95 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.00010822625755373537, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(1.4263990212813822e-07, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(2.013861972329977e-11, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 31 학습 종료.......\n",
      "2020-11-08:18:36:21\n",
      "종가대비 예측확률 =  tf.Tensor(0.5341246290801187, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.47774480712166173\n",
      "시가대비 best profits_accuracy =  0.5103857566765578\n",
      "시가대비 자산 배분 반영 best profits =  [30.287588]\n",
      "WARNING:tensorflow:Layer lstm_96 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_98 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(1.7559596259216616e-07, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.00025494249278762817, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(1.2408820182690347e-12, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 32 학습 종료.......\n",
      "2020-11-08:22:06:36\n",
      "종가대비 예측확률 =  tf.Tensor(0.4629080118694362, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.456973293768546\n",
      "시가대비 best profits_accuracy =  0.5103857566765578\n",
      "시가대비 자산 배분 반영 best profits =  [30.287588]\n",
      "WARNING:tensorflow:Layer lstm_99 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_101 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(6.461476965890412e-11, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.0002566169706260724, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.08450088495695679, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 33 학습 종료.......\n",
      "2020-11-09:01:40:44\n",
      "종가대비 예측확률 =  tf.Tensor(0.47477744807121663, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.47477744807121663\n",
      "시가대비 best profits_accuracy =  0.5103857566765578\n",
      "시가대비 자산 배분 반영 best profits =  [30.287588]\n",
      "WARNING:tensorflow:Layer lstm_102 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_104 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(3.240807445776764e-05, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(1.4350151883999747e-07, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(5.135200251889594e-12, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 34 학습 종료.......\n",
      "2020-11-09:05:45:07\n",
      "종가대비 예측확률 =  tf.Tensor(0.5014836795252225, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.4629080118694362\n",
      "시가대비 best profits_accuracy =  0.5103857566765578\n",
      "시가대비 자산 배분 반영 best profits =  [30.287588]\n",
      "WARNING:tensorflow:Layer lstm_105 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_107 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(3.697328582178836e-05, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(1.4422926151416552e-07, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(0.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(0.024459692261147314, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 35 학습 종료.......\n",
      "2020-11-09:09:45:16\n",
      "종가대비 예측확률 =  tf.Tensor(0.4896142433234421, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.47774480712166173\n",
      "시가대비 best profits_accuracy =  0.5103857566765578\n",
      "시가대비 자산 배분 반영 best profits =  [30.287588]\n",
      "WARNING:tensorflow:Layer lstm_108 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_110 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(3.251677511380619e-05, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2019-10-24~2019-10-24\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(1.8676036250490877e-07, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "test dates 2020-03-19~2020-03-19\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(2.4811940505578357e-10, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "구간 36 학습 종료.......\n",
      "2020-11-09:13:17:15\n",
      "종가대비 예측확률 =  tf.Tensor(0.4629080118694362, shape=(), dtype=float64)\n",
      "시가대비 예측확률 =  0.47477744807121663\n",
      "시가대비 best profits_accuracy =  0.5103857566765578\n",
      "시가대비 자산 배분 반영 best profits =  [30.287588]\n",
      "WARNING:tensorflow:Layer lstm_111 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_113 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "test dates 2019-05-29~2019-05-29\n",
      "prediction1 accuracy =  tf.Tensor(1.0, shape=(), dtype=float64)\n",
      "model2 loss =  tf.Tensor(7.989397164155553e-05, shape=(), dtype=float64)\n",
      "optimal shares =  tf.Tensor([[1.]], shape=(1, 1), dtype=float32)\n",
      "WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-45f5dba6a798>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mearly_stopping1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;31m#model1.load_weights(checkpoint_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m         \u001b[0mmodel1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain1_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain1_y_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mearly_stopping1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_y_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;31m#model1.save_weights(checkpoint_path)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    597\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"학습 시작\")\n",
    "print(datetime.now().strftime(\"%Y-%m-%d:%H:%M:%S\"))\n",
    "\n",
    "best_prediction1 = []\n",
    "best_prediction2 = []\n",
    "best_accu = 0\n",
    "best_profits = 0.0\n",
    "for n in range(best_pick_iter):\n",
    "    \n",
    "    gc.collect()\n",
    "\n",
    "    model1 = models.LSTM(n_timestep,input_size,n_unit,regularizers_alpha=0.01,drop_rate=0.5)\n",
    "\n",
    "    model1.compile(optimizer='adam',\n",
    "                  loss=loss_fn_model1_1)\n",
    "                      #callbacks=[cp-callback]\n",
    "                  #metrics=['accuracy'])\n",
    "    #model1.save_weights(checkpoint_path) \n",
    "    \n",
    "    # 자산 배분 모델\n",
    "    model2 = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(n_timestep, 2)),\n",
    "        tf.keras.layers.LSTM(10, return_sequences=True, kernel_initializer='he_normal', kernel_regularizer=tf.keras.regularizers.l2(0.01)),\n",
    "        #tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid'),\n",
    "    ])    \n",
    "    \n",
    "    current_train_start = df.loc[prepro.date_to_index(df, train_start), 'date']\n",
    "    current_train_end = df.loc[prepro.date_to_index(df, train_end), 'date']\n",
    "    current_test_start = df.loc[prepro.date_to_index(df, test_start), 'date']\n",
    "    current_test_end = df.loc[prepro.date_to_index(df, test_start) + trans_day - 1, 'date']\n",
    "\n",
    "    #  각 transfer 구간의 예측값들을 합치기 위하여\n",
    "    test_prediction1 = []\n",
    "    test_prediction2 = []\n",
    "    cnt = 0\n",
    "    while True:\n",
    "        cnt += 1\n",
    "        gc.collect()\n",
    "\n",
    "        \n",
    "        train_data, test_data = prepro.get_train_test_data(df, target_column, remove_columns, \n",
    "                                                               current_train_start, current_train_end,\n",
    "                                                               current_test_start, current_test_end,\n",
    "                                                               future_day, n_timestep, time_interval)\n",
    "\n",
    "        # 전체 train, test dataset 생성\n",
    "        train_x, train_y = prepro.get_LSTM_dataset(train_data, n_timestep, time_interval, input_size, future_day)\n",
    "        \n",
    "       \n",
    "        train_x = train_x[:train_end_back]\n",
    "        train_y = train_y[:train_end_back]\n",
    "\n",
    "        test_x, test_y = prepro.get_LSTM_dataset(test_data, n_timestep, time_interval, input_size, future_day)\n",
    "        test_y_target = np.expand_dims(test_y[:, :, 4], axis=2)    \n",
    "\n",
    "        # train, validation set 분리하여 train1은 예측모델로 train2는 손절값 학습 모델로 사용\n",
    "        train1_x, train2_x, train1_y, train2_y = train_test_split(train_x, train_y, test_size=0.65)\n",
    "\n",
    "\n",
    "        # the model1 training\n",
    "        train1_y_target = np.expand_dims(train1_y[:, :, 4], axis=2)\n",
    "        early_stopping1 = tf.keras.callbacks.EarlyStopping(patience=2, verbose=0)\n",
    "        #model1.load_weights(checkpoint_path)\n",
    "        model1.fit(train1_x, train1_y_target, batch_size=batch_size, verbose=0, epochs=5, callbacks=[early_stopping1], validation_data=(test_x, test_y_target))\n",
    "        #model1.save_weights(checkpoint_path)\n",
    "        \n",
    "        # model2 training\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)    \n",
    "        early_stopping2 = learn.EarlyStopping(patience=2, verbose=0)\n",
    "        iter = epochs\n",
    "        basic_epochs = tf.cast(epochs / 5, dtype=tf.int32)\n",
    "        for iteration in range(iter):\n",
    "            batch_input, batch_output = learn.next_random_batch(train2_x, train2_y, batch_size)\n",
    "\n",
    "            #gradients1 = gradient1(model1, model2, batch_input, batch_output)\n",
    "            #optimizer.apply_gradients(zip(gradients1, model1.trainable_variables))\n",
    "            gradients2 = gradient2(model1, model2, batch_input, batch_output)        \n",
    "            optimizer.apply_gradients(zip(gradients2, model2.trainable_variables))\n",
    "\n",
    "            if iteration % 100 == 0:\n",
    "\n",
    "                # model2의 test loss\n",
    "                loss2 = loss_fn_model2(model1, model2, test_x, test_y)\n",
    "            #    print('model2 loss =', loss2)   \n",
    "\n",
    "            if iteration > iter / 2 and early_stopping2.validate(loss2)==True:\n",
    "                break    \n",
    "\n",
    "        if iter > basic_epochs: iter -= basic_epochs\n",
    "        if iter < basic_epochs: iter = basic_epochs\n",
    "        \n",
    "\n",
    "        # prediction1 accuracy\n",
    "        prediction1 = model1.predict(test_x)[:, -1, 0].reshape(-1)\n",
    "\n",
    "        # reinforced prediction2\n",
    "        prediction2 = test(model1, model2, test_x, test_y)\n",
    "        \n",
    "        if cnt % 100 == 0:\n",
    "        \n",
    "            print('test dates ' + current_test_start + \"~\" + current_test_end)            \n",
    "            \n",
    "            updown = np.sign(test_y[:, -1, 4]).reshape((-1))\n",
    "            temp = tf.math.multiply(updown, prediction1)\n",
    "            accu = tf.reduce_sum(list(map(lambda x: 1 if x > 0 else 0, temp)))/len(temp)\n",
    "            print('prediction1 accuracy = ', accu)\n",
    "\n",
    "            loss2 = loss_fn_model2(model1, model2, test_x, test_y)\n",
    "            print('model2 loss = ', loss2)\n",
    "            print('optimal shares = ', prediction2)    \n",
    "              \n",
    "        test_prediction1.append(prediction1)\n",
    "        test_prediction2.append(prediction2)\n",
    "\n",
    "        # escape from while\n",
    "        if current_test_end == test_end:\n",
    "         break\n",
    "\n",
    "        #train, start dates shift\n",
    "        current_train_end = df.loc[prepro.date_to_index(df, current_train_end) + trans_day, 'date']\n",
    "        current_train_start = df.loc[prepro.date_to_index(df, current_train_end) - 1000, 'date']\n",
    "        current_test_start = df.loc[prepro.date_to_index(df, current_test_start) + trans_day, 'date']\n",
    "        if prepro.date_to_index(df, test_end) - prepro.date_to_index(df, current_test_start) < trans_day:\n",
    "            current_test_end = test_end\n",
    "        else:\n",
    "            current_test_end = df.loc[prepro.date_to_index(df, current_test_end) + trans_day, 'date']\n",
    "        \n",
    "        #en dwhile\n",
    "    \n",
    "    t1 = np.concatenate(test_prediction1)\n",
    "    t2 = np.concatenate(test_prediction2) \n",
    "    \n",
    "    print(\"구간 \" + str(n) + \" 학습 종료.......\")\n",
    "    print(datetime.now().strftime(\"%Y-%m-%d:%H:%M:%S\"))\n",
    "    \n",
    "    # 구간 n에 대한 accuracy 계산을 위한 data\n",
    "    test_start_index = prepro.date_to_index(df, test_start)\n",
    "    test_end_index = prepro.date_to_index(df, test_end)\n",
    "    test_open_prices = list(map(float, df.loc[test_start_index - future_day: test_end_index - future_day, '시가']))\n",
    "    test_base_prices = np.array(list(map(float, dataframe.loc[test_start_index - future_day: test_end_index - future_day, '종가']))) \n",
    "    test_output_prices = np.array(list(map(float, dataframe.loc[test_start_index: test_end_index, '종가'])))   \n",
    "    test_predict_prices = test_base_prices * (np.array(t1)/100 + 1)\n",
    "\n",
    "    # 전체 test_oouput 생성\n",
    "    _, test_data = prepro.get_train_test_data(df, target_column, remove_columns,\n",
    "                                                       train_start, train_end,\n",
    "                                                       test_start, test_end,\n",
    "                                                       future_day, n_timestep, time_interval)\n",
    "    _, test_y = prepro.get_LSTM_dataset(test_data, n_timestep, time_interval, input_size, future_day)    \n",
    "    \n",
    "    # 예측확률 계산 -종가 대비\n",
    "    updown = np.sign(test_y[:, -1, 4]).reshape((-1))\n",
    "    temp = tf.math.multiply(updown, t1.reshape((-1)))\n",
    "    close_accu = tf.reduce_sum(list(map(lambda x: 1 if x > 0 else 0, temp)))/len(temp)\n",
    "    print('종가대비 예측확률 = ', close_accu) \n",
    "    \n",
    "    # 예측확률 계산 -시가 대비\n",
    "    cnt = 0\n",
    "    for i in range(len(test_predict_prices)):\n",
    "        if (test_predict_prices[i]-test_open_prices[i])*(test_output_prices[i]-test_open_prices[i]) > 0:\n",
    "            cnt += 1\n",
    "    accu = cnt/len(test_predict_prices)\n",
    "    print('시가대비 예측확률 = ', accu)\n",
    "    \n",
    "    # 자산 배분 반영 손익 계산\n",
    "    profits = 0\n",
    "    for i in range(len(test_predict_prices)):\n",
    "        if test_predict_prices[i]-test_open_prices[i] > 0:\n",
    "            profits += (test_output_prices[i] - test_open_prices[i])*t2[i]\n",
    "        else:\n",
    "            profits += (test_open_prices[i] - test_output_prices[i])*t2[i]    \n",
    "    \n",
    "    # best 에측 확률 저장\n",
    "    if profits > best_profits: \n",
    "        best_accu = accu\n",
    "        best_prediction1 = t1\n",
    "        best_prediction2 = t2\n",
    "        best_profits = profits\n",
    "        \n",
    "        #save model1\n",
    "        model1.save_weights(checkpoint_path)\n",
    "\n",
    "    print('시가대비 best profits_accuracy = ', best_accu)\n",
    "    print('시가대비 자산 배분 반영 best profits = ', best_profits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updown = np.sign(test_y[:, -1, 4]).reshape((-1))\n",
    "#calculate accuracy\n",
    "temp = tf.math.multiply(updown, best_prediction1.reshape((-1)))\n",
    "accu = tf.reduce_sum(list(map(lambda x: 1 if x > 0 else 0, temp)))/len(temp)\n",
    "print('종가대비 best 예측확률 = ', accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원 지수, 날짜는 target conversion이 되기 전 dataframe에서 가져옴 - 종가를 test base price로 하는 경우\n",
    "test_dates, test_base_prices, train_dates, train_base_prices = prepro.get_test_dates_prices(dataframe, test_start, test_end,\n",
    "                                                      train_start, train_end, n_timestep, time_interval, future_day, target_column)\n",
    "result = GenerateResult(best_prediction1, best_prediction2[:, 1].reshape(-1), test_y[:, -1, 4].reshape(-1), test_dates, n_timestep, future_day, train_end_back, trans_day)\n",
    "\n",
    "#result.extract_last_output()\n",
    "result.convert_price(test_base_prices,conversion_type=target_type)\n",
    "\n",
    "# 손익 계산\n",
    "test_start_index = prepro.date_to_index(df, test_start)\n",
    "test_end_index = prepro.date_to_index(df, test_end)\n",
    "test_open_prices = list(map(float, df.loc[test_start_index - future_day: test_end_index - future_day, '시가']))\n",
    "test_high_prices = list(map(float, df.loc[test_start_index - future_day: test_end_index - future_day, '고가']))\n",
    "test_low_prices = list(map(float, df.loc[test_start_index - future_day: test_end_index - future_day, '저가']))\n",
    "\n",
    "profits = np.zeros(len(test_dates))\n",
    "for i in range(len(test_dates)):\n",
    "    if result.test_predict_price[i]-test_open_prices[i] > 0:\n",
    "        profits[i] = result.test_output_price[i] - test_open_prices[i]\n",
    "    else:\n",
    "        profits[i] = test_open_prices[i] - result.test_output_price[i]\n",
    "\n",
    "# 투자 비율 반영 손익 계산\n",
    "profits2 = np.zeros(len(test_dates))\n",
    "for i in range(len(test_dates)):\n",
    "    if result.test_predict_price[i]-test_open_prices[i] > 0:\n",
    "        profits2[i] = (result.test_output_price[i] - test_open_prices[i])*t2[i]\n",
    "    else:\n",
    "        profits2[i] = (test_open_prices[i] - result.test_output_price[i])*t2[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('손익 = ', tf.reduce_sum(profits))\n",
    "print('투자 비율 반영 손익 = ', tf.reduce_sum(profits2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array(result.test_predict_price) - np.array(test_open_prices)\n",
    "trues = np.array(result.test_output_price) - np.array(test_open_prices)\n",
    "result.evaluation(preds, trues)\n",
    "result.table(np.array(test_open_prices), np.array(test_high_prices), np.array(test_low_prices), profits, profits2)\n",
    "result.save_result(model_name,item_name,n_unit,target_type,batch_size,n_timestep,time_interval,epochs,str(alpha),comment)\n",
    "result.save_visualization()\n",
    "#result.save_model(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "current_train_start = df.loc[prepro.date_to_index(df, train_start), 'date']\n",
    "current_train_end = df.loc[prepro.date_to_index(df, train_end), 'date']\n",
    "current_test_start = df.loc[prepro.date_to_index(df, test_start), 'date']\n",
    "current_test_end = df.loc[prepro.date_to_index(df, test_start) + trans_day - 1, 'date']\n",
    "\n",
    "\n",
    "#  각 transfer 구간의 예측값들을 합치기 위하여\n",
    "test_prediction1 = []\n",
    "test_prediction2 = []\n",
    "test_target = []\n",
    "\n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(patience=3, verbose=1)\n",
    "early_stopping = learn.EarlyStopping(patience=2, verbose=1)\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "train_data, test_data = prepro.get_train_test_data(df, target_column, remove_columns, \n",
    "                                                       current_train_start, current_train_end,\n",
    "                                                       current_test_start, current_test_end,\n",
    "                                                       future_day, n_timestep, time_interval)\n",
    "\n",
    "# input_size, columns reset\n",
    "input_size = len(df.columns) - len(remove_columns)\n",
    "input_columns = df.columns.copy()\n",
    "\n",
    "train_x, train_y = prepro.get_LSTM_dataset(train_data, n_timestep, time_interval, input_size, future_day)\n",
    "test_x, test_y = prepro.get_LSTM_dataset(test_data, n_timestep, time_interval, input_size, future_day)\n",
    "\n",
    "#model.compile(optimizer='adam',\n",
    "#          loss=loss_fn)\n",
    "#          #callbacks=[cp-callback]\n",
    "#          #metrics=['accuracy'])\n",
    "\n",
    "# the firs training dataset\n",
    "train_x = train_x[:-future_day]\n",
    "train_y = train_y[:-future_day]    \n",
    "\n",
    "#global_step = tf.train.get_or_create_global_step()\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "#lr_decay = tf.train.exponential_decay(learning_rate, global_step,\n",
    "#                                      train_input.shape[0]/batch_size*5, 0.5, staircase=True)\n",
    "lr_decay = tf.compat.v1.train.exponential_decay(learning_rate,global_step, int(len(train_y)/batch_size), 0.96, staircase=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "#optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "updown = np.sign(test_y[:, -1, 0]).reshape((-1))    \n",
    "epochs = len(train_y)\n",
    "for iteration in range(399):\n",
    "    batch_input, batch_output = learn.next_random_interval_batch(train_x, train_y, batch_size, future_day)\n",
    "\n",
    "    #noise = 2*np.random.randn(batch_size,n_timestep,1)\n",
    "    #batch_output = batch_output+noise\n",
    "    #batch_input = encoder(train_input[idx])\n",
    "    gradients1 = gradient1(model1, model2, batch_input, batch_output)\n",
    "    optimizer.apply_gradients(zip(gradients1, model1.trainable_variables))\n",
    "    \n",
    "    targets = tf.reshape(train_y[:, -1, 0], [-1])\n",
    "    rates = targets / 100\n",
    "    preds = tf.reshape(model1(train_x, training=False)[:, -1, 0], [-1])\n",
    "    \n",
    "    n = len(targets)    \n",
    "    returns = [1.0]\n",
    "    losses = []\n",
    "    for i in range(n - 1):\n",
    "       \n",
    "        # average_return, std of returns, remaining days, preds[0] \n",
    "        state = []\n",
    "        \n",
    "        random_rates = []\n",
    "        for k in range(i+1):\n",
    "            random_rates.append(rates[k])\n",
    "        # 실재 수익률을 기반으로 random 수익률 생성, 예측에 의한 porfits 생성 \n",
    "        for k in range(i+1, n):\n",
    "            random_rates.append(tf.random.normal((), mean=targets[k] / 100, stddev=0.1, dtype=tf.float64))\n",
    "        profits = tf.convert_to_tensor(random_rates, dtype=tf.float64)*tf.cast(preds, dtype=tf.float64) + 1\n",
    "        \n",
    "        # 현재까지의 예측에 의한 수익률 기하평균 구하기\n",
    "        returns_past = []\n",
    "        for k in range(i+1):\n",
    "            returns_past.append(profits[k])\n",
    "        avg_return = tf.cast(tf.math.reduce_prod(returns_past)**(1/(i+1)), dtype=tf.float64)\n",
    "        \n",
    "        state.append(avg_return)\n",
    "        state.append(tf.math.reduce_std(returns))\n",
    "        state.append((n - i) / n)\n",
    "        state.append(preds[i])\n",
    "        state = np.array(state).reshape((1, 4))\n",
    "     \n",
    "        # 목표일까지의 기대 기하 평균 수익률 구하기 \n",
    "        returns_future = []\n",
    "        for j in range(i+1, n):\n",
    "            returns_future.append(profits[j])\n",
    "        avg_return_future = tf.math.reduce_prod(returns_future)**(1/(n-i-1))\n",
    "\n",
    "        # 예측 기하 평균 수익률과 기대 기하 평균 수익률의 MSE\n",
    "        value = tf.cast(model2(state, training=True)[0, 0], dtype=tf.float64)\n",
    "        losses.append((value - avg_return_future)**2)\n",
    "        if n == 3: break\n",
    "    print(\"losses\", losses)\n",
    "    print(\"value\", value)    \n",
    "    with tf.GradientTape() as tape:\n",
    "        gradients2 = tape.gradient(tf.math.reduce_sum(losses), model2.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients2, model2.trainable_variables))\n",
    "    \n",
    "\n",
    "    \n",
    "    if iteration % 100 == 0:\n",
    "        #test_MSE = model.evaluate(test_x, test_y)\n",
    "        prediction = model1.predict(test_x)\n",
    "        prediction_MSE = sum((updown - np.sign(prediction[:, -1, 0]).reshape(-1))**2)/len(test_y)\n",
    "        print('prediction_MSE =', prediction_MSE)\n",
    "\n",
    "    if iteration > epochs / 2 and early_stopping.validate(prediction_MSE)==True:\n",
    "        break\n",
    "epochs -= epochs / 5\n",
    "if epochs <= 0: epochs = 100\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow)",
   "language": "python",
   "name": "conda_tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
