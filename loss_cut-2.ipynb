{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2020 Bimghi Choi. All Rights Reserved.\n",
    "# 예측 + 투자전략 시스템\n",
    "\n",
    "# _*_ coding: utf-8 _*_\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import os\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]='0, 1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import preprocess as prepro\n",
    "import models\n",
    "import learn\n",
    "from learn import GenerateResult\n",
    "import math\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_graphs(history, metric):\n",
    "  plt.plot(history.history[metric])\n",
    "  plt.plot(history.history['val_'+metric], '')\n",
    "  plt.xlabel(\"Epochs\")\n",
    "  plt.ylabel(metric)\n",
    "  plt.legend([metric, 'val_'+metric])\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  # 텐서플로가 첫 번째 GPU에 1GB 메모리만 할당하도록 제한\n",
    "  try:\n",
    "#    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')    \n",
    "    tf.config.experimental.set_visible_devices(gpus[1], 'GPU')    \n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[1],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])\n",
    "  except RuntimeError as e:\n",
    "    # 프로그램 시작시에 가장 장치가 설정되어야만 합니다\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = '~/Data/kospi200f_809_0515.csv'\n",
    "item_name = 'kospi200f_loss_cut'\n",
    "train_start = '2000-01-31'\n",
    "train_end = '2019-01-02'\n",
    "test_start = '2019-01-02'\n",
    "test_end = '2020-05-15'\n",
    "\n",
    "remove_columns = ['date', '종가']\n",
    "target_column = '종가'\n",
    "input_columns = []\n",
    "target_type = 'rate'\n",
    "\n",
    "model_name = 'loss_cut-2'\n",
    "channel = False\n",
    "\n",
    "trans_day = 20\n",
    "\n",
    "target_alpha = 100\n",
    "future_day = 20\n",
    "train_end_back = -1\n",
    "n_timestep = 30\n",
    "max_time_interval = 11\n",
    "time_interval = 1\n",
    "input_size = 809\n",
    "n_unit = 500\n",
    "batch_size = 30\n",
    "learning_rate = 0.0005\n",
    "epochs = 500\n",
    "\n",
    "alpha = 0.005\n",
    "beta = 0.001\n",
    "\n",
    "comment = \"1일 에측을 위해 2일 예측 학습, 모델1은 지수예측(MSE) 모델2는 최적의 손절값 예측, 모델2 input = 모델1 input + output + profits\"\n",
    "\n",
    "from datetime import datetime\n",
    "now = datetime.now().strftime('%Y-%m-%d-%H:%M:%S')\n",
    "checkpoint_path = model_name + \"/pred\"+str(future_day)+\":\"+now+\".ckpt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = util.read_datafile(file_name)\n",
    "df = dataframe.copy()\n",
    "#df = prepro.target_conversion(df, target_column, future_day, type=target_type)\n",
    "a = []\n",
    "b = []\n",
    "for i in range(len(df[target_column]) -future_day):\n",
    "    df.loc[i, target_column] = ((df.loc[i + future_day, target_column] - df.loc[i, target_column]) \n",
    "                                / df.loc[i, target_column]) * target_alpha \n",
    "    df.loc[i, '시가'] = df.loc[i + future_day + train_end_back + 1, '시가']\n",
    "    a.append(max(df.loc[i  + future_day + train_end_back + 1:i + future_day + 1, '고가']))\n",
    "    b.append(min(df.loc[i  + future_day + train_end_back + 1:i + future_day + 1, '저가']))\n",
    "for i in range(len(df['종가']) - future_day):\n",
    "    df.loc[i, '고가'] = a[i]\n",
    "    df.loc[i, '저가'] = b[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>시가지수(포인트)</th>\n",
       "      <th>고가지수(포인트)</th>\n",
       "      <th>저가지수(포인트)</th>\n",
       "      <th>종가지수(포인트)</th>\n",
       "      <th>수익률(%)</th>\n",
       "      <th>수익률 (1주)(%)</th>\n",
       "      <th>수익률 (1개월)(%)</th>\n",
       "      <th>수익률 (3개월)(%)</th>\n",
       "      <th>수익률 (6개월)(%)</th>\n",
       "      <th>...</th>\n",
       "      <th>주요상품선물_천연가스(선물, NYMEX)($/mmBtu)</th>\n",
       "      <th>주요상품선물_금(선물)($/ounce)</th>\n",
       "      <th>주요상품선물_은(선물)($/ounce)</th>\n",
       "      <th>주요상품선물_알루미늄(선물)($/ton)</th>\n",
       "      <th>주요상품선물_옥수수(최근월물)(￠/bu)</th>\n",
       "      <th>대두박(￠/bu)</th>\n",
       "      <th>종가</th>\n",
       "      <th>시가</th>\n",
       "      <th>고가</th>\n",
       "      <th>저가</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-31</td>\n",
       "      <td>-0.218225</td>\n",
       "      <td>-0.493106</td>\n",
       "      <td>-0.400439</td>\n",
       "      <td>-0.223565</td>\n",
       "      <td>0.212872</td>\n",
       "      <td>1.352924</td>\n",
       "      <td>-0.851790</td>\n",
       "      <td>-0.382658</td>\n",
       "      <td>-0.162615</td>\n",
       "      <td>...</td>\n",
       "      <td>1.653438</td>\n",
       "      <td>-0.853010</td>\n",
       "      <td>0.974513</td>\n",
       "      <td>0.827681</td>\n",
       "      <td>0.373581</td>\n",
       "      <td>0.614542</td>\n",
       "      <td>-13.921405</td>\n",
       "      <td>104.15</td>\n",
       "      <td>113.00</td>\n",
       "      <td>100.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-02-01</td>\n",
       "      <td>-0.213200</td>\n",
       "      <td>-0.230417</td>\n",
       "      <td>-0.373954</td>\n",
       "      <td>-0.595373</td>\n",
       "      <td>-0.407635</td>\n",
       "      <td>1.808646</td>\n",
       "      <td>-1.074203</td>\n",
       "      <td>-1.707405</td>\n",
       "      <td>-0.490731</td>\n",
       "      <td>...</td>\n",
       "      <td>1.687061</td>\n",
       "      <td>-0.039689</td>\n",
       "      <td>0.193571</td>\n",
       "      <td>1.002058</td>\n",
       "      <td>0.594935</td>\n",
       "      <td>0.706918</td>\n",
       "      <td>-2.514088</td>\n",
       "      <td>106.50</td>\n",
       "      <td>113.75</td>\n",
       "      <td>106.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-02-02</td>\n",
       "      <td>-0.208520</td>\n",
       "      <td>-0.448886</td>\n",
       "      <td>-0.367787</td>\n",
       "      <td>-0.200398</td>\n",
       "      <td>0.696677</td>\n",
       "      <td>2.107257</td>\n",
       "      <td>-0.769928</td>\n",
       "      <td>-1.558400</td>\n",
       "      <td>0.074389</td>\n",
       "      <td>...</td>\n",
       "      <td>1.809524</td>\n",
       "      <td>0.800355</td>\n",
       "      <td>0.031266</td>\n",
       "      <td>0.069626</td>\n",
       "      <td>0.430159</td>\n",
       "      <td>0.725768</td>\n",
       "      <td>-5.994090</td>\n",
       "      <td>112.45</td>\n",
       "      <td>114.90</td>\n",
       "      <td>109.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-02-03</td>\n",
       "      <td>-0.204125</td>\n",
       "      <td>-0.216603</td>\n",
       "      <td>-0.062403</td>\n",
       "      <td>-0.021445</td>\n",
       "      <td>0.366314</td>\n",
       "      <td>1.488978</td>\n",
       "      <td>-0.625155</td>\n",
       "      <td>-1.268930</td>\n",
       "      <td>-0.079967</td>\n",
       "      <td>...</td>\n",
       "      <td>1.213642</td>\n",
       "      <td>1.515992</td>\n",
       "      <td>0.623370</td>\n",
       "      <td>0.378121</td>\n",
       "      <td>0.084769</td>\n",
       "      <td>0.352875</td>\n",
       "      <td>-5.241090</td>\n",
       "      <td>114.35</td>\n",
       "      <td>114.90</td>\n",
       "      <td>110.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-02-07</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.378008</td>\n",
       "      <td>0.426748</td>\n",
       "      <td>0.592789</td>\n",
       "      <td>0.949260</td>\n",
       "      <td>1.101574</td>\n",
       "      <td>1.281908</td>\n",
       "      <td>-1.288431</td>\n",
       "      <td>1.273730</td>\n",
       "      <td>...</td>\n",
       "      <td>0.689954</td>\n",
       "      <td>3.856851</td>\n",
       "      <td>0.568635</td>\n",
       "      <td>-0.322023</td>\n",
       "      <td>0.518339</td>\n",
       "      <td>0.702038</td>\n",
       "      <td>-7.151614</td>\n",
       "      <td>111.45</td>\n",
       "      <td>114.75</td>\n",
       "      <td>109.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2000-02-08</td>\n",
       "      <td>-0.196110</td>\n",
       "      <td>0.353748</td>\n",
       "      <td>0.677387</td>\n",
       "      <td>0.254527</td>\n",
       "      <td>-0.366454</td>\n",
       "      <td>1.138729</td>\n",
       "      <td>0.998930</td>\n",
       "      <td>-1.329397</td>\n",
       "      <td>0.942469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.332052</td>\n",
       "      <td>2.657684</td>\n",
       "      <td>1.222974</td>\n",
       "      <td>-0.720402</td>\n",
       "      <td>0.474125</td>\n",
       "      <td>0.440887</td>\n",
       "      <td>-5.690722</td>\n",
       "      <td>110.35</td>\n",
       "      <td>115.85</td>\n",
       "      <td>109.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2000-02-09</td>\n",
       "      <td>-0.192447</td>\n",
       "      <td>0.306497</td>\n",
       "      <td>0.819824</td>\n",
       "      <td>0.658684</td>\n",
       "      <td>0.641075</td>\n",
       "      <td>1.076862</td>\n",
       "      <td>1.233568</td>\n",
       "      <td>-1.637689</td>\n",
       "      <td>0.994675</td>\n",
       "      <td>...</td>\n",
       "      <td>0.557674</td>\n",
       "      <td>3.026664</td>\n",
       "      <td>2.114708</td>\n",
       "      <td>-0.407511</td>\n",
       "      <td>0.622228</td>\n",
       "      <td>0.721716</td>\n",
       "      <td>-9.581811</td>\n",
       "      <td>113.90</td>\n",
       "      <td>115.85</td>\n",
       "      <td>110.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2000-02-10</td>\n",
       "      <td>0.330923</td>\n",
       "      <td>0.617187</td>\n",
       "      <td>0.552762</td>\n",
       "      <td>0.366383</td>\n",
       "      <td>-0.311193</td>\n",
       "      <td>0.651653</td>\n",
       "      <td>0.275024</td>\n",
       "      <td>-1.607973</td>\n",
       "      <td>0.269679</td>\n",
       "      <td>...</td>\n",
       "      <td>0.812787</td>\n",
       "      <td>3.316674</td>\n",
       "      <td>1.871659</td>\n",
       "      <td>-0.157431</td>\n",
       "      <td>0.985272</td>\n",
       "      <td>1.379493</td>\n",
       "      <td>-10.093610</td>\n",
       "      <td>113.35</td>\n",
       "      <td>113.70</td>\n",
       "      <td>102.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2000-02-11</td>\n",
       "      <td>0.977388</td>\n",
       "      <td>0.576306</td>\n",
       "      <td>0.048738</td>\n",
       "      <td>-0.013448</td>\n",
       "      <td>-0.428351</td>\n",
       "      <td>0.326747</td>\n",
       "      <td>0.131530</td>\n",
       "      <td>-1.828185</td>\n",
       "      <td>0.546650</td>\n",
       "      <td>...</td>\n",
       "      <td>0.675096</td>\n",
       "      <td>2.384344</td>\n",
       "      <td>0.988933</td>\n",
       "      <td>-0.154629</td>\n",
       "      <td>0.473525</td>\n",
       "      <td>0.907189</td>\n",
       "      <td>-15.059992</td>\n",
       "      <td>108.85</td>\n",
       "      <td>108.85</td>\n",
       "      <td>102.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2000-02-14</td>\n",
       "      <td>-0.649462</td>\n",
       "      <td>-0.806483</td>\n",
       "      <td>-0.954529</td>\n",
       "      <td>-1.222378</td>\n",
       "      <td>-1.533955</td>\n",
       "      <td>-1.217328</td>\n",
       "      <td>-0.075927</td>\n",
       "      <td>-2.486079</td>\n",
       "      <td>0.140712</td>\n",
       "      <td>...</td>\n",
       "      <td>0.504701</td>\n",
       "      <td>1.935983</td>\n",
       "      <td>0.085505</td>\n",
       "      <td>-0.246361</td>\n",
       "      <td>0.529501</td>\n",
       "      <td>0.923675</td>\n",
       "      <td>-10.474138</td>\n",
       "      <td>102.65</td>\n",
       "      <td>105.00</td>\n",
       "      <td>99.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2000-02-15</td>\n",
       "      <td>-1.074607</td>\n",
       "      <td>-1.359121</td>\n",
       "      <td>-1.825901</td>\n",
       "      <td>-1.959771</td>\n",
       "      <td>-1.104848</td>\n",
       "      <td>-1.601324</td>\n",
       "      <td>-0.717085</td>\n",
       "      <td>-2.587555</td>\n",
       "      <td>-0.669097</td>\n",
       "      <td>...</td>\n",
       "      <td>0.904886</td>\n",
       "      <td>1.222183</td>\n",
       "      <td>-0.051403</td>\n",
       "      <td>-1.024949</td>\n",
       "      <td>0.518408</td>\n",
       "      <td>1.160291</td>\n",
       "      <td>-5.412720</td>\n",
       "      <td>101.25</td>\n",
       "      <td>106.70</td>\n",
       "      <td>99.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2000-02-16</td>\n",
       "      <td>-1.939741</td>\n",
       "      <td>-2.072462</td>\n",
       "      <td>-2.426875</td>\n",
       "      <td>-1.820694</td>\n",
       "      <td>0.171634</td>\n",
       "      <td>-1.786449</td>\n",
       "      <td>-0.703717</td>\n",
       "      <td>-2.563710</td>\n",
       "      <td>-0.401469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.590265</td>\n",
       "      <td>1.251580</td>\n",
       "      <td>0.487841</td>\n",
       "      <td>-1.605690</td>\n",
       "      <td>0.609070</td>\n",
       "      <td>0.911960</td>\n",
       "      <td>-8.745081</td>\n",
       "      <td>105.85</td>\n",
       "      <td>108.80</td>\n",
       "      <td>102.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2000-02-17</td>\n",
       "      <td>-2.062374</td>\n",
       "      <td>-1.576333</td>\n",
       "      <td>-1.627765</td>\n",
       "      <td>-1.281362</td>\n",
       "      <td>0.934836</td>\n",
       "      <td>-1.133411</td>\n",
       "      <td>-0.976304</td>\n",
       "      <td>-1.810490</td>\n",
       "      <td>1.006196</td>\n",
       "      <td>...</td>\n",
       "      <td>1.129172</td>\n",
       "      <td>1.099126</td>\n",
       "      <td>0.014284</td>\n",
       "      <td>-1.379601</td>\n",
       "      <td>0.356659</td>\n",
       "      <td>1.165770</td>\n",
       "      <td>-6.302708</td>\n",
       "      <td>108.80</td>\n",
       "      <td>108.80</td>\n",
       "      <td>105.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2000-02-18</td>\n",
       "      <td>-1.203728</td>\n",
       "      <td>-1.119424</td>\n",
       "      <td>-1.312861</td>\n",
       "      <td>-1.662198</td>\n",
       "      <td>-0.622664</td>\n",
       "      <td>-1.227625</td>\n",
       "      <td>-1.271035</td>\n",
       "      <td>-1.891012</td>\n",
       "      <td>0.605513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.911884</td>\n",
       "      <td>1.361637</td>\n",
       "      <td>0.227203</td>\n",
       "      <td>-1.264041</td>\n",
       "      <td>0.245730</td>\n",
       "      <td>1.381693</td>\n",
       "      <td>-5.587841</td>\n",
       "      <td>106.50</td>\n",
       "      <td>108.45</td>\n",
       "      <td>104.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2000-02-21</td>\n",
       "      <td>-2.011534</td>\n",
       "      <td>-2.360013</td>\n",
       "      <td>-2.117703</td>\n",
       "      <td>-2.493118</td>\n",
       "      <td>-1.243495</td>\n",
       "      <td>-1.039810</td>\n",
       "      <td>-0.905187</td>\n",
       "      <td>-2.286667</td>\n",
       "      <td>-0.665131</td>\n",
       "      <td>...</td>\n",
       "      <td>0.863101</td>\n",
       "      <td>1.274963</td>\n",
       "      <td>0.268968</td>\n",
       "      <td>-1.589226</td>\n",
       "      <td>0.185194</td>\n",
       "      <td>1.319954</td>\n",
       "      <td>1.265230</td>\n",
       "      <td>104.35</td>\n",
       "      <td>111.40</td>\n",
       "      <td>104.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2000-02-22</td>\n",
       "      <td>-2.301061</td>\n",
       "      <td>-2.151107</td>\n",
       "      <td>-2.119718</td>\n",
       "      <td>-2.142064</td>\n",
       "      <td>0.422501</td>\n",
       "      <td>-0.214320</td>\n",
       "      <td>-0.740215</td>\n",
       "      <td>-1.957417</td>\n",
       "      <td>-0.444290</td>\n",
       "      <td>...</td>\n",
       "      <td>0.121219</td>\n",
       "      <td>1.230409</td>\n",
       "      <td>0.364772</td>\n",
       "      <td>-1.172751</td>\n",
       "      <td>-0.103243</td>\n",
       "      <td>1.100675</td>\n",
       "      <td>2.237762</td>\n",
       "      <td>108.05</td>\n",
       "      <td>111.40</td>\n",
       "      <td>107.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2000-02-23</td>\n",
       "      <td>-1.670887</td>\n",
       "      <td>-1.656883</td>\n",
       "      <td>-1.569320</td>\n",
       "      <td>-1.267786</td>\n",
       "      <td>1.605126</td>\n",
       "      <td>0.506204</td>\n",
       "      <td>0.050075</td>\n",
       "      <td>-1.494619</td>\n",
       "      <td>-0.699133</td>\n",
       "      <td>...</td>\n",
       "      <td>0.160356</td>\n",
       "      <td>0.708148</td>\n",
       "      <td>-0.052186</td>\n",
       "      <td>-0.980503</td>\n",
       "      <td>-1.014657</td>\n",
       "      <td>0.126165</td>\n",
       "      <td>-2.804161</td>\n",
       "      <td>110.15</td>\n",
       "      <td>112.90</td>\n",
       "      <td>107.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2000-02-24</td>\n",
       "      <td>-1.366238</td>\n",
       "      <td>-1.422324</td>\n",
       "      <td>-1.358331</td>\n",
       "      <td>-1.490032</td>\n",
       "      <td>-0.528189</td>\n",
       "      <td>-0.294080</td>\n",
       "      <td>-0.319013</td>\n",
       "      <td>-1.472892</td>\n",
       "      <td>-1.661528</td>\n",
       "      <td>...</td>\n",
       "      <td>0.228028</td>\n",
       "      <td>0.530452</td>\n",
       "      <td>-1.233068</td>\n",
       "      <td>-0.941193</td>\n",
       "      <td>-0.976769</td>\n",
       "      <td>0.385420</td>\n",
       "      <td>2.342673</td>\n",
       "      <td>108.45</td>\n",
       "      <td>112.90</td>\n",
       "      <td>107.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2000-02-25</td>\n",
       "      <td>-1.680249</td>\n",
       "      <td>-1.665735</td>\n",
       "      <td>-1.510077</td>\n",
       "      <td>-1.461250</td>\n",
       "      <td>0.033092</td>\n",
       "      <td>0.075897</td>\n",
       "      <td>0.493437</td>\n",
       "      <td>-1.430882</td>\n",
       "      <td>-2.424417</td>\n",
       "      <td>...</td>\n",
       "      <td>0.568840</td>\n",
       "      <td>-0.081697</td>\n",
       "      <td>-2.561532</td>\n",
       "      <td>-1.090127</td>\n",
       "      <td>0.174516</td>\n",
       "      <td>-0.148781</td>\n",
       "      <td>1.515152</td>\n",
       "      <td>111.40</td>\n",
       "      <td>113.35</td>\n",
       "      <td>108.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2000-02-28</td>\n",
       "      <td>-1.697353</td>\n",
       "      <td>-2.045428</td>\n",
       "      <td>-2.011407</td>\n",
       "      <td>-2.307633</td>\n",
       "      <td>-1.966604</td>\n",
       "      <td>-0.289909</td>\n",
       "      <td>-1.729787</td>\n",
       "      <td>-1.563181</td>\n",
       "      <td>-3.097790</td>\n",
       "      <td>...</td>\n",
       "      <td>1.162350</td>\n",
       "      <td>-0.149500</td>\n",
       "      <td>-2.235868</td>\n",
       "      <td>-2.265504</td>\n",
       "      <td>-0.179609</td>\n",
       "      <td>0.061992</td>\n",
       "      <td>9.042033</td>\n",
       "      <td>110.55</td>\n",
       "      <td>114.40</td>\n",
       "      <td>109.10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 814 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  시가지수(포인트)  고가지수(포인트)  저가지수(포인트)  종가지수(포인트)    수익률(%)  \\\n",
       "0   2000-01-31  -0.218225  -0.493106  -0.400439  -0.223565  0.212872   \n",
       "1   2000-02-01  -0.213200  -0.230417  -0.373954  -0.595373 -0.407635   \n",
       "2   2000-02-02  -0.208520  -0.448886  -0.367787  -0.200398  0.696677   \n",
       "3   2000-02-03  -0.204125  -0.216603  -0.062403  -0.021445  0.366314   \n",
       "4   2000-02-07  -0.200000   0.378008   0.426748   0.592789  0.949260   \n",
       "5   2000-02-08  -0.196110   0.353748   0.677387   0.254527 -0.366454   \n",
       "6   2000-02-09  -0.192447   0.306497   0.819824   0.658684  0.641075   \n",
       "7   2000-02-10   0.330923   0.617187   0.552762   0.366383 -0.311193   \n",
       "8   2000-02-11   0.977388   0.576306   0.048738  -0.013448 -0.428351   \n",
       "9   2000-02-14  -0.649462  -0.806483  -0.954529  -1.222378 -1.533955   \n",
       "10  2000-02-15  -1.074607  -1.359121  -1.825901  -1.959771 -1.104848   \n",
       "11  2000-02-16  -1.939741  -2.072462  -2.426875  -1.820694  0.171634   \n",
       "12  2000-02-17  -2.062374  -1.576333  -1.627765  -1.281362  0.934836   \n",
       "13  2000-02-18  -1.203728  -1.119424  -1.312861  -1.662198 -0.622664   \n",
       "14  2000-02-21  -2.011534  -2.360013  -2.117703  -2.493118 -1.243495   \n",
       "15  2000-02-22  -2.301061  -2.151107  -2.119718  -2.142064  0.422501   \n",
       "16  2000-02-23  -1.670887  -1.656883  -1.569320  -1.267786  1.605126   \n",
       "17  2000-02-24  -1.366238  -1.422324  -1.358331  -1.490032 -0.528189   \n",
       "18  2000-02-25  -1.680249  -1.665735  -1.510077  -1.461250  0.033092   \n",
       "19  2000-02-28  -1.697353  -2.045428  -2.011407  -2.307633 -1.966604   \n",
       "\n",
       "    수익률 (1주)(%)  수익률 (1개월)(%)  수익률 (3개월)(%)  수익률 (6개월)(%)  ...  \\\n",
       "0      1.352924     -0.851790     -0.382658     -0.162615  ...   \n",
       "1      1.808646     -1.074203     -1.707405     -0.490731  ...   \n",
       "2      2.107257     -0.769928     -1.558400      0.074389  ...   \n",
       "3      1.488978     -0.625155     -1.268930     -0.079967  ...   \n",
       "4      1.101574      1.281908     -1.288431      1.273730  ...   \n",
       "5      1.138729      0.998930     -1.329397      0.942469  ...   \n",
       "6      1.076862      1.233568     -1.637689      0.994675  ...   \n",
       "7      0.651653      0.275024     -1.607973      0.269679  ...   \n",
       "8      0.326747      0.131530     -1.828185      0.546650  ...   \n",
       "9     -1.217328     -0.075927     -2.486079      0.140712  ...   \n",
       "10    -1.601324     -0.717085     -2.587555     -0.669097  ...   \n",
       "11    -1.786449     -0.703717     -2.563710     -0.401469  ...   \n",
       "12    -1.133411     -0.976304     -1.810490      1.006196  ...   \n",
       "13    -1.227625     -1.271035     -1.891012      0.605513  ...   \n",
       "14    -1.039810     -0.905187     -2.286667     -0.665131  ...   \n",
       "15    -0.214320     -0.740215     -1.957417     -0.444290  ...   \n",
       "16     0.506204      0.050075     -1.494619     -0.699133  ...   \n",
       "17    -0.294080     -0.319013     -1.472892     -1.661528  ...   \n",
       "18     0.075897      0.493437     -1.430882     -2.424417  ...   \n",
       "19    -0.289909     -1.729787     -1.563181     -3.097790  ...   \n",
       "\n",
       "    주요상품선물_천연가스(선물, NYMEX)($/mmBtu)  주요상품선물_금(선물)($/ounce)  \\\n",
       "0                          1.653438              -0.853010   \n",
       "1                          1.687061              -0.039689   \n",
       "2                          1.809524               0.800355   \n",
       "3                          1.213642               1.515992   \n",
       "4                          0.689954               3.856851   \n",
       "5                          0.332052               2.657684   \n",
       "6                          0.557674               3.026664   \n",
       "7                          0.812787               3.316674   \n",
       "8                          0.675096               2.384344   \n",
       "9                          0.504701               1.935983   \n",
       "10                         0.904886               1.222183   \n",
       "11                         0.590265               1.251580   \n",
       "12                         1.129172               1.099126   \n",
       "13                         0.911884               1.361637   \n",
       "14                         0.863101               1.274963   \n",
       "15                         0.121219               1.230409   \n",
       "16                         0.160356               0.708148   \n",
       "17                         0.228028               0.530452   \n",
       "18                         0.568840              -0.081697   \n",
       "19                         1.162350              -0.149500   \n",
       "\n",
       "    주요상품선물_은(선물)($/ounce)  주요상품선물_알루미늄(선물)($/ton)  주요상품선물_옥수수(최근월물)(￠/bu)  \\\n",
       "0                0.974513                0.827681                0.373581   \n",
       "1                0.193571                1.002058                0.594935   \n",
       "2                0.031266                0.069626                0.430159   \n",
       "3                0.623370                0.378121                0.084769   \n",
       "4                0.568635               -0.322023                0.518339   \n",
       "5                1.222974               -0.720402                0.474125   \n",
       "6                2.114708               -0.407511                0.622228   \n",
       "7                1.871659               -0.157431                0.985272   \n",
       "8                0.988933               -0.154629                0.473525   \n",
       "9                0.085505               -0.246361                0.529501   \n",
       "10              -0.051403               -1.024949                0.518408   \n",
       "11               0.487841               -1.605690                0.609070   \n",
       "12               0.014284               -1.379601                0.356659   \n",
       "13               0.227203               -1.264041                0.245730   \n",
       "14               0.268968               -1.589226                0.185194   \n",
       "15               0.364772               -1.172751               -0.103243   \n",
       "16              -0.052186               -0.980503               -1.014657   \n",
       "17              -1.233068               -0.941193               -0.976769   \n",
       "18              -2.561532               -1.090127                0.174516   \n",
       "19              -2.235868               -2.265504               -0.179609   \n",
       "\n",
       "    대두박(￠/bu)         종가      시가      고가      저가  \n",
       "0    0.614542 -13.921405  104.15  113.00  100.75  \n",
       "1    0.706918  -2.514088  106.50  113.75  106.35  \n",
       "2    0.725768  -5.994090  112.45  114.90  109.80  \n",
       "3    0.352875  -5.241090  114.35  114.90  110.65  \n",
       "4    0.702038  -7.151614  111.45  114.75  109.95  \n",
       "5    0.440887  -5.690722  110.35  115.85  109.95  \n",
       "6    0.721716  -9.581811  113.90  115.85  110.35  \n",
       "7    1.379493 -10.093610  113.35  113.70  102.65  \n",
       "8    0.907189 -15.059992  108.85  108.85  102.25  \n",
       "9    0.923675 -10.474138  102.65  105.00   99.15  \n",
       "10   1.160291  -5.412720  101.25  106.70   99.15  \n",
       "11   0.911960  -8.745081  105.85  108.80  102.80  \n",
       "12   1.165770  -6.302708  108.80  108.80  105.15  \n",
       "13   1.381693  -5.587841  106.50  108.45  104.15  \n",
       "14   1.319954   1.265230  104.35  111.40  104.15  \n",
       "15   1.100675   2.237762  108.05  111.40  107.30  \n",
       "16   0.126165  -2.804161  110.15  112.90  107.30  \n",
       "17   0.385420   2.342673  108.45  112.90  107.45  \n",
       "18  -0.148781   1.515152  111.40  113.35  108.75  \n",
       "19   0.061992   9.042033  110.55  114.40  109.10  \n",
       "\n",
       "[20 rows x 814 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['date', '시가지수(포인트)', '고가지수(포인트)', '저가지수(포인트)', '종가지수(포인트)', '수익률(%)',\n",
       "       '수익률 (1주)(%)', '수익률 (1개월)(%)', '수익률 (3개월)(%)', '수익률 (6개월)(%)',\n",
       "       ...\n",
       "       '주요상품선물_천연가스(선물, NYMEX)($/mmBtu)', '주요상품선물_금(선물)($/ounce)',\n",
       "       '주요상품선물_은(선물)($/ounce)', '주요상품선물_알루미늄(선물)($/ton)',\n",
       "       '주요상품선물_옥수수(최근월물)(￠/bu)', '대두박(￠/bu)', '종가', '시가', '고가', '저가'],\n",
       "      dtype='object', length=814)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_list = df.columns\n",
    "column_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas_profiling\n",
    "#data = df.loc[:, ['미국 SP 500 Index(종가)(Pt)', '거래량(주)', '외국인보유비중(%)', '종가']]\n",
    "#data = data[:-future_day]\n",
    "#pr = data.profile_report()\n",
    "#pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_start_index = max(df.loc[df['date']<=train_start].index) + time_interval*(n_timestep-1) - 1\n",
    "train_end_index = max(df.loc[df['date']<=train_end].index)\n",
    "base_prices = tf.reduce_mean(df.loc[train_start_index:train_end_index+1, '시가'])  \n",
    "\n",
    "@tf.function\n",
    "def loss_fn_model1_1(targets, preds):\n",
    "    \n",
    "    loss0 = tf,keras.losses.MSE(targets, preds)\n",
    " \n",
    "    preds = tf.reshape(preds[:, n_timestep-1, :], [-1])\n",
    "    targets = tf.reshape(targets[:, n_timestep-1, :], [-1])\n",
    "    \n",
    "   \n",
    "    if alpha != 0:\n",
    "        # add RRL cost - maximize downside sharp ratio\n",
    "\n",
    "        # 1 if (pred - base) * (target - base) > 0, -1 otherwise\n",
    "        F = tf.math.sign(targets*preds)\n",
    "        F = tf.reshape(F, [-1])\n",
    "\n",
    "        # calc returns from each step in batches\n",
    "        R = tf.math.divide(tf.math.multiply(tf.math.abs(targets), (F - 0.00003)), base_prices)\n",
    "        R = tf.reshape(R, [-1])\n",
    "\n",
    "        # calc downside sharp ratio\n",
    "\n",
    "        # downside returns\n",
    "        DR = tf.minimum(0.0, R)\n",
    "        DR = tf.reshape(DR, [-1])\n",
    "\n",
    "        # calc. downside sharp ratio\n",
    "        #s = []\n",
    "        #for i in range(batch_size):\n",
    "        #   std =  tf.keras.backend.std(DR[i, :, 0])\n",
    "        #   s.append(tf.reduce_mean(R[i, :, 0])/tf.maximum(0.01, std))\n",
    "\n",
    "        # calc. downside sharp ratio\n",
    "        loss1 = tf.reduce_mean(R) / (tf.keras.backend.std(DR) + 0.001)\n",
    "    else:\n",
    "        loss1 = 0\n",
    "\n",
    "    \"\"\"\n",
    "    # average profits, loss\n",
    "    avg_plusR = [0.0]\n",
    "    avg_minusR = [0.0]\n",
    "\n",
    "    global num_of_profits\n",
    "    global num_of_losses\n",
    "\n",
    "    num_of_profits = 0\n",
    "    num_of_losses = 0\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        res = tf.cond(R[i, num_steps - 1, 0] > 0, lambda: return_one(), lambda: return_zero())\n",
    "        if res == 1:\n",
    "            avg_plusR.append(R[i, num_steps - 1, 0])\n",
    "        else:\n",
    "            avg_minusR.append(R[i, num_steps - 1, 0])\n",
    "    avg_profit = tf.reduce_mean(avg_plusR) \n",
    "    avg_loss = tf.reduce_mean(avg_minusR) \n",
    "    \"\"\"\n",
    "\n",
    "    if beta != 0:\n",
    "        #compute maximum drawdown\n",
    "\n",
    "        #accm_profit = [0.0]\n",
    "        #for i in range(batch_size):\n",
    "        #    for j in range(num_steps):\n",
    "        #        r = tf.cond((predict_prices[i, num_steps-1, 0] - base_prices[i, num_steps-1, 0]) *\n",
    "        #                   (target_prices[i, num_steps-1, 0] - base_prices[i, num_steps-1, 0]) > 0,\n",
    "        #                   lambda: return_one(),\n",
    "        #                   lambda: return_zero())\n",
    "        #        if r == 1: accm_profit.append(accm_profit[i*num_steps + j] + tf.abs(target_prices[i, j, 0] - base_prices[i, j, 0]))\n",
    "        #        else:      accm_profit.append(accm_profit[i*num_steps + j] - tf.abs(target_prices[i, j, 0] - base_prices[i, j, 0]))\n",
    "\n",
    "        accm_profit = [0.0 for i in range(batch_size)]\n",
    "        for i in range(batch_size):\n",
    "            if i == 0:\n",
    "                accm_profit[0] = tf.sign(preds * targets) * tf.math.abs(targets)\n",
    "            else:\n",
    "                accm_profit[i] = accm_profit[i-1] + tf.sign(preds * targets) * tf.math.abs(targets)\n",
    "        loss2 = (tf.reduce_max(accm_profit) - tf.reduce_min(accm_profit))/batch_size\n",
    "    else:\n",
    "        loss2 = 0\n",
    "\n",
    "    return loss0 + beta*tf.cast(loss2, dtype=tf.float32) - alpha*tf.cast(loss1, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def loss_fn_model1(train_y, preds):\n",
    "    \n",
    "    #preds = tf.cast(preds, dtype=tf.float64)\n",
    "    #train_y = tf.cast(train_y, dtype=tf.float64)\n",
    "    rates = train_y / 100\n",
    "    updown = tf.math.sign(train_y)\n",
    "    profits = 1 + rates*tf.math.sign(preds)\n",
    "    \n",
    "    loss1 = keras.losses.MSE(preds, train_y)\n",
    "    \n",
    "    batches = tf.shape(train_y)[0]\n",
    "\n",
    "    return_plus = 0.000000000\n",
    "    return_minus = 0.000000000\n",
    "    for i in range(batches):\n",
    "        if profits[i, -1, 0] - 1 > 0.0: return_plus += profits[i, -1, 0]-1      \n",
    "        else: return_minus += 1-profits[i, -1, 0]    \n",
    "\n",
    "    loss2 = (return_plus - return_minus)/tf.cast(batches, dtype=tf.float32)\n",
    "    \n",
    "    return loss1 - loss2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def loss_fn_model2(m1, m2, train_x, train_y):\n",
    "    \n",
    "    train_y_target = np.expand_dims(train_y[:, :, 3], axis=2)\n",
    "    \n",
    "    rates = train_y_target / 100\n",
    "    updown = tf.math.sign(train_y_target)\n",
    "    preds = tf.cast(tf.math.sign(m1(train_x)), dtype=tf.float64)\n",
    "    profits = tf.cast(1 + tf.convert_to_tensor(rates, dtype=tf.float64)*preds, dtype=tf.float64)\n",
    "    \n",
    "    batches = tf.cast(train_y.shape[0], dtype=tf.int32)\n",
    "    steps = train_y.shape[1]     \n",
    "\n",
    "    targets = train_y_target.copy()\n",
    "    targets[:, -max(1, int(future_day/time_interval)):, :] = 0\n",
    "    train_x_m2 = tf.concat([train_x, preds, targets], 2)\n",
    "    loss_cuts = tf.reshape(m2(train_x_m2)[:, -1, 0]*10, [-1])\n",
    "    \n",
    "    # 시가, 고가, 저가\n",
    "    train_open_high_low = train_y[:, -1, :3].reshape((batches, 3))\n",
    "    \n",
    "    m2_targets = []\n",
    "    for i in range(batches):\n",
    "        if preds[i, -1, 0] > 0:\n",
    "            #if rates[i, -1, 0] > 0:\n",
    "                m2_targets.append((train_open_high_low[i, 0] - train_open_high_low[i, 2]))# 상승 예측시 손절값 (시가 - 저가)*a(0~1)\n",
    "            #else: m2_targets.append(abs(train_open_high_low[i, 0]*rates[i, -1, 0]/100)*0.5) # 손실*a(0~1)을 손절값으로\n",
    "        else:\n",
    "            #if rates[i, -1, 0] < 0:\n",
    "                m2_targets.append((train_open_high_low[i, 1] - train_open_high_low[i, 0]))# 하락 예측시 손절값 (고가 - 시가)*a(0~1)\n",
    "            #else: m2_targets.append(abs(train_open_high_low[i, 0]*rates[i, -1, 0]/100)*0.5) # 손실*a(0~1)을 손절값으로\n",
    "    return keras.losses.MSE(m2_targets, loss_cuts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@tf.function\n",
    "def gradient1(model1, input_data, output_data):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn_model1(model1, input_data, output_data)\n",
    "    return tape.gradient(loss, model1.trainable_variables)\n",
    "#@tf.function\n",
    "def gradient2(model1, model2, input_data, output_data):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = loss_fn_model2(model1, model2, input_data, output_data)\n",
    "    return tape.gradient(loss, model2.trainable_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "in converted code:\n\n    <ipython-input-11-5c7ae9a707bc>:86 loss_fn_model1_1  *\n        return loss0 + beta*tf.cast(loss2, dtype=tf.float32) - alpha*tf.cast(loss1, dtype=tf.float32)\n    /home/ubuntu/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py:927 r_binary_op_wrapper\n        x = ops.convert_to_tensor(x, dtype=y.dtype.base_dtype, name=\"x\")\n    /home/ubuntu/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1314 convert_to_tensor\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    /home/ubuntu/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py:1368 _autopacking_conversion_function\n        return _autopacking_helper(v, dtype, name or \"packed\")\n    /home/ubuntu/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py:1303 _autopacking_helper\n        constant_op.constant(elem, dtype=dtype, name=str(i)))\n    /home/ubuntu/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py:258 constant\n        allow_broadcast=True)\n    /home/ubuntu/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py:296 _constant_impl\n        allow_broadcast=allow_broadcast))\n    /home/ubuntu/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py:451 make_tensor_proto\n        _AssertCompatible(values, dtype)\n    /home/ubuntu/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py:331 _AssertCompatible\n        (dtype.name, repr(mismatch), type(mismatch).__name__))\n\n    TypeError: Expected float32, got <module 'tensorflow' from '/home/ubuntu/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/__init__.py'> of type 'module' instead.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b18e20bc9fca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m model1.compile(optimizer='adam',\n\u001b[0;32m---> 28\u001b[0;31m               loss=loss_fn_model1_1)\n\u001b[0m\u001b[1;32m     29\u001b[0m                   \u001b[0;31m#callbacks=[cp-callback]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m               \u001b[0;31m#metrics=['accuracy'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, distribute, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m       \u001b[0;31m# Creates the model loss and weighted metrics sub-graphs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compile_weights_loss_and_weighted_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m       \u001b[0;31m# Functions for train, test and predict will\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_compile_weights_loss_and_weighted_metrics\u001b[0;34m(self, sample_weights)\u001b[0m\n\u001b[1;32m   1590\u001b[0m       \u001b[0;31m#                   loss_weight_2 * output_2_loss_fn(...) +\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1591\u001b[0m       \u001b[0;31m#                   layer losses.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1592\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_total_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1594\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_prepare_skip_target_masks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_prepare_total_loss\u001b[0;34m(self, masks)\u001b[0m\n\u001b[1;32m   1650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'reduction'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m             \u001b[0mper_sample_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1653\u001b[0m             weighted_losses = losses_utils.compute_weighted_loss(\n\u001b[1;32m   1654\u001b[0m                 \u001b[0mper_sample_losses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/keras/losses.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, y_true, y_pred)\u001b[0m\n\u001b[1;32m    219\u001b[0m       y_pred, y_true = tf_losses_util.squeeze_or_expand_dimensions(\n\u001b[1;32m    220\u001b[0m           y_pred, y_true)\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 615\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    616\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    617\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    495\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    496\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 497\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2387\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2388\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2389\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2390\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2703\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2705\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2593\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2595\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    976\u001b[0m                                           converted_func)\n\u001b[1;32m    977\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in converted code:\n\n    <ipython-input-11-5c7ae9a707bc>:86 loss_fn_model1_1  *\n        return loss0 + beta*tf.cast(loss2, dtype=tf.float32) - alpha*tf.cast(loss1, dtype=tf.float32)\n    /home/ubuntu/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/ops/math_ops.py:927 r_binary_op_wrapper\n        x = ops.convert_to_tensor(x, dtype=y.dtype.base_dtype, name=\"x\")\n    /home/ubuntu/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/ops.py:1314 convert_to_tensor\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    /home/ubuntu/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py:1368 _autopacking_conversion_function\n        return _autopacking_helper(v, dtype, name or \"packed\")\n    /home/ubuntu/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/ops/array_ops.py:1303 _autopacking_helper\n        constant_op.constant(elem, dtype=dtype, name=str(i)))\n    /home/ubuntu/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py:258 constant\n        allow_broadcast=True)\n    /home/ubuntu/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/constant_op.py:296 _constant_impl\n        allow_broadcast=allow_broadcast))\n    /home/ubuntu/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py:451 make_tensor_proto\n        _AssertCompatible(values, dtype)\n    /home/ubuntu/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow_core/python/framework/tensor_util.py:331 _AssertCompatible\n        (dtype.name, repr(mismatch), type(mismatch).__name__))\n\n    TypeError: Expected float32, got <module 'tensorflow' from '/home/ubuntu/anaconda3/envs/tensorflow/lib/python3.7/site-packages/tensorflow/__init__.py'> of type 'module' instead.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#strategy = tf.distribute.MirroredStrategy()\n",
    "#with strategy.scope():\n",
    "\"\"\"\n",
    "model1 = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(n_timestep, input_size)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model1.summary()\n",
    "model2 = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(n_timestep, input_size+1)),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='tanh')\n",
    "])\n",
    "model2.summary()\n",
    "\"\"\"\n",
    "model1 = models.LSTM(n_timestep,input_size,n_unit,regularizers_alpha=0.01,drop_rate=0.5)\n",
    "model2 = models.LSTM_sigm(n_timestep,input_size+2,n_unit,regularizers_alpha=0.01,drop_rate=0.5)\n",
    "\n",
    "    #cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    #    checkpoint_path, verbose=1, save_weights_only=True,\n",
    "        # 다섯 번째 에포크마다 가중치를 저장합니다\n",
    "    #    save_freq=5)\n",
    "\n",
    "model1.compile(optimizer='adam',\n",
    "              loss=loss_fn_model1_1)\n",
    "                  #callbacks=[cp-callback]\n",
    "              #metrics=['accuracy'])\n",
    "model1.save_weights(checkpoint_path) \n",
    "\n",
    "#    model2 = models.LSTM(n_timestep,input_size,n_unit,regularizers_alpha=0.01,drop_rate=0.5)\n",
    "#    model2.compile(optimizer='adam',\n",
    "#                  loss='mse')\n",
    "                  #callbacks=[cp-callback]\n",
    "              #metrics=['accuracy'])            \n",
    "\n",
    "#modle_name = model_name + \"tanh\"            \n",
    "#model2.save_weights(\"modle2_\"+ checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_train_start = df.loc[prepro.date_to_index(df, train_start), 'date']\n",
    "current_train_end = df.loc[prepro.date_to_index(df, train_end), 'date']\n",
    "current_test_start = df.loc[prepro.date_to_index(df, test_start), 'date']\n",
    "current_test_end = df.loc[prepro.date_to_index(df, test_start) + trans_day - 1, 'date']\n",
    "\n",
    "\n",
    "#  각 transfer 구간의 예측값들을 합치기 위하여\n",
    "test_prediction1 = []\n",
    "test_prediction2 = []\n",
    "test_target = []\n",
    "\n",
    "print(\"학습 시작.....\")\n",
    "print(datetime.now().strftime(\"%Y-%m-%d:%H:%M:%S\"))\n",
    "while True:\n",
    "\n",
    "    gc.collect()\n",
    "    \n",
    "    time_interval = np.random.randint(1, max_time_interval)\n",
    "\n",
    "    train_data, test_data = prepro.get_train_test_data(df, target_column, remove_columns, \n",
    "                                                           current_train_start, current_train_end,\n",
    "                                                           current_test_start, current_test_end,\n",
    "                                                           future_day, n_timestep, time_interval)\n",
    "\n",
    "\n",
    "    train_x, train_y = prepro.get_LSTM_dataset(train_data, n_timestep, time_interval, input_size, future_day)\n",
    "    \n",
    "    test_x, test_y = prepro.get_LSTM_dataset(test_data, n_timestep, time_interval, input_size, future_day)\n",
    "    test_y_target = np.expand_dims(test_y[:, :, 3], axis=2)\n",
    "\n",
    "    # the model1 training dataset\n",
    "    train_x = train_x[:train_end_back]\n",
    "    train_y = train_y[:train_end_back]\n",
    "    train_y_target = np.expand_dims(train_y[:, :, 3], axis=2)\n",
    "    \n",
    "    # model1 training\n",
    "    early_stopping1 = tf.keras.callbacks.EarlyStopping(patience=2, verbose=1)\n",
    "    model1.load_weights(checkpoint_path)\n",
    "    history = model1.fit(train_x, train_y_target, batch_size=batch_size, epochs=3, callbacks=[early_stopping1], validation_data=(test_x, test_y_target))\n",
    "    model1.save_weights(checkpoint_path)\n",
    "    \n",
    "   \n",
    "    # model2 test input 생성\n",
    "    preds = tf.cast(tf.math.sign(model1(test_x)), dtype=tf.float64)\n",
    "    targets = test_y_target.copy()\n",
    "    targets[:, -max(1, int(future_day/time_interval)):, :] = 0    \n",
    "    test_x_m2 = tf.concat([test_x, preds, targets], 2)\n",
    "    \n",
    "    # model2 training\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)    \n",
    "    early_stopping2 = learn.EarlyStopping(patience=2, verbose=1)\n",
    "    iter = epochs\n",
    "    basic_epochs = tf.cast(epochs / 5, dtype=tf.int32)\n",
    "    for iteration in range(iter):\n",
    "        batch_input, batch_output = learn.next_random_interval_batch(train_x, train_y, batch_size, future_day)\n",
    "\n",
    "        gradients2 = gradient2(model1, model2, batch_input, batch_output)\n",
    "        optimizer.apply_gradients(zip(gradients2, model2.trainable_variables))\n",
    "        if iteration % 100 == 0:\n",
    "\n",
    "            # model2의 test output 생성\n",
    "            loss_cuts = tf.reshape(model2(test_x_m2)[:, -1, 0]*10, [-1])\n",
    "            print('loss cuts =', loss_cuts)       \n",
    "\n",
    "            # model2의 test loss\n",
    "            loss2 = loss_fn_model2(model1, model2, test_x, test_y)\n",
    "            print('model2 loss =', loss2)  \n",
    "\n",
    "\n",
    "        if iteration > iter / 2 and early_stopping2.validate(loss2)==True:\n",
    "            break    \n",
    "\n",
    "    if iter > basic_epochs: iter -= basic_epochs\n",
    "    if iter < basic_epochs: iter = basic_epochs\n",
    "\n",
    "    print('test dates ' + current_test_start + \"~\" + current_test_end)\n",
    "\n",
    "    # prediction1 accuracy\n",
    "    updown = np.sign(test_y[:, -1, 3]).reshape((-1))\n",
    "    prediction1 = model1.predict(test_x)[:, -1, 0].reshape(-1)\n",
    "    temp = tf.math.multiply(updown, prediction1)\n",
    "    accu = tf.reduce_sum(list(map(lambda x: 1 if x > 0 else 0, temp)))/len(temp)\n",
    "    print('prediction1 accuracy = ', accu)\n",
    "\n",
    "    # reinforced prediction2\n",
    "    preds = tf.cast(tf.math.sign(model1(test_x)), dtype=tf.float64)\n",
    "    test_x_m2 = tf.concat([test_x, preds, targets], 2)\n",
    "    prediction2 = tf.reshape(model2(test_x_m2)[:, -1, 0]*10, [-1])    \n",
    "    print('prediction2 = ', prediction2)\n",
    "\n",
    "    # loss graph\n",
    "    plot_graphs(history, 'loss')\n",
    "\n",
    "    test_prediction1.append(prediction1)\n",
    "    test_prediction2.append(prediction2)\n",
    "\n",
    "    # escape from while\n",
    "    if current_test_end == test_end:\n",
    "     break\n",
    "\n",
    "    #train, start dates shift\n",
    "    current_train_end = df.loc[prepro.date_to_index(df, current_train_end) + trans_day, 'date']\n",
    "    current_train_start = df.loc[prepro.date_to_index(df, current_train_end) - 1000, 'date']\n",
    "    current_test_start = df.loc[prepro.date_to_index(df, current_test_start) + trans_day, 'date']\n",
    "    if prepro.date_to_index(df, test_end) - prepro.date_to_index(df, current_test_start) < trans_day:\n",
    "        current_test_end = test_end\n",
    "    else:\n",
    "        current_test_end = df.loc[prepro.date_to_index(df, current_test_end) + trans_day, 'date']\n",
    "        \n",
    "print(\"학습 종료.....\")\n",
    "print(datetime.now().strftime(\"%Y-%m-%d:%H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = np.concatenate(test_prediction1)\n",
    "t2 = np.concatenate(test_prediction2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_prediction = np.concatenate(test_prediction, axis=0).reshape((-1, n_timestep, 1))\n",
    "#train_prediction = learn.predict_batch_test(model1, train_x[:batch_size], len(train_x[:batch_size]))\n",
    "\n",
    "# 원 지수, 날짜는 target conversion이 되기 전 dataframe에서 가져옴 - 종가를 test base price로 하는 경우\n",
    "test_dates, test_base_prices, train_dates, train_base_prices = prepro.get_test_dates_prices(dataframe, test_start, test_end,\n",
    "                                                      train_start, train_end, n_timestep, time_interval, future_day, target_column)\n",
    "\n",
    "\n",
    "# 전체 test_oouput 생성\n",
    "_, test_data = prepro.get_train_test_data(df, target_column, remove_columns,\n",
    "                                                   train_start, train_end,\n",
    "                                                   test_start, test_end,\n",
    "                                                   future_day, n_timestep, time_interval)\n",
    "_, test_y = prepro.get_LSTM_dataset(test_data, n_timestep, time_interval, input_size, future_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updown = np.sign(test_y[:, -1, 3]).reshape((-1))\n",
    "#calculate accuracy\n",
    "temp = tf.math.multiply(updown, t1.reshape((-1)))\n",
    "accu = tf.reduce_sum(list(map(lambda x: 1 if x > 0 else 0, temp)))/len(temp)\n",
    "print('accuracy = ', accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updown.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_base_prices = train_base_prices[:batch_size]\n",
    "#train_prediction = train_prediction[:batch_size]\n",
    "#train_y = train_y[:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = GenerateResult(t1, t2, test_y[:, -1, 3].reshape(-1), test_dates, n_timestep, future_day, train_end_back, trans_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_start_index = prepro.date_to_index(df, test_start)\n",
    "test_end_index = prepro.date_to_index(df, test_end)\n",
    "test_open_prices = list(map(float, df.loc[test_start_index - future_day: test_end_index - future_day, '시가']))\n",
    "test_high_prices = list(map(float, df.loc[test_start_index - future_day: test_end_index - future_day, '고가']))\n",
    "test_low_prices = list(map(float, df.loc[test_start_index - future_day: test_end_index - future_day, '저가']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result.extract_last_output()\n",
    "result.convert_price(test_base_prices,conversion_type=target_type)\n",
    "# 손익 계산\n",
    "profits = np.zeros(len(test_dates))\n",
    "for i in range(len(test_dates)):\n",
    "    if result.test_predict_price[i]-test_open_prices[i] > 0:\n",
    "        profits[i] = result.test_output_price[i] - test_open_prices[i]\n",
    "    else:\n",
    "        profits[i] = test_open_prices[i] - result.test_output_price[i]\n",
    "# 손절 손익 계산\n",
    "profits2 = np.zeros(len(test_dates))\n",
    "for i in range(len(test_dates)):\n",
    "    if result.test_predict_price[i] - test_open_prices[i] > 0:\n",
    "        if test_open_prices[i] - test_low_prices[i] > t2[i]:\n",
    "            profits2[i] = -t2[i]\n",
    "        else:\n",
    "            profits2[i] = profits[i]\n",
    "    else:\n",
    "        if test_high_prices[i] - test_open_prices[i] > t2[i]:\n",
    "            profits2[i] = -t2[i]\n",
    "        else:\n",
    "            profits2[i] = profits[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array(result.test_predict_price) - np.array(test_open_prices)\n",
    "trues = np.array(result.test_output_price) - np.array(test_open_prices)\n",
    "result.evaluation(preds, trues)\n",
    "result.table(test_open_prices, test_high_prices, test_low_prices, profits, profits2)\n",
    "result.save_result(model_name,item_name,n_unit,target_type,batch_size,n_timestep,time_interval,epochs,str(alpha),comment)\n",
    "result.save_visualization()\n",
    "result.save_model(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "current_train_start = df.loc[prepro.date_to_index(df, train_start), 'date']\n",
    "current_train_end = df.loc[prepro.date_to_index(df, train_end), 'date']\n",
    "current_test_start = df.loc[prepro.date_to_index(df, test_start), 'date']\n",
    "current_test_end = df.loc[prepro.date_to_index(df, test_start) + trans_day - 1, 'date']\n",
    "\n",
    "\n",
    "#  각 transfer 구간의 예측값들을 합치기 위하여\n",
    "test_prediction1 = []\n",
    "test_prediction2 = []\n",
    "test_target = []\n",
    "\n",
    "# early_stopping = tf.keras.callbacks.EarlyStopping(patience=3, verbose=1)\n",
    "early_stopping = learn.EarlyStopping(patience=2, verbose=1)\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "train_data, test_data = prepro.get_train_test_data(df, target_column, remove_columns, \n",
    "                                                       current_train_start, current_train_end,\n",
    "                                                       current_test_start, current_test_end,\n",
    "                                                       future_day, n_timestep, time_interval)\n",
    "\n",
    "# input_size, columns reset\n",
    "input_size = len(df.columns) - len(remove_columns)\n",
    "input_columns = df.columns.copy()\n",
    "\n",
    "train_x, train_y = prepro.get_LSTM_dataset(train_data, n_timestep, time_interval, input_size, future_day)\n",
    "test_x, test_y = prepro.get_LSTM_dataset(test_data, n_timestep, time_interval, input_size, future_day)\n",
    "\n",
    "#model.compile(optimizer='adam',\n",
    "#          loss=loss_fn)\n",
    "#          #callbacks=[cp-callback]\n",
    "#          #metrics=['accuracy'])\n",
    "\n",
    "# the firs training dataset\n",
    "train_x = train_x[:-future_day]\n",
    "train_y = train_y[:-future_day]    \n",
    "\n",
    "#global_step = tf.train.get_or_create_global_step()\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "#lr_decay = tf.train.exponential_decay(learning_rate, global_step,\n",
    "#                                      train_input.shape[0]/batch_size*5, 0.5, staircase=True)\n",
    "lr_decay = tf.compat.v1.train.exponential_decay(learning_rate,global_step, int(len(train_y)/batch_size), 0.96, staircase=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "#optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "updown = np.sign(test_y[:, -1, 0]).reshape((-1))    \n",
    "epochs = len(train_y)\n",
    "for iteration in range(399):\n",
    "    batch_input, batch_output = learn.next_random_interval_batch(train_x, train_y, batch_size, future_day)\n",
    "\n",
    "    #noise = 2*np.random.randn(batch_size,n_timestep,1)\n",
    "    #batch_output = batch_output+noise\n",
    "    #batch_input = encoder(train_input[idx])\n",
    "    gradients1 = gradient1(model1, model2, batch_input, batch_output)\n",
    "    optimizer.apply_gradients(zip(gradients1, model1.trainable_variables))\n",
    "    \n",
    "    targets = tf.reshape(train_y[:, -1, 0], [-1])\n",
    "    rates = targets / 100\n",
    "    preds = tf.reshape(model1(train_x, training=False)[:, -1, 0], [-1])\n",
    "    \n",
    "    n = len(targets)    \n",
    "    returns = [1.0]\n",
    "    losses = []\n",
    "    for i in range(n - 1):\n",
    "       \n",
    "        # average_return, std of returns, remaining days, preds[0] \n",
    "        state = []\n",
    "        \n",
    "        random_rates = []\n",
    "        for k in range(i+1):\n",
    "            random_rates.append(rates[k])\n",
    "        # 실재 수익률을 기반으로 random 수익률 생성, 예측에 의한 porfits 생성 \n",
    "        for k in range(i+1, n):\n",
    "            random_rates.append(tf.random.normal((), mean=targets[k] / 100, stddev=0.1, dtype=tf.float64))\n",
    "        profits = tf.convert_to_tensor(random_rates, dtype=tf.float64)*tf.cast(preds, dtype=tf.float64) + 1\n",
    "        \n",
    "        # 현재까지의 예측에 의한 수익률 기하평균 구하기\n",
    "        returns_past = []\n",
    "        for k in range(i+1):\n",
    "            returns_past.append(profits[k])\n",
    "        avg_return = tf.cast(tf.math.reduce_prod(returns_past)**(1/(i+1)), dtype=tf.float64)\n",
    "        \n",
    "        state.append(avg_return)\n",
    "        state.append(tf.math.reduce_std(returns))\n",
    "        state.append((n - i) / n)\n",
    "        state.append(preds[i])\n",
    "        state = np.array(state).reshape((1, 4))\n",
    "     \n",
    "        # 목표일까지의 기대 기하 평균 수익률 구하기 \n",
    "        returns_future = []\n",
    "        for j in range(i+1, n):\n",
    "            returns_future.append(profits[j])\n",
    "        avg_return_future = tf.math.reduce_prod(returns_future)**(1/(n-i-1))\n",
    "\n",
    "        # 예측 기하 평균 수익률과 기대 기하 평균 수익률의 MSE\n",
    "        value = tf.cast(model2(state, training=True)[0, 0], dtype=tf.float64)\n",
    "        losses.append((value - avg_return_future)**2)\n",
    "        if n == 3: break\n",
    "    print(\"losses\", losses)\n",
    "    print(\"value\", value)    \n",
    "    with tf.GradientTape() as tape:\n",
    "        gradients2 = tape.gradient(tf.math.reduce_sum(losses), model2.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients2, model2.trainable_variables))\n",
    "    \n",
    "\n",
    "    \n",
    "    if iteration % 100 == 0:\n",
    "        #test_MSE = model.evaluate(test_x, test_y)\n",
    "        prediction = model1.predict(test_x)\n",
    "        prediction_MSE = sum((updown - np.sign(prediction[:, -1, 0]).reshape(-1))**2)/len(test_y)\n",
    "        print('prediction_MSE =', prediction_MSE)\n",
    "\n",
    "    if iteration > epochs / 2 and early_stopping.validate(prediction_MSE)==True:\n",
    "        break\n",
    "epochs -= epochs / 5\n",
    "if epochs <= 0: epochs = 100\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow)",
   "language": "python",
   "name": "conda_tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
